<!DOCTYPE html>
<html  lang="en">
<head><meta charset="utf-8">
<title>Short: Shap values</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary_large_image">
<meta property="og:image" content="/social-card-preview.png">
<meta property="og:image:width" content="400">
<meta property="og:image:height" content="300">
<meta property="og:image:alt" content="An image showcasing my project.">
<meta property="og:title" content="Short: Shap values">
<meta name="description" content="Hello readers, it has been quite some time since our last blog post, and for that, I apologize. Due to personal and work matters, I have been away for almost two months. However, I am excited to be back and share with you a topic that I think you will find interesting: Shap values.">
<meta property="og:description" content="Hello readers, it has been quite some time since our last blog post, and for that, I apologize. Due to personal and work matters, I have been away for almost two months. However, I am excited to be back and share with you a topic that I think you will find interesting: Shap values.">
<link rel="preload" as="fetch" crossorigin="anonymous" href="/articles/2023-04-14-short-shap/_payload.json">
<link rel="stylesheet" href="/_nuxt/entry.2b709d1c.css">
<link rel="stylesheet" href="/_nuxt/DocumentDrivenNotFound.aa939160.css">
<link rel="stylesheet" href="/_nuxt/article.06aa1a19.css">
<link rel="stylesheet" href="/_nuxt/ProseA.baee409d.css">
<link rel="stylesheet" href="/_nuxt/ProseH1.6d63403c.css">
<link rel="stylesheet" href="/_nuxt/ProseStrong.b01d4b3b.css">
<link rel="stylesheet" href="/_nuxt/ProseP.b99f89cd.css">
<link rel="stylesheet" href="/_nuxt/ProseH2.3a63b076.css">
<link rel="stylesheet" href="/_nuxt/ProseImg.eeea4224.css">
<link rel="stylesheet" href="/_nuxt/ProseUl.0edd7272.css">
<link rel="stylesheet" href="/_nuxt/ProseLi.a0b5f8a8.css">
<link rel="stylesheet" href="/_nuxt/ProseEm.35a26f4d.css">
<link rel="stylesheet" href="/_nuxt/ProseCodeInline.873c7ac7.css">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/entry.5b2fbc2b.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/NuxtImg.71ecd94d.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/document-driven.02488242.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenEmpty.33a0504c.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRenderer.ea978627.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.vue.233735c0.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenNotFound.e61e620a.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/article.74583e02.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseA.b25940ab.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/date.824a539b.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.c50a67fb.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH1.0f011535.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseStrong.11c76ad1.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseP.df39db4a.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH2.106e00e3.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseImg.f7a26e58.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseUl.98914a64.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseLi.377a25ce.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseEm.ab0e77c6.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseCodeInline.837c2ab9.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/default.9a1873bb.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/page.dfa15be1.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/client-db.b6bffc10.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/pipeline.b9c7be3a.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/client-db.33f4b423.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/debug.fde61575.js">
<link rel="prefetch" as="style" href="/_nuxt/useStudio.a92b5f33.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/useStudio.8bae12b1.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/asyncData.57510f6c.js">
<link rel="prefetch" as="style" href="/_nuxt/error-404.7910d5ca.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-404.3de20a2d.js">
<link rel="prefetch" as="style" href="/_nuxt/error-500.1db01289.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-500.2ac66414.js">
<script type="module" src="/_nuxt/entry.5b2fbc2b.js" crossorigin></script><style id="pinceau-runtime-hydratable">@media{.phy[--]{--puid:D206xZ-v;}.pv-kIiWdX{max-width:var(--elements-container-maxWidth);padding-left:var(--elements-container-padding-mobile);padding-right:var(--elements-container-padding-mobile);}@media (min-width: 475px){.pv-kIiWdX{padding-left:var(--elements-container-padding-xs);padding-right:var(--elements-container-padding-xs);}}@media (min-width: 640px){.pv-kIiWdX{padding-left:var(--elements-container-padding-sm);padding-right:var(--elements-container-padding-sm);}}@media (min-width: 768px){.pv-kIiWdX{padding-left:var(--elements-container-padding-md);padding-right:var(--elements-container-padding-md);}}} </style><style id="pinceau-theme">@media { :root {--pinceau-mq: initial; --alpine-readableLine: 68ch;--alpine-backdrop-backgroundColor: #f4f4f5b3;--prose-code-inline-padding: 0.2rem 0.375rem 0.2rem 0.375rem;--prose-code-block-backdropFilter: contrast(1);--prose-code-block-border-style: solid;--prose-code-block-border-width: 1px;--prose-tbody-tr-borderBottom-style: dashed;--prose-tbody-tr-borderBottom-width: 1px;--prose-th-textAlign: inherit;--prose-thead-borderBottom-style: solid;--prose-thead-borderBottom-width: 1px;--prose-thead-border-style: solid;--prose-thead-border-width: 0px;--prose-table-textAlign: start;--prose-hr-width: 1px;--prose-hr-style: solid;--prose-li-listStylePosition: outside;--prose-ol-li-markerColor: currentColor;--prose-ol-paddingInlineStart: 21px;--prose-ol-listStyleType: decimal;--prose-ul-li-markerColor: currentColor;--prose-ul-paddingInlineStart: 21px;--prose-ul-listStyleType: disc;--prose-blockquote-border-style: solid;--prose-blockquote-border-width: 4px;--prose-blockquote-quotes: '201C' '201D' '2018' '2019';--prose-blockquote-paddingInlineStart: 24px;--prose-a-code-color-hover: currentColor;--prose-a-code-color-static: currentColor;--prose-a-hasCode-borderBottom: none;--prose-a-border-distance: 2px;--prose-a-border-color-hover: currentColor;--prose-a-border-color-static: currentColor;--prose-a-border-style-hover: solid;--prose-a-border-style-static: dashed;--prose-a-border-width: 1px;--prose-a-color-static: inherit;--prose-a-textDecoration: none;--prose-h6-margin: 3rem 0 2rem;--prose-h5-margin: 3rem 0 2rem;--prose-h4-margin: 3rem 0 2rem;--prose-h3-margin: 3rem 0 2rem;--prose-h2-margin: 3rem 0 2rem;--prose-h1-margin: 0 0 2rem;--prose-p-fontSize: 18px;--typography-lead-loose: 2;--typography-lead-relaxed: 1.625;--typography-lead-normal: 1.5;--typography-lead-snug: 1.375;--typography-lead-tight: 1.25;--typography-lead-none: 1;--typography-lead-10: 2.5rem;--typography-lead-9: 2.25rem;--typography-lead-8: 2rem;--typography-lead-7: 1.75rem;--typography-lead-6: 1.5rem;--typography-lead-5: 1.25rem;--typography-lead-4: 1rem;--typography-lead-3: .75rem;--typography-lead-2: .5rem;--typography-lead-1: .025rem;--typography-fontWeight-black: 900;--typography-fontWeight-extrabold: 800;--typography-fontWeight-bold: 700;--typography-fontWeight-semibold: 600;--typography-fontWeight-medium: 500;--typography-fontWeight-normal: 400;--typography-fontWeight-light: 300;--typography-fontWeight-extralight: 200;--typography-fontWeight-thin: 100;--typography-fontSize-9xl: 128px;--typography-fontSize-8xl: 96px;--typography-fontSize-7xl: 72px;--typography-fontSize-6xl: 60px;--typography-fontSize-5xl: 48px;--typography-fontSize-4xl: 36px;--typography-fontSize-3xl: 30px;--typography-fontSize-2xl: 24px;--typography-fontSize-xl: 20px;--typography-fontSize-lg: 18px;--typography-fontSize-base: 16px;--typography-fontSize-sm: 14px;--typography-fontSize-xs: 12px;--typography-letterSpacing-wide: 0.025em;--typography-letterSpacing-tight: -0.025em;--typography-verticalMargin-base: 24px;--typography-verticalMargin-sm: 16px;--elements-border-secondary-hover: [object Object];--elements-backdrop-background: #fffc;--elements-backdrop-filter: saturate(200%) blur(20px);--elements-container-maxWidth: 64rem;--lead-loose: 2;--lead-relaxed: 1.625;--lead-normal: 1.5;--lead-snug: 1.375;--lead-tight: 1.25;--lead-none: 1;--lead-10: 2.5rem;--lead-9: 2.25rem;--lead-8: 2rem;--lead-7: 1.75rem;--lead-6: 1.5rem;--lead-5: 1.25rem;--lead-4: 1rem;--lead-3: .75rem;--lead-2: .5rem;--lead-1: .025rem;--letterSpacing-widest: 0.1em;--letterSpacing-wider: 0.05em;--letterSpacing-wide: 0.025em;--letterSpacing-normal: 0em;--letterSpacing-tight: -0.025em;--letterSpacing-tighter: -0.05em;--fontSize-9xl: 8rem;--fontSize-8xl: 6rem;--fontSize-7xl: 4.5rem;--fontSize-6xl: 3.75rem;--fontSize-5xl: 3rem;--fontSize-4xl: 2.25rem;--fontSize-3xl: 1.875rem;--fontSize-2xl: 1.5rem;--fontSize-xl: 1.25rem;--fontSize-lg: 1.125rem;--fontSize-base: 1rem;--fontSize-sm: 0.875rem;--fontSize-xs: 0.75rem;--fontWeight-black: 900;--fontWeight-extrabold: 800;--fontWeight-bold: 700;--fontWeight-semibold: 600;--fontWeight-medium: 500;--fontWeight-normal: 400;--fontWeight-light: 300;--fontWeight-extralight: 200;--fontWeight-thin: 100;--font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace;--font-serif: ui-serif, Georgia, Cambria, Times New Roman, Times, serif;--font-sans: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;--opacity-total: 1;--opacity-high: 0.8;--opacity-medium: 0.5;--opacity-soft: 0.3;--opacity-light: 0.15;--opacity-bright: 0.1;--opacity-noOpacity: 0;--borderWidth-lg: 3px;--borderWidth-md: 2px;--borderWidth-sm: 1px;--borderWidth-noBorder: 0;--space-rem-875: 0.875rem;--space-rem-625: 0.625rem;--space-rem-375: 0.375rem;--space-rem-125: 0.125rem;--space-px: 1px;--space-128: 32rem;--space-96: 24rem;--space-80: 20rem;--space-72: 18rem;--space-64: 16rem;--space-60: 15rem;--space-56: 14rem;--space-52: 13rem;--space-48: 12rem;--space-44: 11rem;--space-40: 10rem;--space-36: 9rem;--space-32: 8rem;--space-28: 7rem;--space-24: 6rem;--space-20: 5rem;--space-16: 4rem;--space-14: 3.5rem;--space-12: 3rem;--space-11: 2.75rem;--space-10: 2.5rem;--space-9: 2.25rem;--space-8: 2rem;--space-7: 1.75rem;--space-6: 1.5rem;--space-5: 1.25rem;--space-4: 1rem;--space-3: 0.75rem;--space-2: 0.5rem;--space-1: 0.25rem;--space-0: 0px;--size-full: 100%;--size-7xl: 80rem;--size-6xl: 72rem;--size-5xl: 64rem;--size-4xl: 56rem;--size-3xl: 48rem;--size-2xl: 42rem;--size-xl: 36rem;--size-lg: 32rem;--size-md: 28rem;--size-sm: 24rem;--size-xs: 20rem;--size-200: 200px;--size-104: 104px;--size-80: 80px;--size-64: 64px;--size-56: 56px;--size-48: 48px;--size-40: 40px;--size-32: 32px;--size-24: 24px;--size-20: 20px;--size-16: 16px;--size-12: 12px;--size-8: 8px;--size-6: 6px;--size-4: 4px;--size-2: 2px;--size-0: 0px;--radii-full: 9999px;--radii-3xl: 1.75rem;--radii-2xl: 1.5rem;--radii-xl: 1rem;--radii-lg: 0.75rem;--radii-md: 0.5rem;--radii-sm: 0.375rem;--radii-xs: 0.25rem;--radii-2xs: 0.125rem;--radii-none: 0px;--shadow-none: 0px 0px 0px 0px transparent;--shadow-lg: 0px 10px 15px -3px #000000, 0px 4px 6px -4px #000000;--shadow-md: 0px 4px 6px -1px #000000, 0px 2px 4px -2px #000000;--shadow-sm: 0px 1px 3px 0px #000000, 0px 1px 2px -1px #000000;--shadow-xs: 0px 1px 2px 0px #000000;--height-screen: 100vh;--width-screen: 100vw;--color-primary-900: #002e38;--color-primary-800: #005c70;--color-primary-700: #008aa9;--color-primary-600: #00b9e1;--color-primary-500: #1ad6ff;--color-primary-400: #40ddff;--color-primary-300: #66e4ff;--color-primary-200: #8deaff;--color-primary-100: #b3f1ff;--color-primary-50: #d9f8ff;--color-ruby-900: #380011;--color-ruby-800: #700021;--color-ruby-700: #a90032;--color-ruby-600: #e10043;--color-ruby-500: #ff1a5e;--color-ruby-400: #ff4079;--color-ruby-300: #ff6694;--color-ruby-200: #ff8dae;--color-ruby-100: #ffb3c9;--color-ruby-50: #ffd9e4;--color-pink-900: #380025;--color-pink-800: #70004b;--color-pink-700: #a90070;--color-pink-600: #e10095;--color-pink-500: #ff1ab2;--color-pink-400: #ff40bf;--color-pink-300: #ff66cc;--color-pink-200: #ff8dd8;--color-pink-100: #ffb3e5;--color-pink-50: #ffd9f2;--color-purple-900: #190038;--color-purple-800: #330070;--color-purple-700: #4c00a9;--color-purple-600: #6500e1;--color-purple-500: #811aff;--color-purple-400: #9640ff;--color-purple-300: #ab66ff;--color-purple-200: #c08dff;--color-purple-100: #d5b3ff;--color-purple-50: #ead9ff;--color-royalblue-900: #0b0531;--color-royalblue-800: #160a62;--color-royalblue-700: #211093;--color-royalblue-600: #2c15c4;--color-royalblue-500: #4127e8;--color-royalblue-400: #614bec;--color-royalblue-300: #806ff0;--color-royalblue-200: #a093f3;--color-royalblue-100: #c0b7f7;--color-royalblue-50: #dfdbfb;--color-indigoblue-900: #001238;--color-indigoblue-800: #002370;--color-indigoblue-700: #0035a9;--color-indigoblue-600: #0047e1;--color-indigoblue-500: #1a62ff;--color-indigoblue-400: #407cff;--color-indigoblue-300: #6696ff;--color-indigoblue-200: #8db0ff;--color-indigoblue-100: #b3cbff;--color-indigoblue-50: #d9e5ff;--color-blue-900: #002438;--color-blue-800: #004870;--color-blue-700: #006ca9;--color-blue-600: #0090e1;--color-blue-500: #1aadff;--color-blue-400: #40bbff;--color-blue-300: #66c8ff;--color-blue-200: #8dd6ff;--color-blue-100: #b3e4ff;--color-blue-50: #d9f1ff;--color-lightblue-900: #002e38;--color-lightblue-800: #005c70;--color-lightblue-700: #008aa9;--color-lightblue-600: #00b9e1;--color-lightblue-500: #1ad6ff;--color-lightblue-400: #40ddff;--color-lightblue-300: #66e4ff;--color-lightblue-200: #8deaff;--color-lightblue-100: #b3f1ff;--color-lightblue-50: #d9f8ff;--color-teal-900: #062a28;--color-teal-800: #0b544f;--color-teal-700: #117d77;--color-teal-600: #16a79e;--color-teal-500: #1cd1c6;--color-teal-400: #36e4da;--color-teal-300: #5fe9e1;--color-teal-200: #87efe9;--color-teal-100: #aff4f0;--color-teal-50: #d7faf8;--color-pear-900: #2a2b09;--color-pear-800: #545512;--color-pear-700: #7e801b;--color-pear-600: #a8aa24;--color-pear-500: #d0d32f;--color-pear-400: #d8da52;--color-pear-300: #e0e274;--color-pear-200: #e8e997;--color-pear-100: #eff0ba;--color-pear-50: #f7f8dc;--color-red-900: #380300;--color-red-800: #700700;--color-red-700: #a90a00;--color-red-600: #e10e00;--color-red-500: #ff281a;--color-red-400: #ff4c40;--color-red-300: #ff7066;--color-red-200: #ff948d;--color-red-100: #ffb7b3;--color-red-50: #ffdbd9;--color-orange-900: #381800;--color-orange-800: #702f00;--color-orange-700: #a94700;--color-orange-600: #e15e00;--color-orange-500: #ff7a1a;--color-orange-400: #ff9040;--color-orange-300: #ffa666;--color-orange-200: #ffbd8d;--color-orange-100: #ffd3b3;--color-orange-50: #ffe9d9;--color-yellow-900: #362b03;--color-yellow-800: #6d5605;--color-yellow-700: #a38108;--color-yellow-600: #daac0a;--color-yellow-500: #f5c828;--color-yellow-400: #f7d14c;--color-yellow-300: #f8da70;--color-yellow-200: #fae393;--color-yellow-100: #fcedb7;--color-yellow-50: #fdf6db;--color-green-900: #003f25;--color-green-800: #005e38;--color-green-700: #007e4a;--color-green-600: #009d5d;--color-green-500: #00bd6f;--color-green-400: #00dc82;--color-green-300: #30ffaa;--color-green-200: #83ffcc;--color-green-100: #acffdd;--color-green-50: #d6ffee;--color-gray-900: #18181B;--color-gray-800: #27272A;--color-gray-700: #3f3f46;--color-gray-600: #52525B;--color-gray-500: #71717A;--color-gray-400: #a1a1aa;--color-gray-300: #D4d4d8;--color-gray-200: #e4e4e7;--color-gray-100: #f4f4f5;--color-gray-50: #fafafa;--color-black: #0c0c0d;--color-white: #FFFFFF;--media-portrait: only screen and (orientation: portrait);--media-landscape: only screen and (orientation: landscape);--media-rm: (prefers-reduced-motion: reduce);--media-2xl: (min-width: 1536px);--media-xl: (min-width: 1280px);--media-lg: (min-width: 1024px);--media-md: (min-width: 768px);--media-sm: (min-width: 640px);--media-xs: (min-width: 475px);--alpine-body-color: var(--color-gray-800);--alpine-body-backgroundColor: var(--color-white);--prose-code-inline-fontWeight: var(--typography-fontWeight-normal);--prose-code-inline-fontSize: var(--typography-fontSize-sm);--prose-code-inline-borderRadius: var(--radii-xs);--prose-code-block-pre-padding: var(--typography-verticalMargin-sm);--prose-code-block-margin: var(--typography-verticalMargin-base) 0;--prose-code-block-fontSize: var(--typography-fontSize-sm);--prose-tbody-code-inline-fontSize: var(--typography-fontSize-sm);--prose-tbody-td-padding: var(--typography-verticalMargin-sm);--prose-th-fontWeight: var(--typography-fontWeight-semibold);--prose-th-padding: 0 var(--typography-verticalMargin-sm) var(--typography-verticalMargin-sm) var(--typography-verticalMargin-sm);--prose-table-lineHeight: var(--typography-lead-6);--prose-table-fontSize: var(--typography-fontSize-sm);--prose-table-margin: var(--typography-verticalMargin-base) 0;--prose-hr-margin: var(--typography-verticalMargin-base) 0;--prose-li-margin: var(--typography-verticalMargin-sm) 0;--prose-ol-margin: var(--typography-verticalMargin-base) 0;--prose-ul-margin: var(--typography-verticalMargin-base) 0;--prose-blockquote-margin: var(--typography-verticalMargin-base) 0;--prose-a-code-border-style: var(--prose-a-border-style-static);--prose-a-code-border-width: var(--prose-a-border-width);--prose-a-fontWeight: var(--typography-fontWeight-medium);--prose-img-margin: var(--typography-verticalMargin-base) 0;--prose-strong-fontWeight: var(--typography-fontWeight-semibold);--prose-h6-iconSize: var(--typography-fontSize-base);--prose-h6-fontWeight: var(--typography-fontWeight-semibold);--prose-h6-lineHeight: var(--typography-lead-normal);--prose-h6-fontSize: var(--typography-fontSize-lg);--prose-h5-iconSize: var(--typography-fontSize-lg);--prose-h5-fontWeight: var(--typography-fontWeight-semibold);--prose-h5-lineHeight: var(--typography-lead-snug);--prose-h5-fontSize: var(--typography-fontSize-xl);--prose-h4-iconSize: var(--typography-fontSize-lg);--prose-h4-letterSpacing: var(--typography-letterSpacing-tight);--prose-h4-fontWeight: var(--typography-fontWeight-semibold);--prose-h4-lineHeight: var(--typography-lead-snug);--prose-h4-fontSize: var(--typography-fontSize-2xl);--prose-h3-iconSize: var(--typography-fontSize-xl);--prose-h3-letterSpacing: var(--typography-letterSpacing-tight);--prose-h3-fontWeight: var(--typography-fontWeight-semibold);--prose-h3-lineHeight: var(--typography-lead-snug);--prose-h3-fontSize: var(--typography-fontSize-3xl);--prose-h2-iconSize: var(--typography-fontSize-2xl);--prose-h2-letterSpacing: var(--typography-letterSpacing-tight);--prose-h2-fontWeight: var(--typography-fontWeight-semibold);--prose-h2-lineHeight: var(--typography-lead-tight);--prose-h2-fontSize: var(--typography-fontSize-4xl);--prose-h1-iconSize: var(--typography-fontSize-3xl);--prose-h1-letterSpacing: var(--typography-letterSpacing-tight);--prose-h1-fontWeight: var(--typography-fontWeight-bold);--prose-h1-lineHeight: var(--typography-lead-tight);--prose-h1-fontSize: var(--typography-fontSize-5xl);--prose-p-br-margin: var(--typography-verticalMargin-base) 0 0 0;--prose-p-margin: var(--typography-verticalMargin-base) 0;--prose-p-lineHeight: var(--typography-lead-normal);--typography-color-primary-900: var(--color-primary-900);--typography-color-primary-800: var(--color-primary-800);--typography-color-primary-700: var(--color-primary-700);--typography-color-primary-600: var(--color-primary-600);--typography-color-primary-500: var(--color-primary-500);--typography-color-primary-400: var(--color-primary-400);--typography-color-primary-300: var(--color-primary-300);--typography-color-primary-200: var(--color-primary-200);--typography-color-primary-100: var(--color-primary-100);--typography-color-primary-50: var(--color-primary-50);--typography-font-code: var(--font-mono);--typography-font-body: var(--font-sans);--typography-font-display: var(--font-sans);--typography-body-backgroundColor: var(--color-white);--typography-body-color: var(--color-black);--elements-state-danger-borderColor-secondary: var(--color-red-200);--elements-state-danger-borderColor-primary: var(--color-red-100);--elements-state-danger-backgroundColor-secondary: var(--color-red-100);--elements-state-danger-backgroundColor-primary: var(--color-red-50);--elements-state-danger-color-secondary: var(--color-red-600);--elements-state-danger-color-primary: var(--color-red-500);--elements-state-warning-borderColor-secondary: var(--color-yellow-200);--elements-state-warning-borderColor-primary: var(--color-yellow-100);--elements-state-warning-backgroundColor-secondary: var(--color-yellow-100);--elements-state-warning-backgroundColor-primary: var(--color-yellow-50);--elements-state-warning-color-secondary: var(--color-yellow-700);--elements-state-warning-color-primary: var(--color-yellow-600);--elements-state-success-borderColor-secondary: var(--color-green-200);--elements-state-success-borderColor-primary: var(--color-green-100);--elements-state-success-backgroundColor-secondary: var(--color-green-100);--elements-state-success-backgroundColor-primary: var(--color-green-50);--elements-state-success-color-secondary: var(--color-green-600);--elements-state-success-color-primary: var(--color-green-500);--elements-state-info-borderColor-secondary: var(--color-blue-200);--elements-state-info-borderColor-primary: var(--color-blue-100);--elements-state-info-backgroundColor-secondary: var(--color-blue-100);--elements-state-info-backgroundColor-primary: var(--color-blue-50);--elements-state-info-color-secondary: var(--color-blue-600);--elements-state-info-color-primary: var(--color-blue-500);--elements-state-primary-borderColor-secondary: var(--color-primary-200);--elements-state-primary-borderColor-primary: var(--color-primary-100);--elements-state-primary-backgroundColor-secondary: var(--color-primary-100);--elements-state-primary-backgroundColor-primary: var(--color-primary-50);--elements-state-primary-color-secondary: var(--color-primary-700);--elements-state-primary-color-primary: var(--color-primary-600);--elements-surface-secondary-backgroundColor: var(--color-gray-200);--elements-surface-primary-backgroundColor: var(--color-gray-100);--elements-surface-background-base: var(--color-gray-100);--elements-border-secondary-static: var(--color-gray-200);--elements-border-primary-hover: var(--color-gray-200);--elements-border-primary-static: var(--color-gray-100);--elements-container-padding-md: var(--space-16);--elements-container-padding-sm: var(--space-12);--elements-container-padding-xs: var(--space-8);--elements-container-padding-mobile: var(--space-6);--elements-text-secondary-color-hover: var(--color-gray-700);--elements-text-secondary-color-static: var(--color-gray-500);--elements-text-primary-color-static: var(--color-gray-900);--text-6xl-lineHeight: var(--lead-none);--text-6xl-fontSize: var(--fontSize-6xl);--text-5xl-lineHeight: var(--lead-none);--text-5xl-fontSize: var(--fontSize-5xl);--text-4xl-lineHeight: var(--lead-10);--text-4xl-fontSize: var(--fontSize-4xl);--text-3xl-lineHeight: var(--lead-9);--text-3xl-fontSize: var(--fontSize-3xl);--text-2xl-lineHeight: var(--lead-8);--text-2xl-fontSize: var(--fontSize-2xl);--text-xl-lineHeight: var(--lead-7);--text-xl-fontSize: var(--fontSize-xl);--text-lg-lineHeight: var(--lead-7);--text-lg-fontSize: var(--fontSize-lg);--text-base-lineHeight: var(--lead-6);--text-base-fontSize: var(--fontSize-base);--text-sm-lineHeight: var(--lead-5);--text-sm-fontSize: var(--fontSize-sm);--text-xs-lineHeight: var(--lead-4);--text-xs-fontSize: var(--fontSize-xs);--shadow-2xl: 0px 25px 50px -12px var(--color-gray-900);--shadow-xl: 0px 20px 25px -5px var(--color-gray-400), 0px 8px 10px -6px #000000;--color-secondary-900: var(--color-gray-900);--color-secondary-800: var(--color-gray-800);--color-secondary-700: var(--color-gray-700);--color-secondary-600: var(--color-gray-600);--color-secondary-500: var(--color-gray-500);--color-secondary-400: var(--color-gray-400);--color-secondary-300: var(--color-gray-300);--color-secondary-200: var(--color-gray-200);--color-secondary-100: var(--color-gray-100);--color-secondary-50: var(--color-gray-50);--prose-a-code-background-hover: var(--typography-color-primary-50);--prose-a-code-border-color-hover: var(--typography-color-primary-500);--prose-a-color-hover: var(--typography-color-primary-500);--typography-color-secondary-900: var(--color-secondary-900);--typography-color-secondary-800: var(--color-secondary-800);--typography-color-secondary-700: var(--color-secondary-700);--typography-color-secondary-600: var(--color-secondary-600);--typography-color-secondary-500: var(--color-secondary-500);--typography-color-secondary-400: var(--color-secondary-400);--typography-color-secondary-300: var(--color-secondary-300);--typography-color-secondary-200: var(--color-secondary-200);--typography-color-secondary-100: var(--color-secondary-100);--typography-color-secondary-50: var(--color-secondary-50);--prose-code-inline-backgroundColor: var(--typography-color-secondary-100);--prose-code-inline-color: var(--typography-color-secondary-700);--prose-code-block-backgroundColor: var(--typography-color-secondary-100);--prose-code-block-color: var(--typography-color-secondary-700);--prose-code-block-border-color: var(--typography-color-secondary-200);--prose-tbody-tr-borderBottom-color: var(--typography-color-secondary-200);--prose-th-color: var(--typography-color-secondary-600);--prose-thead-borderBottom-color: var(--typography-color-secondary-200);--prose-thead-border-color: var(--typography-color-secondary-300);--prose-hr-color: var(--typography-color-secondary-200);--prose-blockquote-border-color: var(--typography-color-secondary-200);--prose-blockquote-color: var(--typography-color-secondary-500);--prose-a-code-border-color-static: var(--typography-color-secondary-400); } }@media { :root.dark {--pinceau-mq: dark; --alpine-backdrop-backgroundColor: #18181bb3;--prose-code-block-backdropFilter: contrast(1);--prose-ol-li-markerColor: currentColor;--prose-ul-li-markerColor: currentColor;--prose-a-code-color-hover: currentColor;--prose-a-code-color-static: currentColor;--prose-a-border-color-hover: currentColor;--prose-a-border-color-static: currentColor;--prose-a-color-static: inherit;--elements-backdrop-background: #0c0d0ccc;--alpine-body-color: var(--color-gray-200);--alpine-body-backgroundColor: var(--color-black);--typography-body-backgroundColor: var(--color-black);--typography-body-color: var(--color-white);--elements-state-danger-borderColor-secondary: var(--color-red-700);--elements-state-danger-borderColor-primary: var(--color-red-800);--elements-state-danger-backgroundColor-secondary: var(--color-red-800);--elements-state-danger-backgroundColor-primary: var(--color-red-900);--elements-state-danger-color-secondary: var(--color-red-200);--elements-state-danger-color-primary: var(--color-red-300);--elements-state-warning-borderColor-secondary: var(--color-yellow-700);--elements-state-warning-borderColor-primary: var(--color-yellow-800);--elements-state-warning-backgroundColor-secondary: var(--color-yellow-800);--elements-state-warning-backgroundColor-primary: var(--color-yellow-900);--elements-state-warning-color-secondary: var(--color-yellow-200);--elements-state-warning-color-primary: var(--color-yellow-400);--elements-state-success-borderColor-secondary: var(--color-green-700);--elements-state-success-borderColor-primary: var(--color-green-800);--elements-state-success-backgroundColor-secondary: var(--color-green-800);--elements-state-success-backgroundColor-primary: var(--color-green-900);--elements-state-success-color-secondary: var(--color-green-200);--elements-state-success-color-primary: var(--color-green-400);--elements-state-info-borderColor-secondary: var(--color-blue-700);--elements-state-info-borderColor-primary: var(--color-blue-800);--elements-state-info-backgroundColor-secondary: var(--color-blue-800);--elements-state-info-backgroundColor-primary: var(--color-blue-900);--elements-state-info-color-secondary: var(--color-blue-200);--elements-state-info-color-primary: var(--color-blue-400);--elements-state-primary-borderColor-secondary: var(--color-primary-700);--elements-state-primary-borderColor-primary: var(--color-primary-800);--elements-state-primary-backgroundColor-secondary: var(--color-primary-800);--elements-state-primary-backgroundColor-primary: var(--color-primary-900);--elements-state-primary-color-secondary: var(--color-primary-200);--elements-state-primary-color-primary: var(--color-primary-400);--elements-surface-secondary-backgroundColor: var(--color-gray-800);--elements-surface-primary-backgroundColor: var(--color-gray-900);--elements-surface-background-base: var(--color-gray-900);--elements-border-secondary-static: var(--color-gray-800);--elements-border-primary-hover: var(--color-gray-800);--elements-border-primary-static: var(--color-gray-900);--elements-text-secondary-color-hover: var(--color-gray-200);--elements-text-secondary-color-static: var(--color-gray-400);--elements-text-primary-color-static: var(--color-gray-50);--prose-a-code-background-hover: var(--typography-color-primary-900);--prose-a-code-border-color-hover: var(--typography-color-primary-600);--prose-a-color-hover: var(--typography-color-primary-400);--prose-code-inline-backgroundColor: var(--typography-color-secondary-800);--prose-code-inline-color: var(--typography-color-secondary-200);--prose-code-block-backgroundColor: var(--typography-color-secondary-900);--prose-code-block-color: var(--typography-color-secondary-200);--prose-code-block-border-color: var(--typography-color-secondary-800);--prose-tbody-tr-borderBottom-color: var(--typography-color-secondary-800);--prose-th-color: var(--typography-color-secondary-400);--prose-thead-borderBottom-color: var(--typography-color-secondary-800);--prose-thead-border-color: var(--typography-color-secondary-600);--prose-hr-color: var(--typography-color-secondary-800);--prose-blockquote-border-color: var(--typography-color-secondary-700);--prose-blockquote-color: var(--typography-color-secondary-400);--prose-a-code-border-color-static: var(--typography-color-secondary-600); } }</style><script>"use strict";(()=>{const a=window,e=document.documentElement,m=["dark","light"],c=window.localStorage.getItem("nuxt-color-mode")||"system";let n=c==="system"?f():c;const l=e.getAttribute("data-color-mode-forced");l&&(n=l),i(n),a["__NUXT_COLOR_MODE__"]={preference:c,value:n,getColorScheme:f,addColorScheme:i,removeColorScheme:d};function i(o){const t=""+o+"",s="";e.classList?e.classList.add(t):e.className+=" "+t,s&&e.setAttribute("data-"+s,o)}function d(o){const t=""+o+"",s="";e.classList?e.classList.remove(t):e.className=e.className.replace(new RegExp(t,"g"),""),s&&e.removeAttribute("data-"+s)}function r(o){return a.matchMedia("(prefers-color-scheme"+o+")")}function f(){if(a.matchMedia&&r("").media!=="not all"){for(const o of m)if(r(":"+o).matches)return o}return"light"}})();
</script></head>
<body ><div id="__nuxt"><div class="container pv-kIiWdX pc-D206xZ app-layout" data-v-93c22c3b data-v-6d327d86><!--[--><div class="nuxt-progress" style="width:0%;height:3px;opacity:0;background-size:Infinity% auto;" data-v-93c22c3b></div><header class="left" data-v-93c22c3b data-v-ebfd1e7c><div class="menu" data-v-ebfd1e7c><button aria-label="Navigation Menu" data-v-ebfd1e7c><svg width="24" height="24" viewBox="0 0 68 68" fill="currentColor" xmlns="http://www.w3.org/2000/svg" data-v-ebfd1e7c><path d="M8 34C8 32.1362 8 31.2044 8.30448 30.4693C8.71046 29.4892 9.48915 28.7105 10.4693 28.3045C11.2044 28 12.1362 28 14 28C15.8638 28 16.7956 28 17.5307 28.3045C18.5108 28.7105 19.2895 29.4892 19.6955 30.4693C20 31.2044 20 32.1362 20 34C20 35.8638 20 36.7956 19.6955 37.5307C19.2895 38.5108 18.5108 39.2895 17.5307 39.6955C16.7956 40 15.8638 40 14 40C12.1362 40 11.2044 40 10.4693 39.6955C9.48915 39.2895 8.71046 38.5108 8.30448 37.5307C8 36.7956 8 35.8638 8 34Z" data-v-ebfd1e7c></path><path d="M28 34C28 32.1362 28 31.2044 28.3045 30.4693C28.7105 29.4892 29.4892 28.7105 30.4693 28.3045C31.2044 28 32.1362 28 34 28C35.8638 28 36.7956 28 37.5307 28.3045C38.5108 28.7105 39.2895 29.4892 39.6955 30.4693C40 31.2044 40 32.1362 40 34C40 35.8638 40 36.7956 39.6955 37.5307C39.2895 38.5108 38.5108 39.2895 37.5307 39.6955C36.7956 40 35.8638 40 34 40C32.1362 40 31.2044 40 30.4693 39.6955C29.4892 39.2895 28.7105 38.5108 28.3045 37.5307C28 36.7956 28 35.8638 28 34Z" data-v-ebfd1e7c></path><path d="M48 34C48 32.1362 48 31.2044 48.3045 30.4693C48.7105 29.4892 49.4892 28.7105 50.4693 28.3045C51.2044 28 52.1362 28 54 28C55.8638 28 56.7956 28 57.5307 28.3045C58.5108 28.7105 59.2895 29.4892 59.6955 30.4693C60 31.2044 60 32.1362 60 34C60 35.8638 60 36.7956 59.6955 37.5307C59.2895 38.5108 58.5108 39.2895 57.5307 39.6955C56.7956 40 55.8638 40 54 40C52.1362 40 51.2044 40 50.4693 39.6955C49.4892 39.2895 48.7105 38.5108 48.3045 37.5307C48 36.7956 48 35.8638 48 34Z" data-v-ebfd1e7c></path></svg></button></div><div class="overlay" data-v-ebfd1e7c><nav data-v-ebfd1e7c data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div><div class="logo" data-v-ebfd1e7c><a href="/" class="" data-v-ebfd1e7c><img src="/logo-dark.svg" class="dark-img" alt="alpine" width="89" height="31" data-v-ebfd1e7c><img src="/logo.svg" class="light-img" alt="alpine" width="89" height="31" data-v-ebfd1e7c></a></div><div class="main-nav" data-v-ebfd1e7c><nav data-v-ebfd1e7c data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div></header><!--[--><div class="document-driven-page"><article data-v-f252e39d><a href="/articles" class="back" data-v-f252e39d><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-f252e39d style="" width="1em" height="1em" viewBox="0 0 256 256" data-v-121c6e7d><path fill="currentColor" d="M224 128a8 8 0 0 1-8 8H59.31l58.35 58.34a8 8 0 0 1-11.32 11.32l-72-72a8 8 0 0 1 0-11.32l72-72a8 8 0 0 1 11.32 11.32L59.31 120H216a8 8 0 0 1 8 8Z"/></svg><span data-v-f252e39d> Back </span></a><header data-v-f252e39d><h1 class="title" data-v-f252e39d>Short: Shap values</h1><time datetime="2023-04-14T00:00:00.000Z" data-v-f252e39d>April 14, 2023</time></header><div class="prose" data-v-f252e39d><!--[--><div><h1 id="short-shap-values" data-v-a5759516><a aria-current="page" href="/articles/2023-04-14-short-shap#short-shap-values" class="router-link-active router-link-exact-active" data-v-a5759516><!--[--><strong data-v-9d7bd52e><!--[-->Short: Shap values<!--]--></strong><!--]--><!----></a></h1><p data-v-63bfa697><!--[-->Hello readers, it has been quite some time since our last blog post, and for that, I apologize. Due to personal and work matters, I have been away for almost two months. However, I am excited to be back and share with you a topic that I think you will find interesting: Shap values.<!--]--></p><h2 id="motivation-why-is-this-todays-topic" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#motivation-why-is-this-todays-topic" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Motivation: Why is this today&#39;s topic?<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->I recently used Shap values in a project and found it to be a chance to tell you something of interest. Shap values are something I&#39;ve known for some time and they are a powerful and incredibly useful tool for explainable AI. While many of you may be familiar with Shap values, I will take the time to contextualize where they come from, how they work, and how to use them in your projects. For the people who know what it is, you may want to check what you are actually using, while for those who are unfamiliar, I would recommend you use this as an opportunity to learn a new tool and acquire the criterion to choose when it may be useful. So, let&#39;s dive in!<!--]--></p><h2 id="some-game-theory-principles" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#some-game-theory-principles" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Some Game Theory principles<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->No, I&#39;m not throwing random topics at you. Actually, Game Theory is highly correlated to Shap values and now you&#39;ll know why.<!--]--></p><p data-v-63bfa697><!--[-->Game Theory is not a game. It is a branch of mathematics that aims to <strong data-v-9d7bd52e><!--[-->understand how people make decisions when their choices are influenced by others<!--]--></strong>. It helps us understand how different individuals or groups (and attention to that difference) interact strategically in a decision-making scenario. So it provides a powerful framework to model the behavior of rational agents. Game theory has many applications, from economics and politics to biology and psychology, and its concepts have been used to analyze and solve a variety of real-world problems.<!--]--></p><p data-v-63bfa697><!--[-->In essence, Game theory considers that the outcome of a game depends on the strategies chosen by all players involved. Each player has a set of possible actions, and each action leads to a specific outcome. The players aim to maximize their payoff, whether monetary or non-monetary (but quantizable anyway), by choosing a strategy that gives them the best chance of success. Game theory assumes that players are rational and aware of each other&#39;s strategies, and therefore, they act strategically to maximize their outcomes.<!--]--></p><p data-v-63bfa697><!--[-->Game theory provides several tools/scenarios for analyzing strategic interactions. An example of a Game Theory scenario is Nash equilibrium, where no player has the incentive to change their strategy.<!--]--></p><p data-v-63bfa697><!--[-->Another important concept in Game Theory is the idea of cooperative games, where players form coalitions and share the payoff. In cooperative games, the focus is on how to distribute the payoff among the players, which leads us to the concept of Shapley values in Game Theory.<!--]--></p><p data-v-63bfa697><!--[--><img src="https://www.gametheory.online/projects/1547107965.jpg" alt data-v-e5a4106d><!--]--></p><h2 id="shap-values-in-game-theory" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#shap-values-in-game-theory" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Shap values in Game Theory<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->Shapley values were born as a concept in cooperative Game Theory, aiming to distribute the payoff among the players fairly and efficiently. It was introduced by <a href="https://en.wikipedia.org/wiki/Lloyd_Shapley" rel="nofollow" data-v-af1c0c3b><!--[-->Lloyd Shapley<!--]--></a> in 1953 and has since become an important tool in game theory and Machine Learning. Shapley values take into account the contributions of each player to the coalition, considering all possible ways that the players can form a coalition. The idea is to calculate the marginal contribution of each player to the coalition, which is the difference between the payoff of the coalition with the player and without the player.<!--]--></p><p data-v-63bfa697><!--[-->The Shapley values provide a unique way to distribute the payoff among the players, based on their contributions to the coalition. They are widely used in cooperative game theory to solve problems such as resource allocation, cost sharing, and voting systems.<!--]--></p><p data-v-63bfa697><!--[--><img src="https://miro.medium.com/v2/resize:fit:1400/1*AzGc8wSKrP7TzLh84N8Lcg.png" alt data-v-e5a4106d><!--]--></p><h2 id="from-game-theory-to-predictive-modeling" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#from-game-theory-to-predictive-modeling" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->From Game Theory to predictive modeling<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->In machine learning, Shapley values are used to explain the output of a model by assigning a contribution to each feature of the input. This allows us to understand the importance of each feature in the model&#39;s decision and to identify any biases or inconsistencies. Shapley values have become an essential tool in the field of explainable AI, providing a way to interpret the output of complex models and ensuring transparency and fairness in their decision-making processes.<!--]--></p><p data-v-63bfa697><!--[-->An important note on Shap values is that they are strictly detached from the model. This also gives it a Model-Agnostic nature which is a crucial feature to have (in some scenarios I would say unnegotiable).<!--]--></p><h2 id="an-idea-of-how-are-shap-values-computed" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#an-idea-of-how-are-shap-values-computed" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->An idea of how are SHAP values computed<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->The scope of this post is not to be a tutorial, but in order to get a better idea of Shap values, there are some key ideas on the computation:<!--]--></p><ul data-v-5feda7b5><!--[--><li data-v-996e086c><!--[-->Shapley values are computed by considering all possible permutations of players in a coalition<!--]--></li><li data-v-996e086c><!--[-->The marginal contribution of each player is calculated for each permutation<!--]--></li><li data-v-996e086c><!--[-->The marginal contribution is the difference in the value of the coalition with and without the player, considering all possible subsets of the coalition<!--]--></li><li data-v-996e086c><!--[-->The formula for computing Shapley values involves averaging the marginal contributions over all possible permutations of players in the coalition, which can be time-consuming for larger datasets and complex models<!--]--></li><li data-v-996e086c><!--[-->Efficient algorithms such as the Monte Carlo method and Shapley value regression have been developed to speed up the computation of Shapley values<!--]--></li><li data-v-996e086c><!--[-->The Shapley value regression uses a linear regression model to approximate the Shapley values based on a set of inputs and their associated outcomes<!--]--></li><!--]--></ul><h2 id="shap-values-starter-pack" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#shap-values-starter-pack" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Shap values starter pack<!--]--></strong><!--]--><!----></a></h2><ul data-v-5feda7b5><!--[--><li data-v-996e086c><!--[-->Python&#39;s library <em data-v-177b5f01><!--[-->shap<!--]--></em> (<code data-v-c81ed8f1><!--[-->pip install shap<!--]--></code>)<!--]--></li><!--]--></ul><p data-v-63bfa697><!--[-->That&#39;s all. Yes, python already offers you a package with all you need. This, with a good tutorial on Shap values usage, should be enough to use in your project.<!--]--></p><p data-v-63bfa697><!--[--><img src="https://shap.readthedocs.io/en/latest/_images/shap_header.png" alt data-v-e5a4106d><!--]--></p><h2 id="the-bad-side" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#the-bad-side" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->The &quot;bad&quot; side<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->Although Shap values are a simple yet powerful tool for explaining ML predictive models, they still require the user to understand their value or otherwise they will be giving empty insights.<!--]--></p><p data-v-63bfa697><!--[-->For example, don&#39;t you miss here any mention of Computer Vision, NLP, or other fields? Well, Shap values are especially valuable for exploring the contribution of raw features in a model. However, in such fields, we already know a complex feature structure that the models will learn from. For example, in Computer Vision the most granular features are the image pixels (in a single channel, single frame). However, as any Computer Vision community member knows, to be feasible instead of feeding a linear model with the raw pixels, we use (normally) convolutional models over a set of values with a predefined relation among them. This relation is above any insight that Shap values can give since it comes from the nature of our knowledge, and Shap values will intuitively not give any valuable insight from this kind of feature. In the case of CNN&#39;s, there are other ways to interpret and explain models, such as filter visualization or more automatic tools (which deserve a specific topic).<!--]--></p><p data-v-63bfa697><!--[-->Does that mean (in this case) that Shapley values have no place in Computer Vision? Obviously not, there are other elements in Computer Vision that interact in a more raw manner, such as channels or more concrete elements in custom pipelines (such as instances in some intermediate stages in tasks like captioning, etc).<!--]--></p><p data-v-63bfa697><!--[-->And the same applies in other fields, such as NLP, where the elements also have their particular relations.<!--]--></p><p data-v-63bfa697><!--[--><img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" alt data-v-e5a4106d><!--]--></p><h2 id="summarizing" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#summarizing" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Summarizing<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->Shapley values are a valuable tool, inspired by cooperative Game theory (where they represent each team member&#39;s contribution to the outcome) and adapted to ML predictive modeling in order to explain the contribution of different elements (such as features) to a given prediction. There are tools to compute them efficiently and should be considered for explainable AI, especially where no relation is preset among these elements (e.g. raw columns in tabular data).<!--]--></p><h2 id="thank-you-reader" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#thank-you-reader" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->Thank you, reader<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[-->Thank you again for reading this post! As you know any feedback is welcome. I really hope we can meet soon this time.<!--]--></p><h2 id="references" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#references" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[--><strong data-v-9d7bd52e><!--[-->References<!--]--></strong><!--]--><!----></a></h2><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>1</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://www.kaggle.com/code/dansbecker/shap-values" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Using SHAP Values to Explain How Your Machine Learning Model Works<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>2</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://towardsdatascience.com/explainable-ai-application-of-shapely-values-in-marketing-analytics-57b716fc9d1f" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Explainable AI: Application of shapely values in Marketing Analytics<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>3</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://dev.to/mage_ai/how-to-interpret-machine-learning-models-with-shap-values-54jf#:~:text=SHAP%20values%20can%20be%20used,to%20explain%20limited%20model%20types." rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->How to interpret machine learning models with SHAP values<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>4</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://plato.stanford.edu/entries/game-theory/" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Game Theory<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>5</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://www.skillsyouneed.com/lead/game-theory.html" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Understanding Game Theory<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>6</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://www.statcan.gc.ca/en/data-science/network/explainable-learning" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Explainable Machine Learning, Game Theory, and Shapley Values: A technical review<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>7</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Deep Learning Model Interpretation Using SHAP<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><h2 id="some-resources" data-v-1daf0210><a aria-current="page" href="/articles/2023-04-14-short-shap#some-resources" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[-->Some resources<!--]--><!----></a></h2><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>1</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://www.kaggle.com/code/dansbecker/shap-values" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->Kaggle Shap tutorial<!--]--></em><!--]--></a><!--]--></strong><!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[--><span>2</span><!--]--></em> <strong data-v-9d7bd52e><!--[--><a href="https://shap.readthedocs.io/en/latest/index.html" rel="nofollow" data-v-af1c0c3b><!--[--><em data-v-177b5f01><!--[-->SHAP library documentation<!--]--></em><!--]--></a><!--]--></strong><!--]--></p></div><!--]--><div class="back-to-top" data-v-f252e39d><a data-v-f252e39d data-v-af1c0c3b><!--[-->Back to top <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-f252e39d style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M11 20V7.825l-5.6 5.6L4 12l8-8l8 8l-1.4 1.425l-5.6-5.6V20h-2Z"/></svg><!--]--></a></div></div></article></div><!--]--><footer class="" data-v-93c22c3b data-v-d63b5c07><a href="https://github.com/Metabloggism/metabloggism.github.io" rel="noopener noreferrer" class="credits" data-v-d63b5c07>Metabloggism</a><div class="navigation" data-v-d63b5c07><nav data-v-d63b5c07 data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div><p class="message" data-v-d63b5c07>Follow me on</p><div class="icons" data-v-d63b5c07><div class="social" data-v-d63b5c07><!--[--><a href="https://github.com/https://github.com/MrLeylo" rel="noopener noreferrer" target="_blank" title="https://github.com/MrLeylo" aria-label="https://github.com/MrLeylo" data-v-ddf2f94a><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-ddf2f94a style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M12 2.247a10 10 0 0 0-3.162 19.487c.5.088.687-.212.687-.475c0-.237-.012-1.025-.012-1.862c-2.513.462-3.163-.613-3.363-1.175a3.636 3.636 0 0 0-1.025-1.413c-.35-.187-.85-.65-.013-.662a2.001 2.001 0 0 1 1.538 1.025a2.137 2.137 0 0 0 2.912.825a2.104 2.104 0 0 1 .638-1.338c-2.225-.25-4.55-1.112-4.55-4.937a3.892 3.892 0 0 1 1.025-2.688a3.594 3.594 0 0 1 .1-2.65s.837-.262 2.75 1.025a9.427 9.427 0 0 1 5 0c1.912-1.3 2.75-1.025 2.75-1.025a3.593 3.593 0 0 1 .1 2.65a3.869 3.869 0 0 1 1.025 2.688c0 3.837-2.338 4.687-4.563 4.937a2.368 2.368 0 0 1 .675 1.85c0 1.338-.012 2.413-.012 2.75c0 .263.187.575.687.475A10.005 10.005 0 0 0 12 2.247Z"/></svg></a><a href="https://www.linkedin.com/company/nuxtlabs" rel="noopener noreferrer" target="_blank" title="LinkedIn" aria-label="LinkedIn" data-v-ddf2f94a><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-ddf2f94a style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M20.47 2H3.53a1.45 1.45 0 0 0-1.47 1.43v17.14A1.45 1.45 0 0 0 3.53 22h16.94a1.45 1.45 0 0 0 1.47-1.43V3.43A1.45 1.45 0 0 0 20.47 2ZM8.09 18.74h-3v-9h3ZM6.59 8.48a1.56 1.56 0 1 1 0-3.12a1.57 1.57 0 1 1 0 3.12Zm12.32 10.26h-3v-4.83c0-1.21-.43-2-1.52-2A1.65 1.65 0 0 0 12.85 13a2 2 0 0 0-.1.73v5h-3v-9h3V11a3 3 0 0 1 2.71-1.5c2 0 3.45 1.29 3.45 4.06Z"/></svg></a><!--]--></div><div class="color-mode-switch" data-v-d63b5c07><button aria-label="Color Mode" data-v-d63b5c07 data-v-693a1e33><span data-v-693a1e33></span></button></div></div></footer><!--]--></div></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true" data-src="/articles/2023-04-14-short-shap/_payload.json">[{"state":1,"_errors":1609,"serverRendered":1567,"path":7,"prerenderedAt":-1},["Reactive",2],{"$sdd-pages":3,"$sdd-surrounds":571,"$sdd-globals":1563,"$scolor-mode":1565,"$sdd-navigation":1568,"$sicons":1598},["ShallowRef",4],["ShallowReactive",5],{"/articles/2023-04-14-short-shap":6},{"_path":7,"_dir":8,"_draft":9,"_partial":9,"_locale":10,"title":11,"description":12,"date":13,"layout":14,"body":15,"_type":566,"_id":567,"_source":568,"_file":569,"_extension":570},"/articles/2023-04-14-short-shap","articles",false,"","Short: Shap values","Hello readers, it has been quite some time since our last blog post, and for that, I apologize. Due to personal and work matters, I have been away for almost two months. However, I am excited to be back and share with you a topic that I think you will find interesting: Shap values.","2023-04-14T00:00:00.000Z","article",{"type":16,"children":17,"toc":552},"root",[18,30,35,45,50,59,64,76,81,86,91,99,108,124,129,136,145,150,155,164,169,204,213,237,242,249,258,263,268,273,278,285,294,299,308,313,322,349,374,399,424,449,474,499,505,528],{"type":19,"tag":20,"props":21,"children":23},"element","h1",{"id":22},"short-shap-values",[24],{"type":19,"tag":25,"props":26,"children":27},"strong",{},[28],{"type":29,"value":11},"text",{"type":19,"tag":31,"props":32,"children":33},"p",{},[34],{"type":29,"value":12},{"type":19,"tag":36,"props":37,"children":39},"h2",{"id":38},"motivation-why-is-this-todays-topic",[40],{"type":19,"tag":25,"props":41,"children":42},{},[43],{"type":29,"value":44},"Motivation: Why is this today's topic?",{"type":19,"tag":31,"props":46,"children":47},{},[48],{"type":29,"value":49},"I recently used Shap values in a project and found it to be a chance to tell you something of interest. Shap values are something I've known for some time and they are a powerful and incredibly useful tool for explainable AI. While many of you may be familiar with Shap values, I will take the time to contextualize where they come from, how they work, and how to use them in your projects. For the people who know what it is, you may want to check what you are actually using, while for those who are unfamiliar, I would recommend you use this as an opportunity to learn a new tool and acquire the criterion to choose when it may be useful. So, let's dive in!",{"type":19,"tag":36,"props":51,"children":53},{"id":52},"some-game-theory-principles",[54],{"type":19,"tag":25,"props":55,"children":56},{},[57],{"type":29,"value":58},"Some Game Theory principles",{"type":19,"tag":31,"props":60,"children":61},{},[62],{"type":29,"value":63},"No, I'm not throwing random topics at you. Actually, Game Theory is highly correlated to Shap values and now you'll know why.",{"type":19,"tag":31,"props":65,"children":66},{},[67,69,74],{"type":29,"value":68},"Game Theory is not a game. It is a branch of mathematics that aims to ",{"type":19,"tag":25,"props":70,"children":71},{},[72],{"type":29,"value":73},"understand how people make decisions when their choices are influenced by others",{"type":29,"value":75},". It helps us understand how different individuals or groups (and attention to that difference) interact strategically in a decision-making scenario. So it provides a powerful framework to model the behavior of rational agents. Game theory has many applications, from economics and politics to biology and psychology, and its concepts have been used to analyze and solve a variety of real-world problems.",{"type":19,"tag":31,"props":77,"children":78},{},[79],{"type":29,"value":80},"In essence, Game theory considers that the outcome of a game depends on the strategies chosen by all players involved. Each player has a set of possible actions, and each action leads to a specific outcome. The players aim to maximize their payoff, whether monetary or non-monetary (but quantizable anyway), by choosing a strategy that gives them the best chance of success. Game theory assumes that players are rational and aware of each other's strategies, and therefore, they act strategically to maximize their outcomes.",{"type":19,"tag":31,"props":82,"children":83},{},[84],{"type":29,"value":85},"Game theory provides several tools/scenarios for analyzing strategic interactions. An example of a Game Theory scenario is Nash equilibrium, where no player has the incentive to change their strategy.",{"type":19,"tag":31,"props":87,"children":88},{},[89],{"type":29,"value":90},"Another important concept in Game Theory is the idea of cooperative games, where players form coalitions and share the payoff. In cooperative games, the focus is on how to distribute the payoff among the players, which leads us to the concept of Shapley values in Game Theory.",{"type":19,"tag":31,"props":92,"children":93},{},[94],{"type":19,"tag":95,"props":96,"children":98},"img",{"alt":10,"src":97},"https://www.gametheory.online/projects/1547107965.jpg",[],{"type":19,"tag":36,"props":100,"children":102},{"id":101},"shap-values-in-game-theory",[103],{"type":19,"tag":25,"props":104,"children":105},{},[106],{"type":29,"value":107},"Shap values in Game Theory",{"type":19,"tag":31,"props":109,"children":110},{},[111,113,122],{"type":29,"value":112},"Shapley values were born as a concept in cooperative Game Theory, aiming to distribute the payoff among the players fairly and efficiently. It was introduced by ",{"type":19,"tag":114,"props":115,"children":119},"a",{"href":116,"rel":117},"https://en.wikipedia.org/wiki/Lloyd_Shapley",[118],"nofollow",[120],{"type":29,"value":121},"Lloyd Shapley",{"type":29,"value":123}," in 1953 and has since become an important tool in game theory and Machine Learning. Shapley values take into account the contributions of each player to the coalition, considering all possible ways that the players can form a coalition. The idea is to calculate the marginal contribution of each player to the coalition, which is the difference between the payoff of the coalition with the player and without the player.",{"type":19,"tag":31,"props":125,"children":126},{},[127],{"type":29,"value":128},"The Shapley values provide a unique way to distribute the payoff among the players, based on their contributions to the coalition. They are widely used in cooperative game theory to solve problems such as resource allocation, cost sharing, and voting systems.",{"type":19,"tag":31,"props":130,"children":131},{},[132],{"type":19,"tag":95,"props":133,"children":135},{"alt":10,"src":134},"https://miro.medium.com/v2/resize:fit:1400/1*AzGc8wSKrP7TzLh84N8Lcg.png",[],{"type":19,"tag":36,"props":137,"children":139},{"id":138},"from-game-theory-to-predictive-modeling",[140],{"type":19,"tag":25,"props":141,"children":142},{},[143],{"type":29,"value":144},"From Game Theory to predictive modeling",{"type":19,"tag":31,"props":146,"children":147},{},[148],{"type":29,"value":149},"In machine learning, Shapley values are used to explain the output of a model by assigning a contribution to each feature of the input. This allows us to understand the importance of each feature in the model's decision and to identify any biases or inconsistencies. Shapley values have become an essential tool in the field of explainable AI, providing a way to interpret the output of complex models and ensuring transparency and fairness in their decision-making processes.",{"type":19,"tag":31,"props":151,"children":152},{},[153],{"type":29,"value":154},"An important note on Shap values is that they are strictly detached from the model. This also gives it a Model-Agnostic nature which is a crucial feature to have (in some scenarios I would say unnegotiable).",{"type":19,"tag":36,"props":156,"children":158},{"id":157},"an-idea-of-how-are-shap-values-computed",[159],{"type":19,"tag":25,"props":160,"children":161},{},[162],{"type":29,"value":163},"An idea of how are SHAP values computed",{"type":19,"tag":31,"props":165,"children":166},{},[167],{"type":29,"value":168},"The scope of this post is not to be a tutorial, but in order to get a better idea of Shap values, there are some key ideas on the computation:",{"type":19,"tag":170,"props":171,"children":172},"ul",{},[173,179,184,189,194,199],{"type":19,"tag":174,"props":175,"children":176},"li",{},[177],{"type":29,"value":178},"Shapley values are computed by considering all possible permutations of players in a coalition",{"type":19,"tag":174,"props":180,"children":181},{},[182],{"type":29,"value":183},"The marginal contribution of each player is calculated for each permutation",{"type":19,"tag":174,"props":185,"children":186},{},[187],{"type":29,"value":188},"The marginal contribution is the difference in the value of the coalition with and without the player, considering all possible subsets of the coalition",{"type":19,"tag":174,"props":190,"children":191},{},[192],{"type":29,"value":193},"The formula for computing Shapley values involves averaging the marginal contributions over all possible permutations of players in the coalition, which can be time-consuming for larger datasets and complex models",{"type":19,"tag":174,"props":195,"children":196},{},[197],{"type":29,"value":198},"Efficient algorithms such as the Monte Carlo method and Shapley value regression have been developed to speed up the computation of Shapley values",{"type":19,"tag":174,"props":200,"children":201},{},[202],{"type":29,"value":203},"The Shapley value regression uses a linear regression model to approximate the Shapley values based on a set of inputs and their associated outcomes",{"type":19,"tag":36,"props":205,"children":207},{"id":206},"shap-values-starter-pack",[208],{"type":19,"tag":25,"props":209,"children":210},{},[211],{"type":29,"value":212},"Shap values starter pack",{"type":19,"tag":170,"props":214,"children":215},{},[216],{"type":19,"tag":174,"props":217,"children":218},{},[219,221,227,229,235],{"type":29,"value":220},"Python's library ",{"type":19,"tag":222,"props":223,"children":224},"em",{},[225],{"type":29,"value":226},"shap",{"type":29,"value":228}," (",{"type":19,"tag":230,"props":231,"children":232},"code",{},[233],{"type":29,"value":234},"pip install shap",{"type":29,"value":236},")",{"type":19,"tag":31,"props":238,"children":239},{},[240],{"type":29,"value":241},"That's all. Yes, python already offers you a package with all you need. This, with a good tutorial on Shap values usage, should be enough to use in your project.",{"type":19,"tag":31,"props":243,"children":244},{},[245],{"type":19,"tag":95,"props":246,"children":248},{"alt":10,"src":247},"https://shap.readthedocs.io/en/latest/_images/shap_header.png",[],{"type":19,"tag":36,"props":250,"children":252},{"id":251},"the-bad-side",[253],{"type":19,"tag":25,"props":254,"children":255},{},[256],{"type":29,"value":257},"The \"bad\" side",{"type":19,"tag":31,"props":259,"children":260},{},[261],{"type":29,"value":262},"Although Shap values are a simple yet powerful tool for explaining ML predictive models, they still require the user to understand their value or otherwise they will be giving empty insights.",{"type":19,"tag":31,"props":264,"children":265},{},[266],{"type":29,"value":267},"For example, don't you miss here any mention of Computer Vision, NLP, or other fields? Well, Shap values are especially valuable for exploring the contribution of raw features in a model. However, in such fields, we already know a complex feature structure that the models will learn from. For example, in Computer Vision the most granular features are the image pixels (in a single channel, single frame). However, as any Computer Vision community member knows, to be feasible instead of feeding a linear model with the raw pixels, we use (normally) convolutional models over a set of values with a predefined relation among them. This relation is above any insight that Shap values can give since it comes from the nature of our knowledge, and Shap values will intuitively not give any valuable insight from this kind of feature. In the case of CNN's, there are other ways to interpret and explain models, such as filter visualization or more automatic tools (which deserve a specific topic).",{"type":19,"tag":31,"props":269,"children":270},{},[271],{"type":29,"value":272},"Does that mean (in this case) that Shapley values have no place in Computer Vision? Obviously not, there are other elements in Computer Vision that interact in a more raw manner, such as channels or more concrete elements in custom pipelines (such as instances in some intermediate stages in tasks like captioning, etc).",{"type":19,"tag":31,"props":274,"children":275},{},[276],{"type":29,"value":277},"And the same applies in other fields, such as NLP, where the elements also have their particular relations.",{"type":19,"tag":31,"props":279,"children":280},{},[281],{"type":19,"tag":95,"props":282,"children":284},{"alt":10,"src":283},"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png",[],{"type":19,"tag":36,"props":286,"children":288},{"id":287},"summarizing",[289],{"type":19,"tag":25,"props":290,"children":291},{},[292],{"type":29,"value":293},"Summarizing",{"type":19,"tag":31,"props":295,"children":296},{},[297],{"type":29,"value":298},"Shapley values are a valuable tool, inspired by cooperative Game theory (where they represent each team member's contribution to the outcome) and adapted to ML predictive modeling in order to explain the contribution of different elements (such as features) to a given prediction. There are tools to compute them efficiently and should be considered for explainable AI, especially where no relation is preset among these elements (e.g. raw columns in tabular data).",{"type":19,"tag":36,"props":300,"children":302},{"id":301},"thank-you-reader",[303],{"type":19,"tag":25,"props":304,"children":305},{},[306],{"type":29,"value":307},"Thank you, reader",{"type":19,"tag":31,"props":309,"children":310},{},[311],{"type":29,"value":312},"Thank you again for reading this post! As you know any feedback is welcome. I really hope we can meet soon this time.",{"type":19,"tag":36,"props":314,"children":316},{"id":315},"references",[317],{"type":19,"tag":25,"props":318,"children":319},{},[320],{"type":29,"value":321},"References",{"type":19,"tag":31,"props":323,"children":324},{},[325,334,336],{"type":19,"tag":222,"props":326,"children":327},{},[328],{"type":19,"tag":329,"props":330,"children":331},"span",{},[332],{"type":29,"value":333},"1",{"type":29,"value":335}," ",{"type":19,"tag":25,"props":337,"children":338},{},[339],{"type":19,"tag":114,"props":340,"children":343},{"href":341,"rel":342},"https://www.kaggle.com/code/dansbecker/shap-values",[118],[344],{"type":19,"tag":222,"props":345,"children":346},{},[347],{"type":29,"value":348},"Using SHAP Values to Explain How Your Machine Learning Model Works",{"type":19,"tag":31,"props":350,"children":351},{},[352,360,361],{"type":19,"tag":222,"props":353,"children":354},{},[355],{"type":19,"tag":329,"props":356,"children":357},{},[358],{"type":29,"value":359},"2",{"type":29,"value":335},{"type":19,"tag":25,"props":362,"children":363},{},[364],{"type":19,"tag":114,"props":365,"children":368},{"href":366,"rel":367},"https://towardsdatascience.com/explainable-ai-application-of-shapely-values-in-marketing-analytics-57b716fc9d1f",[118],[369],{"type":19,"tag":222,"props":370,"children":371},{},[372],{"type":29,"value":373},"Explainable AI: Application of shapely values in Marketing Analytics",{"type":19,"tag":31,"props":375,"children":376},{},[377,385,386],{"type":19,"tag":222,"props":378,"children":379},{},[380],{"type":19,"tag":329,"props":381,"children":382},{},[383],{"type":29,"value":384},"3",{"type":29,"value":335},{"type":19,"tag":25,"props":387,"children":388},{},[389],{"type":19,"tag":114,"props":390,"children":393},{"href":391,"rel":392},"https://dev.to/mage_ai/how-to-interpret-machine-learning-models-with-shap-values-54jf#:~:text=SHAP%20values%20can%20be%20used,to%20explain%20limited%20model%20types.",[118],[394],{"type":19,"tag":222,"props":395,"children":396},{},[397],{"type":29,"value":398},"How to interpret machine learning models with SHAP values",{"type":19,"tag":31,"props":400,"children":401},{},[402,410,411],{"type":19,"tag":222,"props":403,"children":404},{},[405],{"type":19,"tag":329,"props":406,"children":407},{},[408],{"type":29,"value":409},"4",{"type":29,"value":335},{"type":19,"tag":25,"props":412,"children":413},{},[414],{"type":19,"tag":114,"props":415,"children":418},{"href":416,"rel":417},"https://plato.stanford.edu/entries/game-theory/",[118],[419],{"type":19,"tag":222,"props":420,"children":421},{},[422],{"type":29,"value":423},"Game Theory",{"type":19,"tag":31,"props":425,"children":426},{},[427,435,436],{"type":19,"tag":222,"props":428,"children":429},{},[430],{"type":19,"tag":329,"props":431,"children":432},{},[433],{"type":29,"value":434},"5",{"type":29,"value":335},{"type":19,"tag":25,"props":437,"children":438},{},[439],{"type":19,"tag":114,"props":440,"children":443},{"href":441,"rel":442},"https://www.skillsyouneed.com/lead/game-theory.html",[118],[444],{"type":19,"tag":222,"props":445,"children":446},{},[447],{"type":29,"value":448},"Understanding Game Theory",{"type":19,"tag":31,"props":450,"children":451},{},[452,460,461],{"type":19,"tag":222,"props":453,"children":454},{},[455],{"type":19,"tag":329,"props":456,"children":457},{},[458],{"type":29,"value":459},"6",{"type":29,"value":335},{"type":19,"tag":25,"props":462,"children":463},{},[464],{"type":19,"tag":114,"props":465,"children":468},{"href":466,"rel":467},"https://www.statcan.gc.ca/en/data-science/network/explainable-learning",[118],[469],{"type":19,"tag":222,"props":470,"children":471},{},[472],{"type":29,"value":473},"Explainable Machine Learning, Game Theory, and Shapley Values: A technical review",{"type":19,"tag":31,"props":475,"children":476},{},[477,485,486],{"type":19,"tag":222,"props":478,"children":479},{},[480],{"type":19,"tag":329,"props":481,"children":482},{},[483],{"type":29,"value":484},"7",{"type":29,"value":335},{"type":19,"tag":25,"props":487,"children":488},{},[489],{"type":19,"tag":114,"props":490,"children":493},{"href":491,"rel":492},"https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16",[118],[494],{"type":19,"tag":222,"props":495,"children":496},{},[497],{"type":29,"value":498},"Deep Learning Model Interpretation Using SHAP",{"type":19,"tag":36,"props":500,"children":502},{"id":501},"some-resources",[503],{"type":29,"value":504},"Some resources",{"type":19,"tag":31,"props":506,"children":507},{},[508,515,516],{"type":19,"tag":222,"props":509,"children":510},{},[511],{"type":19,"tag":329,"props":512,"children":513},{},[514],{"type":29,"value":333},{"type":29,"value":335},{"type":19,"tag":25,"props":517,"children":518},{},[519],{"type":19,"tag":114,"props":520,"children":522},{"href":341,"rel":521},[118],[523],{"type":19,"tag":222,"props":524,"children":525},{},[526],{"type":29,"value":527},"Kaggle Shap tutorial",{"type":19,"tag":31,"props":529,"children":530},{},[531,538,539],{"type":19,"tag":222,"props":532,"children":533},{},[534],{"type":19,"tag":329,"props":535,"children":536},{},[537],{"type":29,"value":359},{"type":29,"value":335},{"type":19,"tag":25,"props":540,"children":541},{},[542],{"type":19,"tag":114,"props":543,"children":546},{"href":544,"rel":545},"https://shap.readthedocs.io/en/latest/index.html",[118],[547],{"type":19,"tag":222,"props":548,"children":549},{},[550],{"type":29,"value":551},"SHAP library documentation",{"title":10,"searchDepth":553,"depth":553,"links":554},2,[555,556,557,558,559,560,561,562,563,564,565],{"id":38,"depth":553,"text":44},{"id":52,"depth":553,"text":58},{"id":101,"depth":553,"text":107},{"id":138,"depth":553,"text":144},{"id":157,"depth":553,"text":163},{"id":206,"depth":553,"text":212},{"id":251,"depth":553,"text":257},{"id":287,"depth":553,"text":293},{"id":301,"depth":553,"text":307},{"id":315,"depth":553,"text":321},{"id":501,"depth":553,"text":504},"markdown","content:articles:2023-04-14-SHORT-shap.md","content","articles/2023-04-14-SHORT-shap.md","md",["ShallowRef",572],["ShallowReactive",573],{"/articles/2023-04-14-short-shap":574},[575,1562],{"_path":576,"_dir":8,"_draft":9,"_partial":9,"_locale":10,"title":577,"description":578,"date":579,"layout":14,"body":580,"_type":566,"_id":1560,"_source":568,"_file":1561,"_extension":570},"/articles/2023-02-09-short-onnx","Short: ONNX","This is the first SHORT post in Metabloggism. Feel free to contact me to give any feedback!","2023-02-09T00:00:00.000Z",{"type":16,"children":581,"toc":1541},[582,590,602,611,623,632,646,658,663,670,679,684,689,694,702,711,730,753,762,776,784,815,824,838,852,857,865,874,888,897,904,909,916,922,927,935,951,960,974,1462,1468,1479,1487,1493,1498,1515,1521,1526,1530,1535],{"type":19,"tag":20,"props":583,"children":585},{"id":584},"short-onnx",[586],{"type":19,"tag":25,"props":587,"children":588},{},[589],{"type":29,"value":577},{"type":19,"tag":31,"props":591,"children":592},{},[593,595,600],{"type":29,"value":594},"This is the first ",{"type":19,"tag":25,"props":596,"children":597},{},[598],{"type":29,"value":599},"SHORT",{"type":29,"value":601}," post in Metabloggism. Feel free to contact me to give any feedback!",{"type":19,"tag":36,"props":603,"children":605},{"id":604},"the-questions",[606],{"type":19,"tag":25,"props":607,"children":608},{},[609],{"type":29,"value":610},"The questions",{"type":19,"tag":31,"props":612,"children":613},{},[614,616,621],{"type":29,"value":615},"Have you heard the name ",{"type":19,"tag":222,"props":617,"children":618},{},[619],{"type":29,"value":620},"ONNX",{"type":29,"value":622},"? If you have, do you understand the reason why it exists? In this Short I will try to cover the interest to any answer to the previous questions. For those who haven't heard it, read it as if I'm introducing you a tool that may or not be of interest (and that will be your decision). For those who have, but have different levels of knowledge about it, I pretend to cover the gaps in your knowledge and contextualize why you are hearing this. And for those who know it, maybe you are not the main target of the Short and you may decide to skip it, but if still you follow, take it as a review/contrast with your knowledge as well as an opportunity to think about alternative ways to take profit from it.",{"type":19,"tag":36,"props":624,"children":626},{"id":625},"definition-of-onnx",[627],{"type":19,"tag":25,"props":628,"children":629},{},[630],{"type":29,"value":631},"Definition of ONNX",{"type":19,"tag":31,"props":633,"children":634},{},[635,637,644],{"type":29,"value":636},"Instead of talking about others, why don't I let ONNX present itself? From the ",{"type":19,"tag":114,"props":638,"children":641},{"href":639,"rel":640},"https://onnx.ai/",[118],[642],{"type":29,"value":643},"official website",{"type":29,"value":645},":",{"type":19,"tag":647,"props":648,"children":649},"blockquote",{},[650],{"type":19,"tag":31,"props":651,"children":652},{},[653],{"type":19,"tag":222,"props":654,"children":655},{},[656],{"type":29,"value":657},"ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers",{"type":19,"tag":31,"props":659,"children":660},{},[661],{"type":29,"value":662},"Ok, but the blog has the task of explaining it still. In my words, ONNX is a standard format for ML models that delivers a library with conversion tools to and from most of the main ML frameworks. It is also built thinking in users both experimenting and seeking optimization and therefore supports both training and runtime frameworks.",{"type":19,"tag":31,"props":664,"children":665},{},[666],{"type":19,"tag":95,"props":667,"children":669},{"alt":620,"src":668},"https://miro.medium.com/max/720/0*o5doPyWdduatUKtX.PNG",[],{"type":19,"tag":36,"props":671,"children":673},{"id":672},"where-does-onnx-come-from",[674],{"type":19,"tag":25,"props":675,"children":676},{},[677],{"type":29,"value":678},"Where does ONNX come from?",{"type":19,"tag":31,"props":680,"children":681},{},[682],{"type":29,"value":683},"ONNX was the result of a collaboration between big companies. Initially developed by PyTorch team (Facebook), Microsoft and AWS joined in different ways as Co-Founders. They announced it in 2017 as a step further toward a common goal: collaborative development of AI.",{"type":19,"tag":31,"props":685,"children":686},{},[687],{"type":29,"value":688},"This is one key concept as it allows AI researchers/developers to exploit their advances by destroying some limitations given by the differences between frameworks, thus giving an easier access to the AI community. AI research and divulgation (like what this blog aims to do) also point in that direction.",{"type":19,"tag":31,"props":690,"children":691},{},[692],{"type":29,"value":693},"Soon, almost all the main AI frameworks/platforms began giving support to ONNX.",{"type":19,"tag":31,"props":695,"children":696},{},[697],{"type":19,"tag":95,"props":698,"children":701},{"alt":699,"src":700},"Bigtechs","https://i.imgur.com/eHeJ47l.png",[],{"type":19,"tag":36,"props":703,"children":705},{"id":704},"onnx-strategy",[706],{"type":19,"tag":25,"props":707,"children":708},{},[709],{"type":29,"value":710},"ONNX strategy",{"type":19,"tag":31,"props":712,"children":713},{},[714,716,721,723,728],{"type":29,"value":715},"To support interoperability, ONNX relies in instead of being a serialization method (like ",{"type":19,"tag":222,"props":717,"children":718},{},[719],{"type":29,"value":720},"JSON",{"type":29,"value":722}," or ",{"type":19,"tag":222,"props":724,"children":725},{},[726],{"type":29,"value":727},"pkl",{"type":29,"value":729},"), where a deserialization is mandatory and that could depend on the framework (in addition to the extra deserialization step resource consumption), directly stores the raw operations.",{"type":19,"tag":31,"props":731,"children":732},{},[733,735,739,741,751],{"type":29,"value":734},"These raw operations are part of the ",{"type":19,"tag":222,"props":736,"children":737},{},[738],{"type":29,"value":620},{"type":29,"value":740}," standard in a collection called ",{"type":19,"tag":114,"props":742,"children":745},{"href":743,"rel":744},"https://github.com/onnx/onnx/blob/main/docs/Operators.md",[118],[746],{"type":19,"tag":222,"props":747,"children":748},{},[749],{"type":29,"value":750},"opset",{"type":29,"value":752},".",{"type":19,"tag":36,"props":754,"children":756},{"id":755},"onnx-ml-frameworks-support",[757],{"type":19,"tag":25,"props":758,"children":759},{},[760],{"type":29,"value":761},"ONNX ML frameworks support",{"type":19,"tag":31,"props":763,"children":764},{},[765,767,774],{"type":29,"value":766},"The main (in development) ML frameworks support ONNX through some API. The following image (",{"type":19,"tag":114,"props":768,"children":771},{"href":769,"rel":770},"https://onnx.ai/supported-tools.html#buildModel",[118],[772],{"type":29,"value":773},"source",{"type":29,"value":775},") shows some of the frameworks that support ONNX:",{"type":19,"tag":31,"props":777,"children":778},{},[779],{"type":19,"tag":95,"props":780,"children":783},{"alt":781,"src":782},"ML frameworks","https://i.imgur.com/MFt84AK.png",[],{"type":19,"tag":31,"props":785,"children":786},{},[787,789,796,797,804,806,813],{"type":29,"value":788},"Note that not only Neural Networks are included, but also boosting libraries like ",{"type":19,"tag":114,"props":790,"children":793},{"href":791,"rel":792},"https://xgboost.readthedocs.io/en/stable/",[118],[794],{"type":29,"value":795},"XGBoost",{"type":29,"value":722},{"type":19,"tag":114,"props":798,"children":801},{"href":799,"rel":800},"https://catboost.ai/",[118],[802],{"type":29,"value":803},"CatBoost",{"type":29,"value":805},", ",{"type":19,"tag":114,"props":807,"children":810},{"href":808,"rel":809},"https://scikit-learn.org/stable/",[118],[811],{"type":29,"value":812},"SciKit-Learn",{"type":29,"value":814}," (which includes several classical algorithms), ...",{"type":19,"tag":36,"props":816,"children":818},{"id":817},"onnx-runtime-engines-support",[819],{"type":19,"tag":25,"props":820,"children":821},{},[822],{"type":29,"value":823},"ONNX Runtime engines support",{"type":19,"tag":31,"props":825,"children":826},{},[827,829,836],{"type":29,"value":828},"ONNX supports different Runtime engines to allow the users to run the models on their own specific hardware. For example, one could have a Pytorch trained model and wants to include it in an iOS app, but that demands to use ",{"type":19,"tag":114,"props":830,"children":833},{"href":831,"rel":832},"https://developer.apple.com/documentation/coreml",[118],[834],{"type":29,"value":835},"CoreML",{"type":29,"value":837},". As CoreML is included in the supported frameworks, that could be done without issues through ONNX. That applies to any hardware with a runtime engine supporting ONNX. Do you want your model in a smart fridge? You will need a model runtime engine on the fridge. Just include support to ONNX and you will be able to run your PyTorch models.",{"type":19,"tag":31,"props":839,"children":840},{},[841,843,850],{"type":29,"value":842},"However, ONNX itself is independent of other frameworks and includes ",{"type":19,"tag":114,"props":844,"children":847},{"href":845,"rel":846},"https://onnxruntime.ai/",[118],[848],{"type":29,"value":849},"ONNX Runtime",{"type":29,"value":851},", a toolset that allows to directly run the model.",{"type":19,"tag":31,"props":853,"children":854},{},[855],{"type":29,"value":856},"Again, the image below shows some of these runtime frameworks:",{"type":19,"tag":31,"props":858,"children":859},{},[860],{"type":19,"tag":95,"props":861,"children":864},{"alt":862,"src":863},"Runtime engines","https://i.imgur.com/URfGOgr.png",[],{"type":19,"tag":36,"props":866,"children":868},{"id":867},"onnx-model-zoo",[869],{"type":19,"tag":25,"props":870,"children":871},{},[872],{"type":29,"value":873},"ONNX model zoo",{"type":19,"tag":31,"props":875,"children":876},{},[877,879,886],{"type":29,"value":878},"There is also good news if you want even to skip your training phase and quickly run a model in your hardware. ONNX already offers some trained ONNX models in ONNX format in their ",{"type":19,"tag":114,"props":880,"children":883},{"href":881,"rel":882},"https://github.com/onnx/models",[118],[884],{"type":29,"value":885},"model zoo",{"type":29,"value":887}," so you can directly run it with ONNX Runtime (or the chosen Runtime engine).",{"type":19,"tag":36,"props":889,"children":891},{"id":890},"associated-tools",[892],{"type":19,"tag":25,"props":893,"children":894},{},[895],{"type":29,"value":896},"Associated tools",{"type":19,"tag":898,"props":899,"children":901},"h3",{"id":900},"cloud-services",[902],{"type":29,"value":903},"Cloud services",{"type":19,"tag":31,"props":905,"children":906},{},[907],{"type":29,"value":908},"The following Cloud engines give explicit support to ONNX models:",{"type":19,"tag":31,"props":910,"children":911},{},[912],{"type":19,"tag":95,"props":913,"children":915},{"alt":903,"src":914},"https://i.imgur.com/VLydLF0.png",[],{"type":19,"tag":898,"props":917,"children":919},{"id":918},"visualization",[920],{"type":29,"value":921},"Visualization",{"type":19,"tag":31,"props":923,"children":924},{},[925],{"type":29,"value":926},"The following tools allow you to inspect the models in a graphical way:",{"type":19,"tag":31,"props":928,"children":929},{},[930],{"type":19,"tag":95,"props":931,"children":934},{"alt":932,"src":933},"Visualization tools","https://i.imgur.com/Guvmxwg.png",[],{"type":19,"tag":31,"props":936,"children":937},{},[938,940,950],{"type":29,"value":939},"In one section below I will show an example using ",{"type":19,"tag":114,"props":941,"children":944},{"href":942,"rel":943},"https://netron.app/",[118],[945],{"type":19,"tag":222,"props":946,"children":947},{},[948],{"type":29,"value":949},"Netron",{"type":29,"value":752},{"type":19,"tag":36,"props":952,"children":954},{"id":953},"onnx-example-from-pytorch",[955],{"type":19,"tag":25,"props":956,"children":957},{},[958],{"type":29,"value":959},"ONNX example: from Pytorch",{"type":19,"tag":31,"props":961,"children":962},{},[963,965,972],{"type":29,"value":964},"I developed a Python snippet of code that converts ",{"type":19,"tag":114,"props":966,"children":969},{"href":967,"rel":968},"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html",[118],[970],{"type":29,"value":971},"ResNet18 from torchvision",{"type":29,"value":973}," into ONNX format (and stores it in disk). It also includes a method to load the model from the file.",{"type":19,"tag":975,"props":976,"children":980},"pre",{"className":977,"code":978,"language":979,"meta":10},"language-python github-light_github-dark","import torch\nimport torch.onnx\nimport onnx\nfrom torchvision.models import resnet18, ResNet18_Weights\n\n\ndef resnet18_to_onnx(fname='resnet18.onnx', batch_size=8):\n    \"\"\"\n    Saves on disk in ONNX the ResNet18 pretrained model from Pytorch, which works through batches\n\n    :param fname: (str)\n    :param batch_size: (int) batch size of the input\n    :return:\n    \"\"\"\n    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n    batch = torch.zeros([batch_size, 3, 256, 256])   # ImageNet1k size\n    torch.onnx.export(model, batch, fname, input_names=['input'], output_names=['output'])\n\n\ndef load_resnet18_from_onnx(fname='resnet18.onnx'):\n    return onnx.load(fname)\n\n\nif __name__ == \"__main__\":\n    resnet18_to_onnx()\n\n","python",[981],{"type":19,"tag":230,"props":982,"children":983},{"__ignoreMap":10},[984,1001,1013,1026,1049,1056,1063,1118,1132,1141,1148,1157,1166,1175,1184,1227,1277,1333,1340,1347,1380,1398,1405,1412,1453],{"type":19,"tag":329,"props":985,"children":988},{"class":986,"line":987},"line",1,[989,995],{"type":19,"tag":329,"props":990,"children":992},{"class":991},"ct-149352",[993],{"type":29,"value":994},"import",{"type":19,"tag":329,"props":996,"children":998},{"class":997},"ct-553616",[999],{"type":29,"value":1000}," torch\n",{"type":19,"tag":329,"props":1002,"children":1003},{"class":986,"line":553},[1004,1008],{"type":19,"tag":329,"props":1005,"children":1006},{"class":991},[1007],{"type":29,"value":994},{"type":19,"tag":329,"props":1009,"children":1010},{"class":997},[1011],{"type":29,"value":1012}," torch.onnx\n",{"type":19,"tag":329,"props":1014,"children":1016},{"class":986,"line":1015},3,[1017,1021],{"type":19,"tag":329,"props":1018,"children":1019},{"class":991},[1020],{"type":29,"value":994},{"type":19,"tag":329,"props":1022,"children":1023},{"class":997},[1024],{"type":29,"value":1025}," onnx\n",{"type":19,"tag":329,"props":1027,"children":1029},{"class":986,"line":1028},4,[1030,1035,1040,1044],{"type":19,"tag":329,"props":1031,"children":1032},{"class":991},[1033],{"type":29,"value":1034},"from",{"type":19,"tag":329,"props":1036,"children":1037},{"class":997},[1038],{"type":29,"value":1039}," torchvision.models ",{"type":19,"tag":329,"props":1041,"children":1042},{"class":991},[1043],{"type":29,"value":994},{"type":19,"tag":329,"props":1045,"children":1046},{"class":997},[1047],{"type":29,"value":1048}," resnet18, ResNet18_Weights\n",{"type":19,"tag":329,"props":1050,"children":1052},{"class":986,"line":1051},5,[1053],{"type":19,"tag":329,"props":1054,"children":1055},{},[],{"type":19,"tag":329,"props":1057,"children":1059},{"class":986,"line":1058},6,[1060],{"type":19,"tag":329,"props":1061,"children":1062},{},[],{"type":19,"tag":329,"props":1064,"children":1066},{"class":986,"line":1065},7,[1067,1072,1076,1082,1087,1092,1098,1103,1107,1113],{"type":19,"tag":329,"props":1068,"children":1069},{"class":991},[1070],{"type":29,"value":1071},"def",{"type":19,"tag":329,"props":1073,"children":1074},{"class":997},[1075],{"type":29,"value":335},{"type":19,"tag":329,"props":1077,"children":1079},{"class":1078},"ct-762058",[1080],{"type":29,"value":1081},"resnet18_to_onnx",{"type":19,"tag":329,"props":1083,"children":1084},{"class":997},[1085],{"type":29,"value":1086},"(fname",{"type":19,"tag":329,"props":1088,"children":1089},{"class":991},[1090],{"type":29,"value":1091},"=",{"type":19,"tag":329,"props":1093,"children":1095},{"class":1094},"ct-952708",[1096],{"type":29,"value":1097},"'resnet18.onnx'",{"type":19,"tag":329,"props":1099,"children":1100},{"class":997},[1101],{"type":29,"value":1102},", batch_size",{"type":19,"tag":329,"props":1104,"children":1105},{"class":991},[1106],{"type":29,"value":1091},{"type":19,"tag":329,"props":1108,"children":1110},{"class":1109},"ct-617022",[1111],{"type":29,"value":1112},"8",{"type":19,"tag":329,"props":1114,"children":1115},{"class":997},[1116],{"type":29,"value":1117},"):\n",{"type":19,"tag":329,"props":1119,"children":1121},{"class":986,"line":1120},8,[1122,1127],{"type":19,"tag":329,"props":1123,"children":1124},{"class":997},[1125],{"type":29,"value":1126},"    ",{"type":19,"tag":329,"props":1128,"children":1129},{"class":1094},[1130],{"type":29,"value":1131},"\"\"\"\n",{"type":19,"tag":329,"props":1133,"children":1135},{"class":986,"line":1134},9,[1136],{"type":19,"tag":329,"props":1137,"children":1138},{"class":1094},[1139],{"type":29,"value":1140},"    Saves on disk in ONNX the ResNet18 pretrained model from Pytorch, which works through batches\n",{"type":19,"tag":329,"props":1142,"children":1144},{"class":986,"line":1143},10,[1145],{"type":19,"tag":329,"props":1146,"children":1147},{},[],{"type":19,"tag":329,"props":1149,"children":1151},{"class":986,"line":1150},11,[1152],{"type":19,"tag":329,"props":1153,"children":1154},{"class":1094},[1155],{"type":29,"value":1156},"    :param fname: (str)\n",{"type":19,"tag":329,"props":1158,"children":1160},{"class":986,"line":1159},12,[1161],{"type":19,"tag":329,"props":1162,"children":1163},{"class":1094},[1164],{"type":29,"value":1165},"    :param batch_size: (int) batch size of the input\n",{"type":19,"tag":329,"props":1167,"children":1169},{"class":986,"line":1168},13,[1170],{"type":19,"tag":329,"props":1171,"children":1172},{"class":1094},[1173],{"type":29,"value":1174},"    :return:\n",{"type":19,"tag":329,"props":1176,"children":1178},{"class":986,"line":1177},14,[1179],{"type":19,"tag":329,"props":1180,"children":1181},{"class":1094},[1182],{"type":29,"value":1183},"    \"\"\"\n",{"type":19,"tag":329,"props":1185,"children":1187},{"class":986,"line":1186},15,[1188,1193,1197,1202,1208,1212,1217,1222],{"type":19,"tag":329,"props":1189,"children":1190},{"class":997},[1191],{"type":29,"value":1192},"    model ",{"type":19,"tag":329,"props":1194,"children":1195},{"class":991},[1196],{"type":29,"value":1091},{"type":19,"tag":329,"props":1198,"children":1199},{"class":997},[1200],{"type":29,"value":1201}," resnet18(",{"type":19,"tag":329,"props":1203,"children":1205},{"class":1204},"ct-157101",[1206],{"type":29,"value":1207},"weights",{"type":19,"tag":329,"props":1209,"children":1210},{"class":991},[1211],{"type":29,"value":1091},{"type":19,"tag":329,"props":1213,"children":1214},{"class":997},[1215],{"type":29,"value":1216},"ResNet18_Weights.",{"type":19,"tag":329,"props":1218,"children":1219},{"class":1109},[1220],{"type":29,"value":1221},"DEFAULT",{"type":19,"tag":329,"props":1223,"children":1224},{"class":997},[1225],{"type":29,"value":1226},")\n",{"type":19,"tag":329,"props":1228,"children":1230},{"class":986,"line":1229},16,[1231,1236,1240,1245,1249,1253,1258,1262,1266,1271],{"type":19,"tag":329,"props":1232,"children":1233},{"class":997},[1234],{"type":29,"value":1235},"    batch ",{"type":19,"tag":329,"props":1237,"children":1238},{"class":991},[1239],{"type":29,"value":1091},{"type":19,"tag":329,"props":1241,"children":1242},{"class":997},[1243],{"type":29,"value":1244}," torch.zeros([batch_size, ",{"type":19,"tag":329,"props":1246,"children":1247},{"class":1109},[1248],{"type":29,"value":384},{"type":19,"tag":329,"props":1250,"children":1251},{"class":997},[1252],{"type":29,"value":805},{"type":19,"tag":329,"props":1254,"children":1255},{"class":1109},[1256],{"type":29,"value":1257},"256",{"type":19,"tag":329,"props":1259,"children":1260},{"class":997},[1261],{"type":29,"value":805},{"type":19,"tag":329,"props":1263,"children":1264},{"class":1109},[1265],{"type":29,"value":1257},{"type":19,"tag":329,"props":1267,"children":1268},{"class":997},[1269],{"type":29,"value":1270},"])   ",{"type":19,"tag":329,"props":1272,"children":1274},{"class":1273},"ct-086898",[1275],{"type":29,"value":1276},"# ImageNet1k size\n",{"type":19,"tag":329,"props":1278,"children":1280},{"class":986,"line":1279},17,[1281,1286,1291,1295,1300,1305,1310,1315,1319,1323,1328],{"type":19,"tag":329,"props":1282,"children":1283},{"class":997},[1284],{"type":29,"value":1285},"    torch.onnx.export(model, batch, fname, ",{"type":19,"tag":329,"props":1287,"children":1288},{"class":1204},[1289],{"type":29,"value":1290},"input_names",{"type":19,"tag":329,"props":1292,"children":1293},{"class":991},[1294],{"type":29,"value":1091},{"type":19,"tag":329,"props":1296,"children":1297},{"class":997},[1298],{"type":29,"value":1299},"[",{"type":19,"tag":329,"props":1301,"children":1302},{"class":1094},[1303],{"type":29,"value":1304},"'input'",{"type":19,"tag":329,"props":1306,"children":1307},{"class":997},[1308],{"type":29,"value":1309},"], ",{"type":19,"tag":329,"props":1311,"children":1312},{"class":1204},[1313],{"type":29,"value":1314},"output_names",{"type":19,"tag":329,"props":1316,"children":1317},{"class":991},[1318],{"type":29,"value":1091},{"type":19,"tag":329,"props":1320,"children":1321},{"class":997},[1322],{"type":29,"value":1299},{"type":19,"tag":329,"props":1324,"children":1325},{"class":1094},[1326],{"type":29,"value":1327},"'output'",{"type":19,"tag":329,"props":1329,"children":1330},{"class":997},[1331],{"type":29,"value":1332},"])\n",{"type":19,"tag":329,"props":1334,"children":1336},{"class":986,"line":1335},18,[1337],{"type":19,"tag":329,"props":1338,"children":1339},{},[],{"type":19,"tag":329,"props":1341,"children":1343},{"class":986,"line":1342},19,[1344],{"type":19,"tag":329,"props":1345,"children":1346},{},[],{"type":19,"tag":329,"props":1348,"children":1350},{"class":986,"line":1349},20,[1351,1355,1359,1364,1368,1372,1376],{"type":19,"tag":329,"props":1352,"children":1353},{"class":991},[1354],{"type":29,"value":1071},{"type":19,"tag":329,"props":1356,"children":1357},{"class":997},[1358],{"type":29,"value":335},{"type":19,"tag":329,"props":1360,"children":1361},{"class":1078},[1362],{"type":29,"value":1363},"load_resnet18_from_onnx",{"type":19,"tag":329,"props":1365,"children":1366},{"class":997},[1367],{"type":29,"value":1086},{"type":19,"tag":329,"props":1369,"children":1370},{"class":991},[1371],{"type":29,"value":1091},{"type":19,"tag":329,"props":1373,"children":1374},{"class":1094},[1375],{"type":29,"value":1097},{"type":19,"tag":329,"props":1377,"children":1378},{"class":997},[1379],{"type":29,"value":1117},{"type":19,"tag":329,"props":1381,"children":1383},{"class":986,"line":1382},21,[1384,1388,1393],{"type":19,"tag":329,"props":1385,"children":1386},{"class":997},[1387],{"type":29,"value":1126},{"type":19,"tag":329,"props":1389,"children":1390},{"class":991},[1391],{"type":29,"value":1392},"return",{"type":19,"tag":329,"props":1394,"children":1395},{"class":997},[1396],{"type":29,"value":1397}," onnx.load(fname)\n",{"type":19,"tag":329,"props":1399,"children":1401},{"class":986,"line":1400},22,[1402],{"type":19,"tag":329,"props":1403,"children":1404},{},[],{"type":19,"tag":329,"props":1406,"children":1408},{"class":986,"line":1407},23,[1409],{"type":19,"tag":329,"props":1410,"children":1411},{},[],{"type":19,"tag":329,"props":1413,"children":1415},{"class":986,"line":1414},24,[1416,1421,1425,1430,1434,1439,1443,1448],{"type":19,"tag":329,"props":1417,"children":1418},{"class":991},[1419],{"type":29,"value":1420},"if",{"type":19,"tag":329,"props":1422,"children":1423},{"class":997},[1424],{"type":29,"value":335},{"type":19,"tag":329,"props":1426,"children":1427},{"class":1109},[1428],{"type":29,"value":1429},"__name__",{"type":19,"tag":329,"props":1431,"children":1432},{"class":997},[1433],{"type":29,"value":335},{"type":19,"tag":329,"props":1435,"children":1436},{"class":991},[1437],{"type":29,"value":1438},"==",{"type":19,"tag":329,"props":1440,"children":1441},{"class":997},[1442],{"type":29,"value":335},{"type":19,"tag":329,"props":1444,"children":1445},{"class":1094},[1446],{"type":29,"value":1447},"\"__main__\"",{"type":19,"tag":329,"props":1449,"children":1450},{"class":997},[1451],{"type":29,"value":1452},":\n",{"type":19,"tag":329,"props":1454,"children":1456},{"class":986,"line":1455},25,[1457],{"type":19,"tag":329,"props":1458,"children":1459},{"class":997},[1460],{"type":29,"value":1461},"    resnet18_to_onnx()",{"type":19,"tag":898,"props":1463,"children":1465},{"id":1464},"visualization-example",[1466],{"type":29,"value":1467},"Visualization example",{"type":19,"tag":31,"props":1469,"children":1470},{},[1471,1473,1477],{"type":29,"value":1472},"I loaded the ONNX model from the example in the app ",{"type":19,"tag":222,"props":1474,"children":1475},{},[1476],{"type":29,"value":949},{"type":29,"value":1478}," and I could visualize the model in the following image:",{"type":19,"tag":31,"props":1480,"children":1481},{},[1482],{"type":19,"tag":95,"props":1483,"children":1486},{"alt":1484,"src":1485},"Imgur","https://i.imgur.com/lj4aiBU.png",[],{"type":19,"tag":36,"props":1488,"children":1490},{"id":1489},"some-onnx-limitation",[1491],{"type":29,"value":1492},"Some ONNX limitation",{"type":19,"tag":31,"props":1494,"children":1495},{},[1496],{"type":29,"value":1497},"Until now we only talked about how good is ONNX. I strongly believe it is as an initiative, but obviously we can have a conversation on how limited it is (as we could do with anything). We will not do so, but I will remark what I think is the main limitation of ONNX.",{"type":19,"tag":31,"props":1499,"children":1500},{},[1501,1503,1507,1509,1513],{"type":29,"value":1502},"Although ",{"type":19,"tag":222,"props":1504,"children":1505},{},[1506],{"type":29,"value":750},{"type":29,"value":1508}," is pretty wide, we could want to use our custom layers/operations. And that is where ONNX fails us. ONNX only supports operations in the ",{"type":19,"tag":222,"props":1510,"children":1511},{},[1512],{"type":29,"value":750},{"type":29,"value":1514},". In that case, we could find an alternative way to do what we want or just don't use ONNX.",{"type":19,"tag":36,"props":1516,"children":1518},{"id":1517},"conclusions",[1519],{"type":29,"value":1520},"Conclusions",{"type":19,"tag":31,"props":1522,"children":1523},{},[1524],{"type":29,"value":1525},"Although not 100% of the cases can be covered by ONNX, nearly all of them can. Nowadays, frameworks already offer tools to intuitively convert our models into ONNX and load them. Thus, I think we developers should adopt the policy to convert the models to ONNX for our own model zoos. These policies should obviously also consider the possibility of some models not being able to be converted to ONNX, allowing for different solutions in that cases.",{"type":19,"tag":36,"props":1527,"children":1528},{"id":301},[1529],{"type":29,"value":307},{"type":19,"tag":31,"props":1531,"children":1532},{},[1533],{"type":29,"value":1534},"Thank you for going with me through this first Short post! I expect to do more like this in the future so any feedback is welcome.",{"type":19,"tag":1536,"props":1537,"children":1538},"style",{},[1539],{"type":29,"value":1540},".github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-086898{color:#6A737D;}",{"title":10,"searchDepth":553,"depth":553,"links":1542},[1543,1544,1545,1546,1547,1548,1549,1550,1554,1557,1558,1559],{"id":604,"depth":553,"text":610},{"id":625,"depth":553,"text":631},{"id":672,"depth":553,"text":678},{"id":704,"depth":553,"text":710},{"id":755,"depth":553,"text":761},{"id":817,"depth":553,"text":823},{"id":867,"depth":553,"text":873},{"id":890,"depth":553,"text":896,"children":1551},[1552,1553],{"id":900,"depth":1015,"text":903},{"id":918,"depth":1015,"text":921},{"id":953,"depth":553,"text":959,"children":1555},[1556],{"id":1464,"depth":1015,"text":1467},{"id":1489,"depth":553,"text":1492},{"id":1517,"depth":553,"text":1520},{"id":301,"depth":553,"text":307},"content:articles:2023-02-09-SHORT-onnx.md","articles/2023-02-09-SHORT-onnx.md",null,["ShallowRef",1564],{},{"preference":1566,"value":1566,"unknown":1567,"forced":9},"system",true,[1569,1573,1595],{"title":1570,"_path":1571,"layout":1572},"About","/","default",{"title":1574,"_path":1575,"children":1576,"layout":1594},"Articles","/articles",[1577,1580,1583,1586,1589,1592,1593],{"title":1578,"_path":1579,"layout":14},"Welcome Post","/articles/2022-11-15-welcome-post",{"title":1581,"_path":1582,"layout":14},"Meta-Learning explained","/articles/2022-11-21-meta-learning",{"title":1584,"_path":1585,"layout":14},"Meta-Learning implementation","/articles/2022-12-20-meta-learning-implementation",{"title":1587,"_path":1588,"layout":14},"Meta-Learning: MAML evaluation and discussion by Metabloggism","/articles/2023-02-07-meta-learning-analysis",{"title":1590,"_path":1591,"layout":14},"Introduction of SHORTS","/articles/2023-02-08-shorts",{"title":577,"_path":576,"layout":14},{"title":11,"_path":7,"layout":14},"page",{"title":1596,"_path":1597,"layout":1572},"Contact","/contact",{"uil:github":1599,"uil:linkedin":1602,"material-symbols:arrow-upward":1604,"ph:arrow-left":1606},{"left":1600,"top":1600,"width":1414,"height":1414,"rotate":1600,"vFlip":9,"hFlip":9,"body":1601},0,"\u003Cpath fill=\"currentColor\" d=\"M12 2.247a10 10 0 0 0-3.162 19.487c.5.088.687-.212.687-.475c0-.237-.012-1.025-.012-1.862c-2.513.462-3.163-.613-3.363-1.175a3.636 3.636 0 0 0-1.025-1.413c-.35-.187-.85-.65-.013-.662a2.001 2.001 0 0 1 1.538 1.025a2.137 2.137 0 0 0 2.912.825a2.104 2.104 0 0 1 .638-1.338c-2.225-.25-4.55-1.112-4.55-4.937a3.892 3.892 0 0 1 1.025-2.688a3.594 3.594 0 0 1 .1-2.65s.837-.262 2.75 1.025a9.427 9.427 0 0 1 5 0c1.912-1.3 2.75-1.025 2.75-1.025a3.593 3.593 0 0 1 .1 2.65a3.869 3.869 0 0 1 1.025 2.688c0 3.837-2.338 4.687-4.563 4.937a2.368 2.368 0 0 1 .675 1.85c0 1.338-.012 2.413-.012 2.75c0 .263.187.575.687.475A10.005 10.005 0 0 0 12 2.247Z\"/>",{"left":1600,"top":1600,"width":1414,"height":1414,"rotate":1600,"vFlip":9,"hFlip":9,"body":1603},"\u003Cpath fill=\"currentColor\" d=\"M20.47 2H3.53a1.45 1.45 0 0 0-1.47 1.43v17.14A1.45 1.45 0 0 0 3.53 22h16.94a1.45 1.45 0 0 0 1.47-1.43V3.43A1.45 1.45 0 0 0 20.47 2ZM8.09 18.74h-3v-9h3ZM6.59 8.48a1.56 1.56 0 1 1 0-3.12a1.57 1.57 0 1 1 0 3.12Zm12.32 10.26h-3v-4.83c0-1.21-.43-2-1.52-2A1.65 1.65 0 0 0 12.85 13a2 2 0 0 0-.1.73v5h-3v-9h3V11a3 3 0 0 1 2.71-1.5c2 0 3.45 1.29 3.45 4.06Z\"/>",{"left":1600,"top":1600,"width":1414,"height":1414,"rotate":1600,"vFlip":9,"hFlip":9,"body":1605},"\u003Cpath fill=\"currentColor\" d=\"M11 20V7.825l-5.6 5.6L4 12l8-8l8 8l-1.4 1.425l-5.6-5.6V20h-2Z\"/>",{"left":1600,"top":1600,"width":1607,"height":1607,"rotate":1600,"vFlip":9,"hFlip":9,"body":1608},256,"\u003Cpath fill=\"currentColor\" d=\"M224 128a8 8 0 0 1-8 8H59.31l58.35 58.34a8 8 0 0 1-11.32 11.32l-72-72a8 8 0 0 1 0-11.32l72-72a8 8 0 0 1 11.32 11.32L59.31 120H216a8 8 0 0 1 8 8Z\"/>",["Reactive",1610],{}]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{FORMSPREE_URL:"",plausible:{hashMode:false,trackLocalhost:false,domain:"",apiHost:"https://plausible.io",autoPageviews:true,autoOutboundTracking:false},studio:{apiURL:"https://api.nuxt.studio"},mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1693576793614,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:["navTitle","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:{default:"github-light",dark:"github-dark"},preload:["json","js","ts","html","css","vue","diff","shell","markdown","yaml","bash","ini","c","cpp"]},wsUrl:"",documentDriven:{page:true,navigation:true,surround:true,globals:{},layoutFallbacks:["theme"],injectPage:true},host:"",trailingSlash:false,contentHead:true,anchorLinks:{depth:4,exclude:[1]}}},app:{baseURL:"/",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body>
</html>
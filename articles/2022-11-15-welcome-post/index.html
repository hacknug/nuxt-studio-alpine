<!DOCTYPE html>
<html  lang="en">
<head><meta charset="utf-8">
<title>Welcome Post</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary_large_image">
<meta property="og:image:width" content="400">
<meta property="og:image:height" content="300">
<meta property="og:image:alt" content="An image showcasing my project.">
<meta property="og:title" content="Welcome Post">
<meta name="description" content="Learn how to configure Alpine with the app.config.ts file.">
<meta property="og:description" content="Learn how to configure Alpine with the app.config.ts file.">
<meta property="og:image" content="/articles/get-started.webp">
<link rel="preload" as="fetch" crossorigin="anonymous" href="/articles/2022-11-15-welcome-post/_payload.json">
<link rel="stylesheet" href="/_nuxt/entry.2b709d1c.css">
<link rel="stylesheet" href="/_nuxt/DocumentDrivenNotFound.aa939160.css">
<link rel="stylesheet" href="/_nuxt/article.06aa1a19.css">
<link rel="stylesheet" href="/_nuxt/ProseA.baee409d.css">
<link rel="stylesheet" href="/_nuxt/ProseH1.6d63403c.css">
<link rel="stylesheet" href="/_nuxt/ProseH2.3a63b076.css">
<link rel="stylesheet" href="/_nuxt/ProseP.b99f89cd.css">
<link rel="stylesheet" href="/_nuxt/ProseHr.51db3351.css">
<link rel="stylesheet" href="/_nuxt/ProseH3.36490a89.css">
<link rel="stylesheet" href="/_nuxt/ProseEm.35a26f4d.css">
<link rel="stylesheet" href="/_nuxt/ProseStrong.b01d4b3b.css">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/entry.5b2fbc2b.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/NuxtImg.71ecd94d.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/document-driven.02488242.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenEmpty.33a0504c.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRenderer.ea978627.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.vue.233735c0.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenNotFound.e61e620a.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/article.74583e02.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseA.b25940ab.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/date.824a539b.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.c50a67fb.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH1.0f011535.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH2.106e00e3.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseP.df39db4a.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseHr.c81f12b0.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH3.c663c5f8.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseEm.ab0e77c6.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseStrong.11c76ad1.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/default.9a1873bb.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/page.dfa15be1.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/client-db.b6bffc10.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/pipeline.b9c7be3a.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/client-db.33f4b423.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/debug.fde61575.js">
<link rel="prefetch" as="style" href="/_nuxt/useStudio.a92b5f33.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/useStudio.8bae12b1.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/asyncData.57510f6c.js">
<link rel="prefetch" as="style" href="/_nuxt/error-404.7910d5ca.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-404.3de20a2d.js">
<link rel="prefetch" as="style" href="/_nuxt/error-500.1db01289.css">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-500.2ac66414.js">
<script type="module" src="/_nuxt/entry.5b2fbc2b.js" crossorigin></script><style id="pinceau-runtime-hydratable">@media{.phy[--]{--puid:omEISc-v;}.pv-lrHVii{max-width:var(--elements-container-maxWidth);padding-left:var(--elements-container-padding-mobile);padding-right:var(--elements-container-padding-mobile);}@media (min-width: 475px){.pv-lrHVii{padding-left:var(--elements-container-padding-xs);padding-right:var(--elements-container-padding-xs);}}@media (min-width: 640px){.pv-lrHVii{padding-left:var(--elements-container-padding-sm);padding-right:var(--elements-container-padding-sm);}}@media (min-width: 768px){.pv-lrHVii{padding-left:var(--elements-container-padding-md);padding-right:var(--elements-container-padding-md);}}} </style><style id="pinceau-theme">@media { :root {--pinceau-mq: initial; --alpine-readableLine: 68ch;--alpine-backdrop-backgroundColor: #f4f4f5b3;--prose-code-inline-padding: 0.2rem 0.375rem 0.2rem 0.375rem;--prose-code-block-backdropFilter: contrast(1);--prose-code-block-border-style: solid;--prose-code-block-border-width: 1px;--prose-tbody-tr-borderBottom-style: dashed;--prose-tbody-tr-borderBottom-width: 1px;--prose-th-textAlign: inherit;--prose-thead-borderBottom-style: solid;--prose-thead-borderBottom-width: 1px;--prose-thead-border-style: solid;--prose-thead-border-width: 0px;--prose-table-textAlign: start;--prose-hr-width: 1px;--prose-hr-style: solid;--prose-li-listStylePosition: outside;--prose-ol-li-markerColor: currentColor;--prose-ol-paddingInlineStart: 21px;--prose-ol-listStyleType: decimal;--prose-ul-li-markerColor: currentColor;--prose-ul-paddingInlineStart: 21px;--prose-ul-listStyleType: disc;--prose-blockquote-border-style: solid;--prose-blockquote-border-width: 4px;--prose-blockquote-quotes: '201C' '201D' '2018' '2019';--prose-blockquote-paddingInlineStart: 24px;--prose-a-code-color-hover: currentColor;--prose-a-code-color-static: currentColor;--prose-a-hasCode-borderBottom: none;--prose-a-border-distance: 2px;--prose-a-border-color-hover: currentColor;--prose-a-border-color-static: currentColor;--prose-a-border-style-hover: solid;--prose-a-border-style-static: dashed;--prose-a-border-width: 1px;--prose-a-color-static: inherit;--prose-a-textDecoration: none;--prose-h6-margin: 3rem 0 2rem;--prose-h5-margin: 3rem 0 2rem;--prose-h4-margin: 3rem 0 2rem;--prose-h3-margin: 3rem 0 2rem;--prose-h2-margin: 3rem 0 2rem;--prose-h1-margin: 0 0 2rem;--prose-p-fontSize: 18px;--typography-lead-loose: 2;--typography-lead-relaxed: 1.625;--typography-lead-normal: 1.5;--typography-lead-snug: 1.375;--typography-lead-tight: 1.25;--typography-lead-none: 1;--typography-lead-10: 2.5rem;--typography-lead-9: 2.25rem;--typography-lead-8: 2rem;--typography-lead-7: 1.75rem;--typography-lead-6: 1.5rem;--typography-lead-5: 1.25rem;--typography-lead-4: 1rem;--typography-lead-3: .75rem;--typography-lead-2: .5rem;--typography-lead-1: .025rem;--typography-fontWeight-black: 900;--typography-fontWeight-extrabold: 800;--typography-fontWeight-bold: 700;--typography-fontWeight-semibold: 600;--typography-fontWeight-medium: 500;--typography-fontWeight-normal: 400;--typography-fontWeight-light: 300;--typography-fontWeight-extralight: 200;--typography-fontWeight-thin: 100;--typography-fontSize-9xl: 128px;--typography-fontSize-8xl: 96px;--typography-fontSize-7xl: 72px;--typography-fontSize-6xl: 60px;--typography-fontSize-5xl: 48px;--typography-fontSize-4xl: 36px;--typography-fontSize-3xl: 30px;--typography-fontSize-2xl: 24px;--typography-fontSize-xl: 20px;--typography-fontSize-lg: 18px;--typography-fontSize-base: 16px;--typography-fontSize-sm: 14px;--typography-fontSize-xs: 12px;--typography-letterSpacing-wide: 0.025em;--typography-letterSpacing-tight: -0.025em;--typography-verticalMargin-base: 24px;--typography-verticalMargin-sm: 16px;--elements-border-secondary-hover: [object Object];--elements-backdrop-background: #fffc;--elements-backdrop-filter: saturate(200%) blur(20px);--elements-container-maxWidth: 64rem;--lead-loose: 2;--lead-relaxed: 1.625;--lead-normal: 1.5;--lead-snug: 1.375;--lead-tight: 1.25;--lead-none: 1;--lead-10: 2.5rem;--lead-9: 2.25rem;--lead-8: 2rem;--lead-7: 1.75rem;--lead-6: 1.5rem;--lead-5: 1.25rem;--lead-4: 1rem;--lead-3: .75rem;--lead-2: .5rem;--lead-1: .025rem;--letterSpacing-widest: 0.1em;--letterSpacing-wider: 0.05em;--letterSpacing-wide: 0.025em;--letterSpacing-normal: 0em;--letterSpacing-tight: -0.025em;--letterSpacing-tighter: -0.05em;--fontSize-9xl: 8rem;--fontSize-8xl: 6rem;--fontSize-7xl: 4.5rem;--fontSize-6xl: 3.75rem;--fontSize-5xl: 3rem;--fontSize-4xl: 2.25rem;--fontSize-3xl: 1.875rem;--fontSize-2xl: 1.5rem;--fontSize-xl: 1.25rem;--fontSize-lg: 1.125rem;--fontSize-base: 1rem;--fontSize-sm: 0.875rem;--fontSize-xs: 0.75rem;--fontWeight-black: 900;--fontWeight-extrabold: 800;--fontWeight-bold: 700;--fontWeight-semibold: 600;--fontWeight-medium: 500;--fontWeight-normal: 400;--fontWeight-light: 300;--fontWeight-extralight: 200;--fontWeight-thin: 100;--font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace;--font-serif: ui-serif, Georgia, Cambria, Times New Roman, Times, serif;--font-sans: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;--opacity-total: 1;--opacity-high: 0.8;--opacity-medium: 0.5;--opacity-soft: 0.3;--opacity-light: 0.15;--opacity-bright: 0.1;--opacity-noOpacity: 0;--borderWidth-lg: 3px;--borderWidth-md: 2px;--borderWidth-sm: 1px;--borderWidth-noBorder: 0;--space-rem-875: 0.875rem;--space-rem-625: 0.625rem;--space-rem-375: 0.375rem;--space-rem-125: 0.125rem;--space-px: 1px;--space-128: 32rem;--space-96: 24rem;--space-80: 20rem;--space-72: 18rem;--space-64: 16rem;--space-60: 15rem;--space-56: 14rem;--space-52: 13rem;--space-48: 12rem;--space-44: 11rem;--space-40: 10rem;--space-36: 9rem;--space-32: 8rem;--space-28: 7rem;--space-24: 6rem;--space-20: 5rem;--space-16: 4rem;--space-14: 3.5rem;--space-12: 3rem;--space-11: 2.75rem;--space-10: 2.5rem;--space-9: 2.25rem;--space-8: 2rem;--space-7: 1.75rem;--space-6: 1.5rem;--space-5: 1.25rem;--space-4: 1rem;--space-3: 0.75rem;--space-2: 0.5rem;--space-1: 0.25rem;--space-0: 0px;--size-full: 100%;--size-7xl: 80rem;--size-6xl: 72rem;--size-5xl: 64rem;--size-4xl: 56rem;--size-3xl: 48rem;--size-2xl: 42rem;--size-xl: 36rem;--size-lg: 32rem;--size-md: 28rem;--size-sm: 24rem;--size-xs: 20rem;--size-200: 200px;--size-104: 104px;--size-80: 80px;--size-64: 64px;--size-56: 56px;--size-48: 48px;--size-40: 40px;--size-32: 32px;--size-24: 24px;--size-20: 20px;--size-16: 16px;--size-12: 12px;--size-8: 8px;--size-6: 6px;--size-4: 4px;--size-2: 2px;--size-0: 0px;--radii-full: 9999px;--radii-3xl: 1.75rem;--radii-2xl: 1.5rem;--radii-xl: 1rem;--radii-lg: 0.75rem;--radii-md: 0.5rem;--radii-sm: 0.375rem;--radii-xs: 0.25rem;--radii-2xs: 0.125rem;--radii-none: 0px;--shadow-none: 0px 0px 0px 0px transparent;--shadow-lg: 0px 10px 15px -3px #000000, 0px 4px 6px -4px #000000;--shadow-md: 0px 4px 6px -1px #000000, 0px 2px 4px -2px #000000;--shadow-sm: 0px 1px 3px 0px #000000, 0px 1px 2px -1px #000000;--shadow-xs: 0px 1px 2px 0px #000000;--height-screen: 100vh;--width-screen: 100vw;--color-primary-900: #002e38;--color-primary-800: #005c70;--color-primary-700: #008aa9;--color-primary-600: #00b9e1;--color-primary-500: #1ad6ff;--color-primary-400: #40ddff;--color-primary-300: #66e4ff;--color-primary-200: #8deaff;--color-primary-100: #b3f1ff;--color-primary-50: #d9f8ff;--color-ruby-900: #380011;--color-ruby-800: #700021;--color-ruby-700: #a90032;--color-ruby-600: #e10043;--color-ruby-500: #ff1a5e;--color-ruby-400: #ff4079;--color-ruby-300: #ff6694;--color-ruby-200: #ff8dae;--color-ruby-100: #ffb3c9;--color-ruby-50: #ffd9e4;--color-pink-900: #380025;--color-pink-800: #70004b;--color-pink-700: #a90070;--color-pink-600: #e10095;--color-pink-500: #ff1ab2;--color-pink-400: #ff40bf;--color-pink-300: #ff66cc;--color-pink-200: #ff8dd8;--color-pink-100: #ffb3e5;--color-pink-50: #ffd9f2;--color-purple-900: #190038;--color-purple-800: #330070;--color-purple-700: #4c00a9;--color-purple-600: #6500e1;--color-purple-500: #811aff;--color-purple-400: #9640ff;--color-purple-300: #ab66ff;--color-purple-200: #c08dff;--color-purple-100: #d5b3ff;--color-purple-50: #ead9ff;--color-royalblue-900: #0b0531;--color-royalblue-800: #160a62;--color-royalblue-700: #211093;--color-royalblue-600: #2c15c4;--color-royalblue-500: #4127e8;--color-royalblue-400: #614bec;--color-royalblue-300: #806ff0;--color-royalblue-200: #a093f3;--color-royalblue-100: #c0b7f7;--color-royalblue-50: #dfdbfb;--color-indigoblue-900: #001238;--color-indigoblue-800: #002370;--color-indigoblue-700: #0035a9;--color-indigoblue-600: #0047e1;--color-indigoblue-500: #1a62ff;--color-indigoblue-400: #407cff;--color-indigoblue-300: #6696ff;--color-indigoblue-200: #8db0ff;--color-indigoblue-100: #b3cbff;--color-indigoblue-50: #d9e5ff;--color-blue-900: #002438;--color-blue-800: #004870;--color-blue-700: #006ca9;--color-blue-600: #0090e1;--color-blue-500: #1aadff;--color-blue-400: #40bbff;--color-blue-300: #66c8ff;--color-blue-200: #8dd6ff;--color-blue-100: #b3e4ff;--color-blue-50: #d9f1ff;--color-lightblue-900: #002e38;--color-lightblue-800: #005c70;--color-lightblue-700: #008aa9;--color-lightblue-600: #00b9e1;--color-lightblue-500: #1ad6ff;--color-lightblue-400: #40ddff;--color-lightblue-300: #66e4ff;--color-lightblue-200: #8deaff;--color-lightblue-100: #b3f1ff;--color-lightblue-50: #d9f8ff;--color-teal-900: #062a28;--color-teal-800: #0b544f;--color-teal-700: #117d77;--color-teal-600: #16a79e;--color-teal-500: #1cd1c6;--color-teal-400: #36e4da;--color-teal-300: #5fe9e1;--color-teal-200: #87efe9;--color-teal-100: #aff4f0;--color-teal-50: #d7faf8;--color-pear-900: #2a2b09;--color-pear-800: #545512;--color-pear-700: #7e801b;--color-pear-600: #a8aa24;--color-pear-500: #d0d32f;--color-pear-400: #d8da52;--color-pear-300: #e0e274;--color-pear-200: #e8e997;--color-pear-100: #eff0ba;--color-pear-50: #f7f8dc;--color-red-900: #380300;--color-red-800: #700700;--color-red-700: #a90a00;--color-red-600: #e10e00;--color-red-500: #ff281a;--color-red-400: #ff4c40;--color-red-300: #ff7066;--color-red-200: #ff948d;--color-red-100: #ffb7b3;--color-red-50: #ffdbd9;--color-orange-900: #381800;--color-orange-800: #702f00;--color-orange-700: #a94700;--color-orange-600: #e15e00;--color-orange-500: #ff7a1a;--color-orange-400: #ff9040;--color-orange-300: #ffa666;--color-orange-200: #ffbd8d;--color-orange-100: #ffd3b3;--color-orange-50: #ffe9d9;--color-yellow-900: #362b03;--color-yellow-800: #6d5605;--color-yellow-700: #a38108;--color-yellow-600: #daac0a;--color-yellow-500: #f5c828;--color-yellow-400: #f7d14c;--color-yellow-300: #f8da70;--color-yellow-200: #fae393;--color-yellow-100: #fcedb7;--color-yellow-50: #fdf6db;--color-green-900: #003f25;--color-green-800: #005e38;--color-green-700: #007e4a;--color-green-600: #009d5d;--color-green-500: #00bd6f;--color-green-400: #00dc82;--color-green-300: #30ffaa;--color-green-200: #83ffcc;--color-green-100: #acffdd;--color-green-50: #d6ffee;--color-gray-900: #18181B;--color-gray-800: #27272A;--color-gray-700: #3f3f46;--color-gray-600: #52525B;--color-gray-500: #71717A;--color-gray-400: #a1a1aa;--color-gray-300: #D4d4d8;--color-gray-200: #e4e4e7;--color-gray-100: #f4f4f5;--color-gray-50: #fafafa;--color-black: #0c0c0d;--color-white: #FFFFFF;--media-portrait: only screen and (orientation: portrait);--media-landscape: only screen and (orientation: landscape);--media-rm: (prefers-reduced-motion: reduce);--media-2xl: (min-width: 1536px);--media-xl: (min-width: 1280px);--media-lg: (min-width: 1024px);--media-md: (min-width: 768px);--media-sm: (min-width: 640px);--media-xs: (min-width: 475px);--alpine-body-color: var(--color-gray-800);--alpine-body-backgroundColor: var(--color-white);--prose-code-inline-fontWeight: var(--typography-fontWeight-normal);--prose-code-inline-fontSize: var(--typography-fontSize-sm);--prose-code-inline-borderRadius: var(--radii-xs);--prose-code-block-pre-padding: var(--typography-verticalMargin-sm);--prose-code-block-margin: var(--typography-verticalMargin-base) 0;--prose-code-block-fontSize: var(--typography-fontSize-sm);--prose-tbody-code-inline-fontSize: var(--typography-fontSize-sm);--prose-tbody-td-padding: var(--typography-verticalMargin-sm);--prose-th-fontWeight: var(--typography-fontWeight-semibold);--prose-th-padding: 0 var(--typography-verticalMargin-sm) var(--typography-verticalMargin-sm) var(--typography-verticalMargin-sm);--prose-table-lineHeight: var(--typography-lead-6);--prose-table-fontSize: var(--typography-fontSize-sm);--prose-table-margin: var(--typography-verticalMargin-base) 0;--prose-hr-margin: var(--typography-verticalMargin-base) 0;--prose-li-margin: var(--typography-verticalMargin-sm) 0;--prose-ol-margin: var(--typography-verticalMargin-base) 0;--prose-ul-margin: var(--typography-verticalMargin-base) 0;--prose-blockquote-margin: var(--typography-verticalMargin-base) 0;--prose-a-code-border-style: var(--prose-a-border-style-static);--prose-a-code-border-width: var(--prose-a-border-width);--prose-a-fontWeight: var(--typography-fontWeight-medium);--prose-img-margin: var(--typography-verticalMargin-base) 0;--prose-strong-fontWeight: var(--typography-fontWeight-semibold);--prose-h6-iconSize: var(--typography-fontSize-base);--prose-h6-fontWeight: var(--typography-fontWeight-semibold);--prose-h6-lineHeight: var(--typography-lead-normal);--prose-h6-fontSize: var(--typography-fontSize-lg);--prose-h5-iconSize: var(--typography-fontSize-lg);--prose-h5-fontWeight: var(--typography-fontWeight-semibold);--prose-h5-lineHeight: var(--typography-lead-snug);--prose-h5-fontSize: var(--typography-fontSize-xl);--prose-h4-iconSize: var(--typography-fontSize-lg);--prose-h4-letterSpacing: var(--typography-letterSpacing-tight);--prose-h4-fontWeight: var(--typography-fontWeight-semibold);--prose-h4-lineHeight: var(--typography-lead-snug);--prose-h4-fontSize: var(--typography-fontSize-2xl);--prose-h3-iconSize: var(--typography-fontSize-xl);--prose-h3-letterSpacing: var(--typography-letterSpacing-tight);--prose-h3-fontWeight: var(--typography-fontWeight-semibold);--prose-h3-lineHeight: var(--typography-lead-snug);--prose-h3-fontSize: var(--typography-fontSize-3xl);--prose-h2-iconSize: var(--typography-fontSize-2xl);--prose-h2-letterSpacing: var(--typography-letterSpacing-tight);--prose-h2-fontWeight: var(--typography-fontWeight-semibold);--prose-h2-lineHeight: var(--typography-lead-tight);--prose-h2-fontSize: var(--typography-fontSize-4xl);--prose-h1-iconSize: var(--typography-fontSize-3xl);--prose-h1-letterSpacing: var(--typography-letterSpacing-tight);--prose-h1-fontWeight: var(--typography-fontWeight-bold);--prose-h1-lineHeight: var(--typography-lead-tight);--prose-h1-fontSize: var(--typography-fontSize-5xl);--prose-p-br-margin: var(--typography-verticalMargin-base) 0 0 0;--prose-p-margin: var(--typography-verticalMargin-base) 0;--prose-p-lineHeight: var(--typography-lead-normal);--typography-color-primary-900: var(--color-primary-900);--typography-color-primary-800: var(--color-primary-800);--typography-color-primary-700: var(--color-primary-700);--typography-color-primary-600: var(--color-primary-600);--typography-color-primary-500: var(--color-primary-500);--typography-color-primary-400: var(--color-primary-400);--typography-color-primary-300: var(--color-primary-300);--typography-color-primary-200: var(--color-primary-200);--typography-color-primary-100: var(--color-primary-100);--typography-color-primary-50: var(--color-primary-50);--typography-font-code: var(--font-mono);--typography-font-body: var(--font-sans);--typography-font-display: var(--font-sans);--typography-body-backgroundColor: var(--color-white);--typography-body-color: var(--color-black);--elements-state-danger-borderColor-secondary: var(--color-red-200);--elements-state-danger-borderColor-primary: var(--color-red-100);--elements-state-danger-backgroundColor-secondary: var(--color-red-100);--elements-state-danger-backgroundColor-primary: var(--color-red-50);--elements-state-danger-color-secondary: var(--color-red-600);--elements-state-danger-color-primary: var(--color-red-500);--elements-state-warning-borderColor-secondary: var(--color-yellow-200);--elements-state-warning-borderColor-primary: var(--color-yellow-100);--elements-state-warning-backgroundColor-secondary: var(--color-yellow-100);--elements-state-warning-backgroundColor-primary: var(--color-yellow-50);--elements-state-warning-color-secondary: var(--color-yellow-700);--elements-state-warning-color-primary: var(--color-yellow-600);--elements-state-success-borderColor-secondary: var(--color-green-200);--elements-state-success-borderColor-primary: var(--color-green-100);--elements-state-success-backgroundColor-secondary: var(--color-green-100);--elements-state-success-backgroundColor-primary: var(--color-green-50);--elements-state-success-color-secondary: var(--color-green-600);--elements-state-success-color-primary: var(--color-green-500);--elements-state-info-borderColor-secondary: var(--color-blue-200);--elements-state-info-borderColor-primary: var(--color-blue-100);--elements-state-info-backgroundColor-secondary: var(--color-blue-100);--elements-state-info-backgroundColor-primary: var(--color-blue-50);--elements-state-info-color-secondary: var(--color-blue-600);--elements-state-info-color-primary: var(--color-blue-500);--elements-state-primary-borderColor-secondary: var(--color-primary-200);--elements-state-primary-borderColor-primary: var(--color-primary-100);--elements-state-primary-backgroundColor-secondary: var(--color-primary-100);--elements-state-primary-backgroundColor-primary: var(--color-primary-50);--elements-state-primary-color-secondary: var(--color-primary-700);--elements-state-primary-color-primary: var(--color-primary-600);--elements-surface-secondary-backgroundColor: var(--color-gray-200);--elements-surface-primary-backgroundColor: var(--color-gray-100);--elements-surface-background-base: var(--color-gray-100);--elements-border-secondary-static: var(--color-gray-200);--elements-border-primary-hover: var(--color-gray-200);--elements-border-primary-static: var(--color-gray-100);--elements-container-padding-md: var(--space-16);--elements-container-padding-sm: var(--space-12);--elements-container-padding-xs: var(--space-8);--elements-container-padding-mobile: var(--space-6);--elements-text-secondary-color-hover: var(--color-gray-700);--elements-text-secondary-color-static: var(--color-gray-500);--elements-text-primary-color-static: var(--color-gray-900);--text-6xl-lineHeight: var(--lead-none);--text-6xl-fontSize: var(--fontSize-6xl);--text-5xl-lineHeight: var(--lead-none);--text-5xl-fontSize: var(--fontSize-5xl);--text-4xl-lineHeight: var(--lead-10);--text-4xl-fontSize: var(--fontSize-4xl);--text-3xl-lineHeight: var(--lead-9);--text-3xl-fontSize: var(--fontSize-3xl);--text-2xl-lineHeight: var(--lead-8);--text-2xl-fontSize: var(--fontSize-2xl);--text-xl-lineHeight: var(--lead-7);--text-xl-fontSize: var(--fontSize-xl);--text-lg-lineHeight: var(--lead-7);--text-lg-fontSize: var(--fontSize-lg);--text-base-lineHeight: var(--lead-6);--text-base-fontSize: var(--fontSize-base);--text-sm-lineHeight: var(--lead-5);--text-sm-fontSize: var(--fontSize-sm);--text-xs-lineHeight: var(--lead-4);--text-xs-fontSize: var(--fontSize-xs);--shadow-2xl: 0px 25px 50px -12px var(--color-gray-900);--shadow-xl: 0px 20px 25px -5px var(--color-gray-400), 0px 8px 10px -6px #000000;--color-secondary-900: var(--color-gray-900);--color-secondary-800: var(--color-gray-800);--color-secondary-700: var(--color-gray-700);--color-secondary-600: var(--color-gray-600);--color-secondary-500: var(--color-gray-500);--color-secondary-400: var(--color-gray-400);--color-secondary-300: var(--color-gray-300);--color-secondary-200: var(--color-gray-200);--color-secondary-100: var(--color-gray-100);--color-secondary-50: var(--color-gray-50);--prose-a-code-background-hover: var(--typography-color-primary-50);--prose-a-code-border-color-hover: var(--typography-color-primary-500);--prose-a-color-hover: var(--typography-color-primary-500);--typography-color-secondary-900: var(--color-secondary-900);--typography-color-secondary-800: var(--color-secondary-800);--typography-color-secondary-700: var(--color-secondary-700);--typography-color-secondary-600: var(--color-secondary-600);--typography-color-secondary-500: var(--color-secondary-500);--typography-color-secondary-400: var(--color-secondary-400);--typography-color-secondary-300: var(--color-secondary-300);--typography-color-secondary-200: var(--color-secondary-200);--typography-color-secondary-100: var(--color-secondary-100);--typography-color-secondary-50: var(--color-secondary-50);--prose-code-inline-backgroundColor: var(--typography-color-secondary-100);--prose-code-inline-color: var(--typography-color-secondary-700);--prose-code-block-backgroundColor: var(--typography-color-secondary-100);--prose-code-block-color: var(--typography-color-secondary-700);--prose-code-block-border-color: var(--typography-color-secondary-200);--prose-tbody-tr-borderBottom-color: var(--typography-color-secondary-200);--prose-th-color: var(--typography-color-secondary-600);--prose-thead-borderBottom-color: var(--typography-color-secondary-200);--prose-thead-border-color: var(--typography-color-secondary-300);--prose-hr-color: var(--typography-color-secondary-200);--prose-blockquote-border-color: var(--typography-color-secondary-200);--prose-blockquote-color: var(--typography-color-secondary-500);--prose-a-code-border-color-static: var(--typography-color-secondary-400); } }@media { :root.dark {--pinceau-mq: dark; --alpine-backdrop-backgroundColor: #18181bb3;--prose-code-block-backdropFilter: contrast(1);--prose-ol-li-markerColor: currentColor;--prose-ul-li-markerColor: currentColor;--prose-a-code-color-hover: currentColor;--prose-a-code-color-static: currentColor;--prose-a-border-color-hover: currentColor;--prose-a-border-color-static: currentColor;--prose-a-color-static: inherit;--elements-backdrop-background: #0c0d0ccc;--alpine-body-color: var(--color-gray-200);--alpine-body-backgroundColor: var(--color-black);--typography-body-backgroundColor: var(--color-black);--typography-body-color: var(--color-white);--elements-state-danger-borderColor-secondary: var(--color-red-700);--elements-state-danger-borderColor-primary: var(--color-red-800);--elements-state-danger-backgroundColor-secondary: var(--color-red-800);--elements-state-danger-backgroundColor-primary: var(--color-red-900);--elements-state-danger-color-secondary: var(--color-red-200);--elements-state-danger-color-primary: var(--color-red-300);--elements-state-warning-borderColor-secondary: var(--color-yellow-700);--elements-state-warning-borderColor-primary: var(--color-yellow-800);--elements-state-warning-backgroundColor-secondary: var(--color-yellow-800);--elements-state-warning-backgroundColor-primary: var(--color-yellow-900);--elements-state-warning-color-secondary: var(--color-yellow-200);--elements-state-warning-color-primary: var(--color-yellow-400);--elements-state-success-borderColor-secondary: var(--color-green-700);--elements-state-success-borderColor-primary: var(--color-green-800);--elements-state-success-backgroundColor-secondary: var(--color-green-800);--elements-state-success-backgroundColor-primary: var(--color-green-900);--elements-state-success-color-secondary: var(--color-green-200);--elements-state-success-color-primary: var(--color-green-400);--elements-state-info-borderColor-secondary: var(--color-blue-700);--elements-state-info-borderColor-primary: var(--color-blue-800);--elements-state-info-backgroundColor-secondary: var(--color-blue-800);--elements-state-info-backgroundColor-primary: var(--color-blue-900);--elements-state-info-color-secondary: var(--color-blue-200);--elements-state-info-color-primary: var(--color-blue-400);--elements-state-primary-borderColor-secondary: var(--color-primary-700);--elements-state-primary-borderColor-primary: var(--color-primary-800);--elements-state-primary-backgroundColor-secondary: var(--color-primary-800);--elements-state-primary-backgroundColor-primary: var(--color-primary-900);--elements-state-primary-color-secondary: var(--color-primary-200);--elements-state-primary-color-primary: var(--color-primary-400);--elements-surface-secondary-backgroundColor: var(--color-gray-800);--elements-surface-primary-backgroundColor: var(--color-gray-900);--elements-surface-background-base: var(--color-gray-900);--elements-border-secondary-static: var(--color-gray-800);--elements-border-primary-hover: var(--color-gray-800);--elements-border-primary-static: var(--color-gray-900);--elements-text-secondary-color-hover: var(--color-gray-200);--elements-text-secondary-color-static: var(--color-gray-400);--elements-text-primary-color-static: var(--color-gray-50);--prose-a-code-background-hover: var(--typography-color-primary-900);--prose-a-code-border-color-hover: var(--typography-color-primary-600);--prose-a-color-hover: var(--typography-color-primary-400);--prose-code-inline-backgroundColor: var(--typography-color-secondary-800);--prose-code-inline-color: var(--typography-color-secondary-200);--prose-code-block-backgroundColor: var(--typography-color-secondary-900);--prose-code-block-color: var(--typography-color-secondary-200);--prose-code-block-border-color: var(--typography-color-secondary-800);--prose-tbody-tr-borderBottom-color: var(--typography-color-secondary-800);--prose-th-color: var(--typography-color-secondary-400);--prose-thead-borderBottom-color: var(--typography-color-secondary-800);--prose-thead-border-color: var(--typography-color-secondary-600);--prose-hr-color: var(--typography-color-secondary-800);--prose-blockquote-border-color: var(--typography-color-secondary-700);--prose-blockquote-color: var(--typography-color-secondary-400);--prose-a-code-border-color-static: var(--typography-color-secondary-600); } }</style><script>"use strict";(()=>{const a=window,e=document.documentElement,m=["dark","light"],c=window.localStorage.getItem("nuxt-color-mode")||"system";let n=c==="system"?f():c;const l=e.getAttribute("data-color-mode-forced");l&&(n=l),i(n),a["__NUXT_COLOR_MODE__"]={preference:c,value:n,getColorScheme:f,addColorScheme:i,removeColorScheme:d};function i(o){const t=""+o+"",s="";e.classList?e.classList.add(t):e.className+=" "+t,s&&e.setAttribute("data-"+s,o)}function d(o){const t=""+o+"",s="";e.classList?e.classList.remove(t):e.className=e.className.replace(new RegExp(t,"g"),""),s&&e.removeAttribute("data-"+s)}function r(o){return a.matchMedia("(prefers-color-scheme"+o+")")}function f(){if(a.matchMedia&&r("").media!=="not all"){for(const o of m)if(r(":"+o).matches)return o}return"light"}})();
</script></head>
<body ><div id="__nuxt"><div class="container pv-lrHVii pc-omEISc app-layout" data-v-93c22c3b data-v-6d327d86><!--[--><div class="nuxt-progress" style="width:0%;height:3px;opacity:0;background-size:Infinity% auto;" data-v-93c22c3b></div><header class="left" data-v-93c22c3b data-v-ebfd1e7c><div class="menu" data-v-ebfd1e7c><button aria-label="Navigation Menu" data-v-ebfd1e7c><svg width="24" height="24" viewBox="0 0 68 68" fill="currentColor" xmlns="http://www.w3.org/2000/svg" data-v-ebfd1e7c><path d="M8 34C8 32.1362 8 31.2044 8.30448 30.4693C8.71046 29.4892 9.48915 28.7105 10.4693 28.3045C11.2044 28 12.1362 28 14 28C15.8638 28 16.7956 28 17.5307 28.3045C18.5108 28.7105 19.2895 29.4892 19.6955 30.4693C20 31.2044 20 32.1362 20 34C20 35.8638 20 36.7956 19.6955 37.5307C19.2895 38.5108 18.5108 39.2895 17.5307 39.6955C16.7956 40 15.8638 40 14 40C12.1362 40 11.2044 40 10.4693 39.6955C9.48915 39.2895 8.71046 38.5108 8.30448 37.5307C8 36.7956 8 35.8638 8 34Z" data-v-ebfd1e7c></path><path d="M28 34C28 32.1362 28 31.2044 28.3045 30.4693C28.7105 29.4892 29.4892 28.7105 30.4693 28.3045C31.2044 28 32.1362 28 34 28C35.8638 28 36.7956 28 37.5307 28.3045C38.5108 28.7105 39.2895 29.4892 39.6955 30.4693C40 31.2044 40 32.1362 40 34C40 35.8638 40 36.7956 39.6955 37.5307C39.2895 38.5108 38.5108 39.2895 37.5307 39.6955C36.7956 40 35.8638 40 34 40C32.1362 40 31.2044 40 30.4693 39.6955C29.4892 39.2895 28.7105 38.5108 28.3045 37.5307C28 36.7956 28 35.8638 28 34Z" data-v-ebfd1e7c></path><path d="M48 34C48 32.1362 48 31.2044 48.3045 30.4693C48.7105 29.4892 49.4892 28.7105 50.4693 28.3045C51.2044 28 52.1362 28 54 28C55.8638 28 56.7956 28 57.5307 28.3045C58.5108 28.7105 59.2895 29.4892 59.6955 30.4693C60 31.2044 60 32.1362 60 34C60 35.8638 60 36.7956 59.6955 37.5307C59.2895 38.5108 58.5108 39.2895 57.5307 39.6955C56.7956 40 55.8638 40 54 40C52.1362 40 51.2044 40 50.4693 39.6955C49.4892 39.2895 48.7105 38.5108 48.3045 37.5307C48 36.7956 48 35.8638 48 34Z" data-v-ebfd1e7c></path></svg></button></div><div class="overlay" data-v-ebfd1e7c><nav data-v-ebfd1e7c data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div><div class="logo" data-v-ebfd1e7c><a href="/" class="" data-v-ebfd1e7c><img src="/logo-dark.svg" class="dark-img" alt="alpine" width="89" height="31" data-v-ebfd1e7c><img src="/logo.svg" class="light-img" alt="alpine" width="89" height="31" data-v-ebfd1e7c></a></div><div class="main-nav" data-v-ebfd1e7c><nav data-v-ebfd1e7c data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div></header><!--[--><div class="document-driven-page"><article data-v-f252e39d><a href="/articles" class="back" data-v-f252e39d><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-f252e39d style="" width="1em" height="1em" viewBox="0 0 256 256" data-v-121c6e7d><path fill="currentColor" d="M224 128a8 8 0 0 1-8 8H59.31l58.35 58.34a8 8 0 0 1-11.32 11.32l-72-72a8 8 0 0 1 0-11.32l72-72a8 8 0 0 1 11.32 11.32L59.31 120H216a8 8 0 0 1 8 8Z"/></svg><span data-v-f252e39d> Back </span></a><header data-v-f252e39d><h1 class="title" data-v-f252e39d>Welcome Post</h1><time datetime="2022-11-15T00:00:00.000Z" data-v-f252e39d>November 15, 2022</time></header><div class="prose" data-v-f252e39d><!--[--><div><h1 id="welcome-post" data-v-a5759516><a aria-current="page" href="/articles/2022-11-15-welcome-post#welcome-post" class="router-link-active router-link-exact-active" data-v-a5759516><!--[-->Welcome Post<!--]--><!----></a></h1><h2 id="welcome" data-v-1daf0210><a aria-current="page" href="/articles/2022-11-15-welcome-post#welcome" class="router-link-active router-link-exact-active" data-v-1daf0210><!--[-->Welcome<!--]--><!----></a></h2><p data-v-63bfa697><!--[-->My name is Ignasi Mas, I am a Machine Learning/AI Engineer with a broad interest in solving data issues. My background is built in Computer Vision, although my interest focus (which obviously includes Computer Vision) is wider.<!--]--></p><hr id data-v-a7e94ae7><h3 id="about-me" data-v-4ddca5e2><a aria-current="page" href="/articles/2022-11-15-welcome-post#about-me" class="router-link-active router-link-exact-active" data-v-4ddca5e2><!--[-->About me<!--]--><!----></a></h3><p data-v-63bfa697><!--[-->I studied Telecommunication Engineering at <a href="https://telecos.upc.edu/acl_users/credentials_cookie_auth/require_login?came_from=https%3A//telecos.upc.edu/es" rel="nofollow" data-v-af1c0c3b><!--[-->ETSETB, UPC<!--]--></a>. Some day we may talk deeply about my experience studying this career, but for now let&#39;s just say that the content in it is acquiring the knowledge to understand each point in the signal lifecycle.<!--]--></p><p data-v-63bfa697><!--[-->What&#39;s the particularity about this? Well, our knowledge about signal has increased so much during the last decades, that more and more issues and solutions have flourished, thus becoming Telecommunication Engineering a vastly wide career. That is of course something good but is also more sensible to unrelated limitations. In the end, time is limited, and if you want to cover everything you lose granularity.<!--]--></p><p data-v-63bfa697><!--[-->During my progress in my career, I felt more and more attracted to the development of intelligent systems. In that context, I demanded more knowledge than what I was getting, but the loss of granularity mentioned above made that impossible. Once I graduated I felt incomplete about that. I missed something. And that something was Machine Learning. Based <a href="https://www.andrewng.org/" rel="nofollow" data-v-af1c0c3b><!--[-->Andrew NG<!--]--></a> opened my eyes through <a href="https://www.coursera.org/learn/machine-learning" rel="nofollow" data-v-af1c0c3b><!--[-->its course<!--]--></a>.<!--]--></p><p data-v-63bfa697><!--[-->I found the following piece in my career&#39;s puzzle as the <a href="https://www.uab.cat/web/estudiar/official-master-s-degrees/general-information/computer-vision-1096480962610.html?param1=1345648392514" rel="nofollow" data-v-af1c0c3b><!--[-->Master in Computer Vision<!--]--></a> from <a href="http://www.cvc.uab.es/" rel="nofollow" data-v-af1c0c3b><!--[-->Computer Vision Center<!--]--></a>. There perhaps I may be able to merge two of my main academic interests, Machine Learning and Computer Vision. Two years later, I didn&#39;t regret that decision. This Master fed me with the seeds to gain further knowledge, and begin learning everything in the wild.<!--]--></p><p data-v-63bfa697><!--[-->But my adventure still had to deliver another incredible chapter. That was the development of my Master&#39;s thesis. I wanted to focus on something of my special interest, so I researched open and hot topics in Machine Learning. I had many interests so it was hard to choose, but I found in Few-Shot Learning one of my main focuses. There, I learned about Meta-Learning and found one of the potentially needed from Meta-Learning problems in ML: Active Learning.<!--]--></p><p data-v-63bfa697><!--[-->In this blog, we will have time to talk about Meta-Learning and Active Learning further, but just know that I developed my Master&#39;s thesis about Meta-Active Learning. I.e. using Meta-Learning to solve Active Learning problems. I focused on one concrete Active Learning scenario (again, we will study these scenarios someday in this space). I sweated blood just to replicate the State of the Art approaches. I remember the days trying to handle memory in my PyTorch tensors… (in Meta-Learning, memory management works differently since you keep different gradients for different levels… but no more spoilers). And I almost jumped out of joy the day I had some reasonable results in my proposed solutions.<!--]--></p><p data-v-63bfa697><!--[-->This chapter did not finish presenting my dissertation for the Master (which I obviously did and finally got), but I presented it to the MDALC Workshop at <a href="https://iccv2019.thecvf.com/" rel="nofollow" data-v-af1c0c3b><!--[-->ICCV 2019<!--]--></a>, and they accepted it! We published <a href="https://ieeexplore.ieee.org/document/9022361" rel="nofollow" data-v-af1c0c3b><!--[-->the paper<!--]--></a> and it is accessible to anyone since then.<!--]--></p><p data-v-63bfa697><!--[-->Is my academic history finished yet? Of course not! I am actively thinking about a possible PhD that I may do someday. I actually had a couple of opportunities that in the end were not materialized at all, but for sure there will be more. It is not something time sensitive right now, but it would be another way to acquire and deliver more knowledge. Actually, this will not necessarily be my only path, I may find other ways to do so. So new adventures await!<!--]--></p><p data-v-63bfa697><!--[--><em data-v-177b5f01><!--[-->Oh, but Ignasi, you didn&#39;t tell us about your professional trajectory<!--]--></em>. Yes, indeed. I have plenty of experiences and cool projects along with incredible people that I participated in. But that is probably a story for another space. Or maybe another day…<!--]--></p><h3 id="about-this-blog" data-v-4ddca5e2><a aria-current="page" href="/articles/2022-11-15-welcome-post#about-this-blog" class="router-link-active router-link-exact-active" data-v-4ddca5e2><!--[-->About this blog<!--]--><!----></a></h3><p data-v-63bfa697><!--[-->In the current episode, I realized that I spend time browsing the State of the Art in some matters and playing with code, but it is something I have always done by myself. However, wait.... Why not share it with everyone? It is a good trade. You get my knowledge and I get your feedback. So, how can I share it with everyone? Oh, a blog! It is an easy tool where I may focus on the content, instead of the shape. If this grows, I may redefine it later. But at my beginnings, that is the idea.<!--]--></p><p data-v-63bfa697><!--[-->At the time I am writing this, I still have to do <strong data-v-9d7bd52e><!--[-->everything<!--]--></strong> in the blog. My idea here is to post in a more or less formal way (I want to make it easy to read for you) posts about different theory ML topics of my interest as well as maybe some practical exercises where I&#39;ll share my reasoning live.<!--]--></p><p data-v-63bfa697><!--[-->I can&#39;t tell you the frequency at which I will be posting. My idea first is to try to post every two weeks (time enough for researching some problem and preparing the post while I am working because I need to eat, you know?), but I can&#39;t promise anything yet.<!--]--></p><p data-v-63bfa697><!--[-->In the first weeks, I will post topics I already know about and focus on how to present them to you. This way, I will train myself for the future, in which I will deliver to you new topics.<!--]--></p><p data-v-63bfa697><!--[-->So just one more thing: let&#39;s have fun together!<!--]--></p></div><!--]--><div class="back-to-top" data-v-f252e39d><a data-v-f252e39d data-v-af1c0c3b><!--[-->Back to top <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-f252e39d style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M11 20V7.825l-5.6 5.6L4 12l8-8l8 8l-1.4 1.425l-5.6-5.6V20h-2Z"/></svg><!--]--></a></div></div></article></div><!--]--><footer class="" data-v-93c22c3b data-v-d63b5c07><a href="https://github.com/Metabloggism/metabloggism.github.io" rel="noopener noreferrer" class="credits" data-v-d63b5c07>Metabloggism</a><div class="navigation" data-v-d63b5c07><nav data-v-d63b5c07 data-v-47e45ff0><ul data-v-47e45ff0><!--[--><li data-v-47e45ff0><a href="/" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> About</a></li><li data-v-47e45ff0><a href="/articles" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Articles</a></li><li data-v-47e45ff0><a href="/contact" class="" data-v-47e45ff0><span class="underline-fx" data-v-47e45ff0></span> Contact</a></li><!--]--></ul></nav></div><p class="message" data-v-d63b5c07>Follow me on</p><div class="icons" data-v-d63b5c07><div class="social" data-v-d63b5c07><!--[--><a href="https://github.com/https://github.com/MrLeylo" rel="noopener noreferrer" target="_blank" title="https://github.com/MrLeylo" aria-label="https://github.com/MrLeylo" data-v-ddf2f94a><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-ddf2f94a style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M12 2.247a10 10 0 0 0-3.162 19.487c.5.088.687-.212.687-.475c0-.237-.012-1.025-.012-1.862c-2.513.462-3.163-.613-3.363-1.175a3.636 3.636 0 0 0-1.025-1.413c-.35-.187-.85-.65-.013-.662a2.001 2.001 0 0 1 1.538 1.025a2.137 2.137 0 0 0 2.912.825a2.104 2.104 0 0 1 .638-1.338c-2.225-.25-4.55-1.112-4.55-4.937a3.892 3.892 0 0 1 1.025-2.688a3.594 3.594 0 0 1 .1-2.65s.837-.262 2.75 1.025a9.427 9.427 0 0 1 5 0c1.912-1.3 2.75-1.025 2.75-1.025a3.593 3.593 0 0 1 .1 2.65a3.869 3.869 0 0 1 1.025 2.688c0 3.837-2.338 4.687-4.563 4.937a2.368 2.368 0 0 1 .675 1.85c0 1.338-.012 2.413-.012 2.75c0 .263.187.575.687.475A10.005 10.005 0 0 0 12 2.247Z"/></svg></a><a href="https://www.linkedin.com/company/nuxtlabs" rel="noopener noreferrer" target="_blank" title="LinkedIn" aria-label="LinkedIn" data-v-ddf2f94a><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="icon" data-v-ddf2f94a style="" width="1em" height="1em" viewBox="0 0 24 24" data-v-121c6e7d><path fill="currentColor" d="M20.47 2H3.53a1.45 1.45 0 0 0-1.47 1.43v17.14A1.45 1.45 0 0 0 3.53 22h16.94a1.45 1.45 0 0 0 1.47-1.43V3.43A1.45 1.45 0 0 0 20.47 2ZM8.09 18.74h-3v-9h3ZM6.59 8.48a1.56 1.56 0 1 1 0-3.12a1.57 1.57 0 1 1 0 3.12Zm12.32 10.26h-3v-4.83c0-1.21-.43-2-1.52-2A1.65 1.65 0 0 0 12.85 13a2 2 0 0 0-.1.73v5h-3v-9h3V11a3 3 0 0 1 2.71-1.5c2 0 3.45 1.29 3.45 4.06Z"/></svg></a><!--]--></div><div class="color-mode-switch" data-v-d63b5c07><button aria-label="Color Mode" data-v-d63b5c07 data-v-693a1e33><span data-v-693a1e33></span></button></div></div></footer><!--]--></div></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true" data-src="/articles/2022-11-15-welcome-post/_payload.json">[{"state":1,"_errors":3096,"serverRendered":3056,"path":7,"prerenderedAt":-1},["Reactive",2],{"$sdd-pages":3,"$sdd-surrounds":223,"$sdd-globals":3053,"$scolor-mode":3055,"$sdd-navigation":3057,"$sicons":3084},["ShallowRef",4],["ShallowReactive",5],{"/articles/2022-11-15-welcome-post":6},{"_path":7,"_dir":8,"_draft":9,"_partial":9,"_locale":10,"title":11,"description":12,"layout":13,"date":14,"cover":15,"author":16,"body":20,"_type":218,"_id":219,"_source":220,"_file":221,"_extension":222},"/articles/2022-11-15-welcome-post","articles",false,"","Welcome Post","Learn how to configure Alpine with the app.config.ts file.","article","2022-11-15T00:00:00.000Z","/articles/get-started.webp",{"name":17,"avatarUrl":18,"link":19},"Ignasi Mas aka Mr. Leylo","https://pbs.twimg.com/profile_images/1042510623962275840/1Iw_Mvud_400x400.jpg","https://twitter.com/atinux",{"type":21,"children":22,"toc":210},"root",[23,31,38,44,48,55,71,76,99,122,127,132,155,160,171,177,182,195,200,205],{"type":24,"tag":25,"props":26,"children":28},"element","h1",{"id":27},"welcome-post",[29],{"type":30,"value":11},"text",{"type":24,"tag":32,"props":33,"children":35},"h2",{"id":34},"welcome",[36],{"type":30,"value":37},"Welcome",{"type":24,"tag":39,"props":40,"children":41},"p",{},[42],{"type":30,"value":43},"My name is Ignasi Mas, I am a Machine Learning/AI Engineer with a broad interest in solving data issues. My background is built in Computer Vision, although my interest focus (which obviously includes Computer Vision) is wider.",{"type":24,"tag":45,"props":46,"children":47},"hr",{"id":10},[],{"type":24,"tag":49,"props":50,"children":52},"h3",{"id":51},"about-me",[53],{"type":30,"value":54},"About me",{"type":24,"tag":39,"props":56,"children":57},{},[58,60,69],{"type":30,"value":59},"I studied Telecommunication Engineering at ",{"type":24,"tag":61,"props":62,"children":66},"a",{"href":63,"rel":64},"https://telecos.upc.edu/acl_users/credentials_cookie_auth/require_login?came_from=https%3A//telecos.upc.edu/es",[65],"nofollow",[67],{"type":30,"value":68},"ETSETB, UPC",{"type":30,"value":70},". Some day we may talk deeply about my experience studying this career, but for now let's just say that the content in it is acquiring the knowledge to understand each point in the signal lifecycle.",{"type":24,"tag":39,"props":72,"children":73},{},[74],{"type":30,"value":75},"What's the particularity about this? Well, our knowledge about signal has increased so much during the last decades, that more and more issues and solutions have flourished, thus becoming Telecommunication Engineering a vastly wide career. That is of course something good but is also more sensible to unrelated limitations. In the end, time is limited, and if you want to cover everything you lose granularity.",{"type":24,"tag":39,"props":77,"children":78},{},[79,81,88,90,97],{"type":30,"value":80},"During my progress in my career, I felt more and more attracted to the development of intelligent systems. In that context, I demanded more knowledge than what I was getting, but the loss of granularity mentioned above made that impossible. Once I graduated I felt incomplete about that. I missed something. And that something was Machine Learning. Based ",{"type":24,"tag":61,"props":82,"children":85},{"href":83,"rel":84},"https://www.andrewng.org/",[65],[86],{"type":30,"value":87},"Andrew NG",{"type":30,"value":89}," opened my eyes through ",{"type":24,"tag":61,"props":91,"children":94},{"href":92,"rel":93},"https://www.coursera.org/learn/machine-learning",[65],[95],{"type":30,"value":96},"its course",{"type":30,"value":98},".",{"type":24,"tag":39,"props":100,"children":101},{},[102,104,111,113,120],{"type":30,"value":103},"I found the following piece in my career's puzzle as the ",{"type":24,"tag":61,"props":105,"children":108},{"href":106,"rel":107},"https://www.uab.cat/web/estudiar/official-master-s-degrees/general-information/computer-vision-1096480962610.html?param1=1345648392514",[65],[109],{"type":30,"value":110},"Master in Computer Vision",{"type":30,"value":112}," from ",{"type":24,"tag":61,"props":114,"children":117},{"href":115,"rel":116},"http://www.cvc.uab.es/",[65],[118],{"type":30,"value":119},"Computer Vision Center",{"type":30,"value":121},". There perhaps I may be able to merge two of my main academic interests, Machine Learning and Computer Vision. Two years later, I didn't regret that decision. This Master fed me with the seeds to gain further knowledge, and begin learning everything in the wild.",{"type":24,"tag":39,"props":123,"children":124},{},[125],{"type":30,"value":126},"But my adventure still had to deliver another incredible chapter. That was the development of my Master's thesis. I wanted to focus on something of my special interest, so I researched open and hot topics in Machine Learning. I had many interests so it was hard to choose, but I found in Few-Shot Learning one of my main focuses. There, I learned about Meta-Learning and found one of the potentially needed from Meta-Learning problems in ML: Active Learning.",{"type":24,"tag":39,"props":128,"children":129},{},[130],{"type":30,"value":131},"In this blog, we will have time to talk about Meta-Learning and Active Learning further, but just know that I developed my Master's thesis about Meta-Active Learning. I.e. using Meta-Learning to solve Active Learning problems. I focused on one concrete Active Learning scenario (again, we will study these scenarios someday in this space). I sweated blood just to replicate the State of the Art approaches. I remember the days trying to handle memory in my PyTorch tensors… (in Meta-Learning, memory management works differently since you keep different gradients for different levels… but no more spoilers). And I almost jumped out of joy the day I had some reasonable results in my proposed solutions.",{"type":24,"tag":39,"props":133,"children":134},{},[135,137,144,146,153],{"type":30,"value":136},"This chapter did not finish presenting my dissertation for the Master (which I obviously did and finally got), but I presented it to the MDALC Workshop at ",{"type":24,"tag":61,"props":138,"children":141},{"href":139,"rel":140},"https://iccv2019.thecvf.com/",[65],[142],{"type":30,"value":143},"ICCV 2019",{"type":30,"value":145},", and they accepted it! We published ",{"type":24,"tag":61,"props":147,"children":150},{"href":148,"rel":149},"https://ieeexplore.ieee.org/document/9022361",[65],[151],{"type":30,"value":152},"the paper",{"type":30,"value":154}," and it is accessible to anyone since then.",{"type":24,"tag":39,"props":156,"children":157},{},[158],{"type":30,"value":159},"Is my academic history finished yet? Of course not! I am actively thinking about a possible PhD that I may do someday. I actually had a couple of opportunities that in the end were not materialized at all, but for sure there will be more. It is not something time sensitive right now, but it would be another way to acquire and deliver more knowledge. Actually, this will not necessarily be my only path, I may find other ways to do so. So new adventures await!",{"type":24,"tag":39,"props":161,"children":162},{},[163,169],{"type":24,"tag":164,"props":165,"children":166},"em",{},[167],{"type":30,"value":168},"Oh, but Ignasi, you didn't tell us about your professional trajectory",{"type":30,"value":170},". Yes, indeed. I have plenty of experiences and cool projects along with incredible people that I participated in. But that is probably a story for another space. Or maybe another day…",{"type":24,"tag":49,"props":172,"children":174},{"id":173},"about-this-blog",[175],{"type":30,"value":176},"About this blog",{"type":24,"tag":39,"props":178,"children":179},{},[180],{"type":30,"value":181},"In the current episode, I realized that I spend time browsing the State of the Art in some matters and playing with code, but it is something I have always done by myself. However, wait.... Why not share it with everyone? It is a good trade. You get my knowledge and I get your feedback. So, how can I share it with everyone? Oh, a blog! It is an easy tool where I may focus on the content, instead of the shape. If this grows, I may redefine it later. But at my beginnings, that is the idea.",{"type":24,"tag":39,"props":183,"children":184},{},[185,187,193],{"type":30,"value":186},"At the time I am writing this, I still have to do ",{"type":24,"tag":188,"props":189,"children":190},"strong",{},[191],{"type":30,"value":192},"everything",{"type":30,"value":194}," in the blog. My idea here is to post in a more or less formal way (I want to make it easy to read for you) posts about different theory ML topics of my interest as well as maybe some practical exercises where I'll share my reasoning live.",{"type":24,"tag":39,"props":196,"children":197},{},[198],{"type":30,"value":199},"I can't tell you the frequency at which I will be posting. My idea first is to try to post every two weeks (time enough for researching some problem and preparing the post while I am working because I need to eat, you know?), but I can't promise anything yet.",{"type":24,"tag":39,"props":201,"children":202},{},[203],{"type":30,"value":204},"In the first weeks, I will post topics I already know about and focus on how to present them to you. This way, I will train myself for the future, in which I will deliver to you new topics.",{"type":24,"tag":39,"props":206,"children":207},{},[208],{"type":30,"value":209},"So just one more thing: let's have fun together!",{"title":10,"searchDepth":211,"depth":211,"links":212},2,[213],{"id":34,"depth":211,"text":37,"children":214},[215,217],{"id":51,"depth":216,"text":54},3,{"id":173,"depth":216,"text":176},"markdown","content:articles:2022-11-15-welcome-post.md","content","articles/2022-11-15-welcome-post.md","md",["ShallowRef",224],["ShallowReactive",225],{"/articles/2022-11-15-welcome-post":226},[227,248],{"_path":228,"_dir":10,"_draft":9,"_partial":9,"_locale":10,"title":229,"description":10,"layout":230,"body":231,"_type":218,"_id":246,"_source":220,"_file":247,"_extension":222},"/contact","Contact","default",{"type":21,"children":232,"toc":244},[233,239],{"type":24,"tag":25,"props":234,"children":236},{"id":235},"get-in-touch",[237],{"type":30,"value":238},"Get in touch",{"type":24,"tag":240,"props":241,"children":243},"contact-form",{":fields":242},"[{\"type\":\"text\",\"name\":\"name\",\"label\":\"Your name\",\"required\":true},{\"type\":\"email\",\"name\":\"email\",\"label\":\"Your email\",\"required\":true},{\"type\":\"text\",\"name\":\"subject\",\"label\":\"Subject\",\"required\":false},{\"type\":\"textarea\",\"name\":\"message\",\"label\":\"Message\",\"required\":true}]",[],{"title":10,"searchDepth":211,"depth":211,"links":245},[],"content:3.contact.md","3.contact.md",{"_path":249,"_dir":8,"_draft":9,"_partial":9,"_locale":10,"title":250,"description":12,"cover":251,"author":252,"date":256,"layout":13,"body":257,"_type":218,"_id":3051,"_source":220,"_file":3052,"_extension":222},"/articles/2022-11-21-meta-learning","Meta-Learning explained","/articles/configure-alpine.webp",{"name":253,"avatarUrl":254,"link":255},"Clément Ollivier","https://pbs.twimg.com/profile_images/1370286658432724996/ZMSDzzIi_400x400.jpg","https://twitter.com/clemcodes","2022-11-21T00:00:00.000Z",{"type":21,"children":258,"toc":3032},[259,267,276,297,330,343,352,398,406,428,489,576,678,686,711,915,922,955,965,972,980,987,995,1002,1007,1016,1105,1166,1177,1203,1265,1272,1338,1345,1350,1386,1410,1417,1429,1438,1464,1501,1508,1555,1573,1582,1603,1609,1670,1677,1683,1767,1774,1781,1787,1937,1944,1951,1958,1964,2159,2166,2173,2180,2187,2194,2200,2245,2252,2258,2263,2289,2298,2328,2337,2349,2368,2426,2444,2453,2474,2483,2488,2497,2523,2546,2569,2593,2616,2639,2662,2685,2708,2731,2755,2778,2801,2824,2847,2870,2894,2917,2940,2963,2986,3009],{"type":24,"tag":25,"props":260,"children":262},{"id":261},"meta-learning-explained",[263],{"type":24,"tag":188,"props":264,"children":265},{},[266],{"type":30,"value":250},{"type":24,"tag":32,"props":268,"children":270},{"id":269},"what-will-be-reviewed",[271],{"type":24,"tag":188,"props":272,"children":273},{},[274],{"type":30,"value":275},"What will be reviewed",{"type":24,"tag":39,"props":277,"children":278},{},[279,281,286,288,295],{"type":30,"value":280},"In this first post (aside from the Welcome one) I will expose a brief summary of ",{"type":24,"tag":188,"props":282,"children":283},{},[284],{"type":30,"value":285},"Meta-Learning",{"type":30,"value":287},". It was one of the topics I researched for ",{"type":24,"tag":61,"props":289,"children":292},{"href":290,"rel":291},"https://upcommons.upc.edu/bitstream/handle/2117/179428/cMas.pdf;jsessionid=18807FB3EE2D5343E5F0DF9A5BA37D7F?sequence=1",[65],[293],{"type":30,"value":294},"my Master's dissertation",{"type":30,"value":296}," (and one of the main topics I actually developed it about) and since then, one of the topics I am more interested in.",{"type":24,"tag":39,"props":298,"children":299},{},[300,302,307,309,314,316,321,323,328],{"type":30,"value":301},"As you will see, the summary will not be the latest trend, because I want to give it a more ",{"type":24,"tag":188,"props":303,"children":304},{},[305],{"type":30,"value":306},"historical explanation",{"type":30,"value":308},". The ",{"type":24,"tag":188,"props":310,"children":311},{},[312],{"type":30,"value":313},"State of the Art may be reviewed in a different post",{"type":30,"value":315},". Instead, this post will focus on taking an interesting ",{"type":24,"tag":188,"props":317,"children":318},{},[319],{"type":30,"value":320},"tour along the Meta-Learning evolution",{"type":30,"value":322}," and understanding the ",{"type":24,"tag":188,"props":324,"children":325},{},[326],{"type":30,"value":327},"context",{"type":30,"value":329}," to build more efficiently in the future.",{"type":24,"tag":39,"props":331,"children":332},{},[333,335,341],{"type":30,"value":334},"No need to say, but the content does not end at this post. You can contact me at ",{"type":24,"tag":61,"props":336,"children":338},{"href":337},"i.masmend@gmail.com",[339],{"type":30,"value":340},"my mail",{"type":30,"value":342}," (info is also below in the blog) if you have any doubt or suggestion and I'll be happy to discuss.",{"type":24,"tag":32,"props":344,"children":346},{"id":345},"what-is-meta-learning",[347],{"type":24,"tag":188,"props":348,"children":349},{},[350],{"type":30,"value":351},"What is Meta-Learning?",{"type":24,"tag":39,"props":353,"children":354},{},[355,357,362,364,369,371,376,378,383,385,390,392,397],{"type":30,"value":356},"Any ",{"type":24,"tag":188,"props":358,"children":359},{},[360],{"type":30,"value":361},"task or process",{"type":30,"value":363}," can involve a ",{"type":24,"tag":188,"props":365,"children":366},{},[367],{"type":30,"value":368},"process of learning",{"type":30,"value":370}," in order to ",{"type":24,"tag":188,"props":372,"children":373},{},[374],{"type":30,"value":375},"improve performance",{"type":30,"value":377}," in it. Systems are no strangers to this, for example, imagine a system that has to perform some kind of face identification. The system will perform better when it has lived a learning process before. Well, that is exactly the point of Machine Learning, isn't it? But let's switch the focus. ",{"type":24,"tag":188,"props":379,"children":380},{},[381],{"type":30,"value":382},"Learning itself is a process",{"type":30,"value":384},". So according to the previous statement, it ",{"type":24,"tag":188,"props":386,"children":387},{},[388],{"type":30,"value":389},"can also be learned",{"type":30,"value":391},". And this is the exact ",{"type":24,"tag":188,"props":393,"children":394},{},[395],{"type":30,"value":396},"definition of Meta-Learning: Learning to Learn",{"type":30,"value":98},{"type":24,"tag":39,"props":399,"children":400},{},[401],{"type":24,"tag":402,"props":403,"children":405},"img",{"alt":10,"src":404},"https://i.imgur.com/Wc5zMl2.png",[],{"type":24,"tag":39,"props":407,"children":408},{},[409,411,426],{"type":30,"value":410},"| ",{"type":24,"tag":412,"props":413,"children":414},"b",{},[415,417,424],{"type":30,"value":416},"Meme, credits to ",{"type":24,"tag":61,"props":418,"children":421},{"href":419,"rel":420},"https://twitter.com/joavanschoren",[65],[422],{"type":30,"value":423},"@joavanschoren",{"type":30,"value":425},"... but I cut out the end since it contained a spoiler of content in the post below",{"type":30,"value":427}," |",{"type":24,"tag":39,"props":429,"children":430},{},[431,433,438,440,445,447,452,454,459,461,466,468,473,475,480,482,487],{"type":30,"value":432},"From another perspective, we ",{"type":24,"tag":188,"props":434,"children":435},{},[436],{"type":30,"value":437},"humans do not learn most things from scratch",{"type":30,"value":439},". For example, if I present you with a ",{"type":24,"tag":188,"props":441,"children":442},{},[443],{"type":30,"value":444},"new bird species",{"type":30,"value":446}," (unknown to you) and tell you ",{"type":24,"tag":188,"props":448,"children":449},{},[450],{"type":30,"value":451},"\"Have you seen this bird?\"",{"type":30,"value":453}," you will probably ",{"type":24,"tag":188,"props":455,"children":456},{},[457],{"type":30,"value":458},"learn its patterns with just a quick look",{"type":30,"value":460},". This is because ",{"type":24,"tag":188,"props":462,"children":463},{},[464],{"type":30,"value":465},"you have seen so many birds",{"type":30,"value":467}," in your life, and you'll directly look at the differential features (feathers color, feet, beak shape...) to absorb the information. Also, ",{"type":24,"tag":188,"props":469,"children":470},{},[471],{"type":30,"value":472},"you may understand the context",{"type":30,"value":474}," (e.g. identify how does it fly with respect to the ground, wind, etc) because you already know things about this context. In contrast, if you present it to a ",{"type":24,"tag":188,"props":476,"children":477},{},[478],{"type":30,"value":479},"baby",{"type":30,"value":481},", he will ",{"type":24,"tag":188,"props":483,"children":484},{},[485],{"type":30,"value":486},"not understand at all because he still has to learn everything",{"type":30,"value":488},". Another simple example is how before analyzing a book from a literature perspective a kid has to learn how to read.",{"type":24,"tag":39,"props":490,"children":491},{},[492,494,498,500,505,507,512,514,519,521,526,528,533,535,540,542,547,549,554,556,561,563,568,570,575],{"type":30,"value":493},"Thus, ",{"type":24,"tag":188,"props":495,"children":496},{},[497],{"type":30,"value":285},{"type":30,"value":499}," is ",{"type":24,"tag":188,"props":501,"children":502},{},[503],{"type":30,"value":504},"extending the Learning process to one level above",{"type":30,"value":506},", and ",{"type":24,"tag":188,"props":508,"children":509},{},[510],{"type":30,"value":511},"Learning to Learn",{"type":30,"value":513},". And, how does that contribute? To make it simple, it makes the ",{"type":24,"tag":188,"props":515,"children":516},{},[517],{"type":30,"value":518},"Learning process more efficient",{"type":30,"value":520}," (in any way, which could be faster, more stable, more qualitative...), and this allows us to overcome some important issues (we'll discuss that later). When looking at that, we could say we work at ",{"type":24,"tag":188,"props":522,"children":523},{},[524],{"type":30,"value":525},"2 levels",{"type":30,"value":527},", the ",{"type":24,"tag":188,"props":529,"children":530},{},[531],{"type":30,"value":532},"Learning level",{"type":30,"value":534}," and the ",{"type":24,"tag":188,"props":536,"children":537},{},[538],{"type":30,"value":539},"Meta-Learning level",{"type":30,"value":541},". You may also note that this can be even extended one level above since Learning to Learn is another process. You are right, this can be done and we then would achieve the Meta-meta-Learning level, thus working at 3 levels. And this is also ",{"type":24,"tag":188,"props":543,"children":544},{},[545],{"type":30,"value":546},"extendible to any level until infinity",{"type":30,"value":548}," (so yes, you could learn how to learn how to learn how to learn...how to learn). However, such a process is obviously limited by our capacity. We as ",{"type":24,"tag":188,"props":550,"children":551},{},[552],{"type":30,"value":553},"humans, apply this at many levels",{"type":30,"value":555},", but it is also ",{"type":24,"tag":188,"props":557,"children":558},{},[559],{"type":30,"value":560},"limited by our brain capacity",{"type":30,"value":562},". From a ",{"type":24,"tag":188,"props":564,"children":565},{},[566],{"type":30,"value":567},"system",{"type":30,"value":569}," perspective, it is limited by its ",{"type":24,"tag":188,"props":571,"children":572},{},[573],{"type":30,"value":574},"computational power",{"type":30,"value":98},{"type":24,"tag":39,"props":577,"children":578},{},[579,581,586,588,592,594,599,601,606,608,613,615,620,622,627,629,634,636,641,643,648,650,655,657,662,664,669,671,676],{"type":30,"value":580},"So that said, how does this fit in out** Machine Learning** (ML) interest? Well, there are ",{"type":24,"tag":188,"props":582,"children":583},{},[584],{"type":30,"value":585},"different ways",{"type":30,"value":587}," of applying ",{"type":24,"tag":188,"props":589,"children":590},{},[591],{"type":30,"value":285},{"type":30,"value":593}," in ML that will be reviewed in this post, but keep in mind a setting where an ",{"type":24,"tag":188,"props":595,"children":596},{},[597],{"type":30,"value":598},"inner algorithm",{"type":30,"value":600}," works for a ",{"type":24,"tag":188,"props":602,"children":603},{},[604],{"type":30,"value":605},"prediction task",{"type":30,"value":607},". This algorithm learns under some ",{"type":24,"tag":188,"props":609,"children":610},{},[611],{"type":30,"value":612},"conditions",{"type":30,"value":614},", by updating some ",{"type":24,"tag":188,"props":616,"children":617},{},[618],{"type":30,"value":619},"model",{"type":30,"value":621}," by some ",{"type":24,"tag":188,"props":623,"children":624},{},[625],{"type":30,"value":626},"Learning Rule",{"type":30,"value":628}," depending in some ",{"type":24,"tag":188,"props":630,"children":631},{},[632],{"type":30,"value":633},"data",{"type":30,"value":635},". However, in a ",{"type":24,"tag":188,"props":637,"children":638},{},[639],{"type":30,"value":640},"vanilla ML",{"type":30,"value":642}," setting this decisions are usually taken ",{"type":24,"tag":188,"props":644,"children":645},{},[646],{"type":30,"value":647},"manually",{"type":30,"value":649}," and ",{"type":24,"tag":188,"props":651,"children":652},{},[653],{"type":30,"value":654},"suboptimal",{"type":30,"value":656},". In ",{"type":24,"tag":188,"props":658,"children":659},{},[660],{"type":30,"value":661},"Meta-learning",{"type":30,"value":663}," there would also be an ",{"type":24,"tag":188,"props":665,"children":666},{},[667],{"type":30,"value":668},"outer optimizer",{"type":30,"value":670}," whose task is to ",{"type":24,"tag":188,"props":672,"children":673},{},[674],{"type":30,"value":675},"optimize these decisions",{"type":30,"value":677},". Some ways to achieve it could be updating the Learning Rule, selecting the model (architecture or initial parameters) or rescheduling the data.",{"type":24,"tag":39,"props":679,"children":680},{},[681],{"type":24,"tag":402,"props":682,"children":685},{"alt":683,"src":684},"Imgur","https://i.imgur.com/zyRBmGS.png",[],{"type":24,"tag":39,"props":687,"children":688},{},[689,691,696,698,703,705,709],{"type":30,"value":690},"Until this point, there could be a bit of ",{"type":24,"tag":188,"props":692,"children":693},{},[694],{"type":30,"value":695},"confusion between Meta-Learning and other techniques",{"type":30,"value":697},". The difference is how is the ",{"type":24,"tag":188,"props":699,"children":700},{},[701],{"type":30,"value":702},"schedule",{"type":30,"value":704}," built and where is the ",{"type":24,"tag":188,"props":706,"children":707},{},[708],{"type":30,"value":633},{"type":30,"value":710}," taken from. In Meta-Learning the flow works as follows:",{"type":24,"tag":712,"props":713,"children":714},"blockquote",{},[715],{"type":24,"tag":39,"props":716,"children":717},{},[718,720,725,727,731,733,738,740,745,747,752,754,759,761,766,768,773,775,780,782,786,788,793,795,800,802,806,808,813,815,819,821,826,828,833,835,840,842,847,849,854,856,860,862,867,869,873,875,879,881,886,888,893,895,899,901,906,908,913],{"type":30,"value":719},"Pretend we are aiming to solve an specific ",{"type":24,"tag":188,"props":721,"children":722},{},[723],{"type":30,"value":724},"task",{"type":30,"value":726},". This ",{"type":24,"tag":188,"props":728,"children":729},{},[730],{"type":30,"value":724},{"type":30,"value":732}," may be drawn from a bigger ",{"type":24,"tag":188,"props":734,"children":735},{},[736],{"type":30,"value":737},"domain of tasks",{"type":30,"value":739},". In the example below, imagine we face a binary image classification task among a series of animal classes (monkey, dog, cat, elephant, fish, snake, hippo...). For example, imagine that we in the end will end up having to classify between dogs and snakes. We want to ",{"type":24,"tag":188,"props":741,"children":742},{},[743],{"type":30,"value":744},"learn how to learn",{"type":30,"value":746}," efficiently this specific task. We could define the domain as binary animal image classification tasks. The domain also includes a ",{"type":24,"tag":188,"props":748,"children":749},{},[750],{"type":30,"value":751},"series of conditions",{"type":30,"value":753}," below (RGB camera images, full body, denoised, real...). Now, along all the ",{"type":24,"tag":188,"props":755,"children":756},{},[757],{"type":30,"value":758},"domain",{"type":30,"value":760}," we may draw a ",{"type":24,"tag":188,"props":762,"children":763},{},[764],{"type":30,"value":765},"series of tasks",{"type":30,"value":767}," different than the one we are aiming to solve. To avoid this happening we may drop both dog and snake classes from a bag with all ",{"type":24,"tag":188,"props":769,"children":770},{},[771],{"type":30,"value":772},"classes",{"type":30,"value":774},", and build ",{"type":24,"tag":188,"props":776,"children":777},{},[778],{"type":30,"value":779},"tasks",{"type":30,"value":781}," by picking combinations of two classes. Thus, for each task, we will have to classify images between both classes and then that will be a binary animal image classification task. These ",{"type":24,"tag":188,"props":783,"children":784},{},[785],{"type":30,"value":779},{"type":30,"value":787}," will be ",{"type":24,"tag":188,"props":789,"children":790},{},[791],{"type":30,"value":792},"equivalent",{"type":30,"value":794}," to the ",{"type":24,"tag":188,"props":796,"children":797},{},[798],{"type":30,"value":799},"samples",{"type":30,"value":801}," in the ",{"type":24,"tag":188,"props":803,"children":804},{},[805],{"type":30,"value":532},{"type":30,"value":807}," (or ",{"type":24,"tag":188,"props":809,"children":810},{},[811],{"type":30,"value":812},"task level",{"type":30,"value":814},") but in the ",{"type":24,"tag":188,"props":816,"children":817},{},[818],{"type":30,"value":539},{"type":30,"value":820},". So, equivalently to what we would do at the ",{"type":24,"tag":188,"props":822,"children":823},{},[824],{"type":30,"value":825},"training level",{"type":30,"value":827}," we will build two ",{"type":24,"tag":188,"props":829,"children":830},{},[831],{"type":30,"value":832},"(meta-)sets",{"type":30,"value":834},". One will have ",{"type":24,"tag":188,"props":836,"children":837},{},[838],{"type":30,"value":839},"Meta-training tasks",{"type":30,"value":841}," while the other ",{"type":24,"tag":188,"props":843,"children":844},{},[845],{"type":30,"value":846},"Meta-test tasks",{"type":30,"value":848},". And yeah, if you do things correctly you would also have a ",{"type":24,"tag":188,"props":850,"children":851},{},[852],{"type":30,"value":853},"Meta-validation meta-set",{"type":30,"value":855},", of course. Then, for each ",{"type":24,"tag":188,"props":857,"children":858},{},[859],{"type":30,"value":724},{"type":30,"value":861}," we will work as always at the ",{"type":24,"tag":188,"props":863,"children":864},{},[865],{"type":30,"value":866},"Learning Level",{"type":30,"value":868},", getting ",{"type":24,"tag":188,"props":870,"children":871},{},[872],{"type":30,"value":799},{"type":30,"value":874}," for the ",{"type":24,"tag":188,"props":876,"children":877},{},[878],{"type":30,"value":724},{"type":30,"value":880}," and splitting them between **train, validation and test (as always). At this level, we will ",{"type":24,"tag":188,"props":882,"children":883},{},[884],{"type":30,"value":885},"train the model as usual",{"type":30,"value":887},", evaluate, etc., so we will end up having some ",{"type":24,"tag":188,"props":889,"children":890},{},[891],{"type":30,"value":892},"performance measure",{"type":30,"value":894}," (usually a Loss value). These individual task results will serve in the ",{"type":24,"tag":188,"props":896,"children":897},{},[898],{"type":30,"value":539},{"type":30,"value":900}," to ",{"type":24,"tag":188,"props":902,"children":903},{},[904],{"type":30,"value":905},"evaluate the outer optimizer",{"type":30,"value":907}," and making the corresponding ",{"type":24,"tag":188,"props":909,"children":910},{},[911],{"type":30,"value":912},"updates",{"type":30,"value":914},". Below in the example, each task updates the model, but it is just an example. Actually, just like at the Learning level, it can work by batches (in this case, batches of tasks). Just good luck with your hardware limitations. Then the Meta-test meta-set is used to evaluate by any given metric.",{"type":24,"tag":39,"props":916,"children":917},{},[918],{"type":24,"tag":402,"props":919,"children":921},{"alt":683,"src":920},"https://i.imgur.com/a9Fr97l.png",[],{"type":24,"tag":39,"props":923,"children":924},{},[925,927,932,934,939,941,946,948,953],{"type":30,"value":926},"Note that all this process is ",{"type":24,"tag":188,"props":928,"children":929},{},[930],{"type":30,"value":931},"designed from the beginning",{"type":30,"value":933}," to optimize the process of Learning in our target task. So our Meta-Learning schedule is indeed a ",{"type":24,"tag":188,"props":935,"children":936},{},[937],{"type":30,"value":938},"schedule for Learning to Learn",{"type":30,"value":940},". Any approach that falls into that definition is a Meta-Learning approach. You must also notice the difference between that and other similar techniques. For example, in ",{"type":24,"tag":188,"props":942,"children":943},{},[944],{"type":30,"value":945},"Transfer Learning",{"type":30,"value":947}," (another different whole topic) you do not learn how to learn for an specific task (or a task from an specific domain), but instead ",{"type":24,"tag":188,"props":949,"children":950},{},[951],{"type":30,"value":952},"use old knowledge to get closer to the optimal solution",{"type":30,"value":954}," (when you learned that old knowledge, you learned it for a whole different solution and was not intended to extend to any other different problem, thus there doesn't exist a Meta-Learning level). Some related topics are:",{"type":24,"tag":956,"props":957,"children":958},"ul",{},[959],{"type":24,"tag":960,"props":961,"children":962},"li",{},[963],{"type":30,"value":964},"Transfer Learning: using old knowledge optimized to solve another task to solve the current target task.",{"type":24,"tag":39,"props":966,"children":967},{},[968],{"type":24,"tag":402,"props":969,"children":971},{"alt":683,"src":970},"https://i.imgur.com/4kN4Xu8.png",[],{"type":24,"tag":956,"props":973,"children":974},{},[975],{"type":24,"tag":960,"props":976,"children":977},{},[978],{"type":30,"value":979},"Domain adaptation: learning from a different domain with some common conditions (e.g. in our example above learning the dog vs snake task from synthetic images).",{"type":24,"tag":39,"props":981,"children":982},{},[983],{"type":24,"tag":402,"props":984,"children":986},{"alt":683,"src":985},"https://i.imgur.com/FLTKEbi.png",[],{"type":24,"tag":956,"props":988,"children":989},{},[990],{"type":24,"tag":960,"props":991,"children":992},{},[993],{"type":30,"value":994},"Multimodal Learning: learning from different modes of data (e.g. a text description), although it can be viewed from a Meta-Learning level it covers a different topic and is treated differently (since it builds a different setting).",{"type":24,"tag":39,"props":996,"children":997},{},[998],{"type":24,"tag":402,"props":999,"children":1001},{"alt":683,"src":1000},"https://i.imgur.com/rTmjzmU.png",[],{"type":24,"tag":39,"props":1003,"children":1004},{},[1005],{"type":30,"value":1006},"Ok, so now we have an idea of what is Meta-Learning and how to use it. But why use it? Motivations are diverse and have varied over time. Actually, one can use it whenever it is beneficial for his task. But the important question is what did raise the interest of researchers to present schedules, definitions and solutions that include Meta-Learning? To do so, we may have a quick recap of Meta-Learning history.",{"type":24,"tag":32,"props":1008,"children":1010},{"id":1009},"origins-of-meta-learning",[1011],{"type":24,"tag":188,"props":1012,"children":1013},{},[1014],{"type":30,"value":1015},"Origins of Meta-Learning",{"type":24,"tag":39,"props":1017,"children":1018},{},[1019,1021,1031,1033,1038,1040,1045,1047,1052,1054,1059,1061,1066,1068,1073,1075,1080,1082,1087,1089,1096,1098,1103],{"type":30,"value":1020},"The term arose in a publication in Jürgen Schmidhuber's thesis called ",{"type":24,"tag":61,"props":1022,"children":1025},{"href":1023,"rel":1024},"https://people.idsia.ch/~juergen/diploma1987ocr.pdf",[65],[1026],{"type":24,"tag":164,"props":1027,"children":1028},{},[1029],{"type":30,"value":1030},"Evolutionary Principles in Self-Referential Learning",{"type":30,"value":1032}," (1987). The paper is incredibly dense, but a mine of knowledge and talks about some deep topics such as Information, Entropy, Evolution... We may talk specifically about this paper in future posts, but what concerns us now is that it ",{"type":24,"tag":188,"props":1034,"children":1035},{},[1036],{"type":30,"value":1037},"presented the idea of Meta-Learning",{"type":30,"value":1039}," as a way to modify the ",{"type":24,"tag":188,"props":1041,"children":1042},{},[1043],{"type":30,"value":1044},"plans",{"type":30,"value":1046}," (the equivalent of what the ",{"type":24,"tag":188,"props":1048,"children":1049},{},[1050],{"type":30,"value":1051},"schedulers + optimizers",{"type":30,"value":1053}," in ML mean nowadays) in order to generalize to a ",{"type":24,"tag":188,"props":1055,"children":1056},{},[1057],{"type":30,"value":1058},"whole group of domains",{"type":30,"value":1060},". This group is again some kind of ",{"type":24,"tag":188,"props":1062,"children":1063},{},[1064],{"type":30,"value":1065},"(meta-)domain",{"type":30,"value":1067},", so it also needed some ",{"type":24,"tag":188,"props":1069,"children":1070},{},[1071],{"type":30,"value":1072},"(meta-)plan",{"type":30,"value":1074},". The hypothesis that Schmidhuber did back in 1987 is that this concept was extendable to ",{"type":24,"tag":188,"props":1076,"children":1077},{},[1078],{"type":30,"value":1079},"infinite levels of abstraction",{"type":30,"value":1081},", thus allowing the definition (apart from the domain level and the meta-level) of a meta-meta-level, a meta-meta-meta-level, and so on, although the paper also points the compromise that the realization has with the ",{"type":24,"tag":188,"props":1083,"children":1084},{},[1085],{"type":30,"value":1086},"computation capacity of the hardware",{"type":30,"value":1088}," (plus recall we are in 1987, there were no NVIDIA RTX 4090... actually ",{"type":24,"tag":61,"props":1090,"children":1093},{"href":1091,"rel":1092},"https://en.wikipedia.org/wiki/GeForce_256",[65],[1094],{"type":30,"value":1095},"the first GPU",{"type":30,"value":1097}," came out 12 years later). He proposed ",{"type":24,"tag":188,"props":1099,"children":1100},{},[1101],{"type":30,"value":1102},"2 ways",{"type":30,"value":1104}," to implement Meta-Learning:",{"type":24,"tag":956,"props":1106,"children":1107},{},[1108,1140],{"type":24,"tag":960,"props":1109,"children":1110},{},[1111,1113,1118,1120,1125,1127,1132,1134,1139],{"type":30,"value":1112},"First, as a ",{"type":24,"tag":188,"props":1114,"children":1115},{},[1116],{"type":30,"value":1117},"Genetic Algorithm",{"type":30,"value":1119},". My interpretation is that he was wondering about what nowadays is ",{"type":24,"tag":188,"props":1121,"children":1122},{},[1123],{"type":30,"value":1124},"Curriculum Learning",{"type":30,"value":1126},", but from a Meta-Learning perspective. He proposed that at the Meta-Level the plan should schedule the best samples for the domain level. His own concerns? About \"",{"type":24,"tag":188,"props":1128,"children":1129},{},[1130],{"type":30,"value":1131},"nature",{"type":30,"value":1133},"\" (AI then aimed more to mimic true intelligence) and ",{"type":24,"tag":188,"props":1135,"children":1136},{},[1137],{"type":30,"value":1138},"feasibility",{"type":30,"value":98},{"type":24,"tag":960,"props":1141,"children":1142},{},[1143,1145,1150,1152,1157,1159,1164],{"type":30,"value":1144},"Second, as a ",{"type":24,"tag":188,"props":1146,"children":1147},{},[1148],{"type":30,"value":1149},"hierarchy of classifiers",{"type":30,"value":1151}," building the Genetic Algorithm, which will act at the ",{"type":24,"tag":188,"props":1153,"children":1154},{},[1155],{"type":30,"value":1156},"meta-level",{"type":30,"value":1158},". Schmidhuber pointed out that this way the mechanism could work with a ",{"type":24,"tag":188,"props":1160,"children":1161},{},[1162],{"type":30,"value":1163},"fixed number of levels",{"type":30,"value":1165},", just the domain level (each classifier) and the meta-level (the Genetic Algorithm).",{"type":24,"tag":39,"props":1167,"children":1168},{},[1169,1173],{"type":24,"tag":402,"props":1170,"children":1172},{"alt":683,"src":1171},"https://i.imgur.com/KWobPej.png",[],{"type":24,"tag":402,"props":1174,"children":1176},{"alt":683,"src":1175},"https://i.imgur.com/blCSh2S.png",[],{"type":24,"tag":39,"props":1178,"children":1179},{},[1180,1182,1187,1189,1194,1196,1201],{"type":30,"value":1181},"Another interesting interpretation from the paper is the way a plan works in a Genetic Algorithm, which is similar to the environment in ",{"type":24,"tag":188,"props":1183,"children":1184},{},[1185],{"type":30,"value":1186},"Reinforcement Learning",{"type":30,"value":1188},". Thus, ",{"type":24,"tag":188,"props":1190,"children":1191},{},[1192],{"type":30,"value":1193},"surviving",{"type":30,"value":1195}," a plan in a Genetic Algorithm can be viewed as getting a ",{"type":24,"tag":188,"props":1197,"children":1198},{},[1199],{"type":30,"value":1200},"reward",{"type":30,"value":1202}," in Reinforcement Learning.",{"type":24,"tag":39,"props":1204,"children":1205},{},[1206,1208,1218,1220,1225,1227,1232,1234,1239,1241,1246,1248,1255,1257,1264],{"type":30,"value":1207},"5 years later, Schmidhuber made another important contribution to Meta-Learning in ",{"type":24,"tag":61,"props":1209,"children":1212},{"href":1210,"rel":1211},"https://people.idsia.ch/~juergen/FKI-147-91ocr.pdf",[65],[1213],{"type":24,"tag":164,"props":1214,"children":1215},{},[1216],{"type":30,"value":1217},"Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks",{"type":30,"value":1219},". He defined a series of sequential (by episodes or plain timesteps) problems. To make you an idea, one of these problems consisted in predicting the parking slot where some car parked given a series of sensor states (distributed along the parking ground) at different time instants. In this case, no episodes were used as it was an online problem (prediction was made at the same training time). Instead, a prediction network was used for the task. The particularity is that another ",{"type":24,"tag":188,"props":1221,"children":1222},{},[1223],{"type":30,"value":1224},"network at a level above learned the weights updates",{"type":30,"value":1226}," that the domain one should experiment. Thus, the sequence could be seen as an ",{"type":24,"tag":188,"props":1228,"children":1229},{},[1230],{"type":30,"value":1231},"artificial meta-level",{"type":30,"value":1233},", while at each timestep the inner network performed the task. In the offline setting, the behavior was the same, just defining the sequence through bounded episodes. This was the ",{"type":24,"tag":188,"props":1235,"children":1236},{},[1237],{"type":30,"value":1238},"first published Meta-Learning approach",{"type":30,"value":1240}," that worked for ",{"type":24,"tag":188,"props":1242,"children":1243},{},[1244],{"type":30,"value":1245},"practical tasks",{"type":30,"value":1247},". Surprisingly, ",{"type":24,"tag":61,"props":1249,"children":1252},{"href":1250,"rel":1251},"https://imgur.com/9uvwpUb",[65],[1253],{"type":30,"value":1254},"the word meta is missing",{"type":30,"value":1256}," in that paper, but the interpretation seems clear to me to give the idea of an inner and an outer model that Schmidhuber was working around at that time, isn't it? When I researched about that, ",{"type":24,"tag":61,"props":1258,"children":1261},{"href":1259,"rel":1260},"https://people.idsia.ch/~juergen/metalearning.html",[65],[1262],{"type":30,"value":1263},"Schmidhuber himself considered this as a way of Meta-Learning",{"type":30,"value":98},{"type":24,"tag":39,"props":1266,"children":1267},{},[1268],{"type":24,"tag":402,"props":1269,"children":1271},{"alt":683,"src":1270},"https://i.imgur.com/tDtaaKC.jpg",[],{"type":24,"tag":39,"props":1273,"children":1274},{},[1275,1277,1287,1289,1294,1296,1301,1303,1308,1310,1315,1317,1322,1324,1329,1331,1336],{"type":30,"value":1276},"But that was not the only meaningful publication of Meta-Learning in 1992. Bengio (both Samy and Yoshua) et al. published this same year the paper called ",{"type":24,"tag":61,"props":1278,"children":1281},{"href":1279,"rel":1280},"http://www.iro.umontreal.ca/~lisa/pointeurs/bengio_1995_oban.pdf",[65],[1282],{"type":24,"tag":164,"props":1283,"children":1284},{},[1285],{"type":30,"value":1286},"On the Optimization of a Synaptic Learning Rule",{"type":30,"value":1288},", which probably is the first definition of a ",{"type":24,"tag":188,"props":1290,"children":1291},{},[1292],{"type":30,"value":1293},"Meta-Learning setting",{"type":30,"value":1295}," how we imagine that nowadays (although again they do not refer to the word meta!). They just defined a framework with an ",{"type":24,"tag":188,"props":1297,"children":1298},{},[1299],{"type":30,"value":1300},"inner prediction algorithm",{"type":30,"value":1302}," (the previously called domain level), which actually was a ",{"type":24,"tag":188,"props":1304,"children":1305},{},[1306],{"type":30,"value":1307},"Neural Network",{"type":30,"value":1309}," (yes, Neural Networks existed before LeNet, didn't you know?) and was ",{"type":24,"tag":188,"props":1311,"children":1312},{},[1313],{"type":30,"value":1314},"optimized through a Synaptic Learning Rule",{"type":30,"value":1316},", which again was ",{"type":24,"tag":188,"props":1318,"children":1319},{},[1320],{"type":30,"value":1321},"optimized by an outer optimization algorithm",{"type":30,"value":1323}," (what would be the meta-level). The point of the paper is that this Synaptic Learning Rule should be ",{"type":24,"tag":188,"props":1325,"children":1326},{},[1327],{"type":30,"value":1328},"parametric",{"type":30,"value":1330},", so the outer optimizer should just ",{"type":24,"tag":188,"props":1332,"children":1333},{},[1334],{"type":30,"value":1335},"update its parameters",{"type":30,"value":1337},". Thus, by defining well the episodic nature of the updates at the beginning of the whole process, the Synaptic Learning Rule should be able to learn from the task results a generalization of the whole domain of tasks.",{"type":24,"tag":39,"props":1339,"children":1340},{},[1341],{"type":24,"tag":402,"props":1342,"children":1344},{"alt":683,"src":1343},"https://i.imgur.com/aw60MAG.png",[],{"type":24,"tag":39,"props":1346,"children":1347},{},[1348],{"type":30,"value":1349},"For sure 1992 was an important year for Meta-Learning! It was also the year I was born, so maybe it was my destiny to study this field.",{"type":24,"tag":39,"props":1351,"children":1352},{},[1353,1355,1360,1362,1372,1374,1384],{"type":30,"value":1354},"Later on, Schmidhuber continued his study in Meta-Learning by extending it to ",{"type":24,"tag":188,"props":1356,"children":1357},{},[1358],{"type":30,"value":1359},"Meta-Reinforcement Learning",{"type":30,"value":1361}," with publications such as ",{"type":24,"tag":61,"props":1363,"children":1366},{"href":1364,"rel":1365},"https://people.idsia.ch/~juergen/fki198-94.pdf",[65],[1367],{"type":24,"tag":164,"props":1368,"children":1369},{},[1370],{"type":30,"value":1371},"On learning how to learn learning strategies",{"type":30,"value":1373}," (1994) or ",{"type":24,"tag":61,"props":1375,"children":1378},{"href":1376,"rel":1377},"https://people.idsia.ch/~juergen/interest.html",[65],[1379],{"type":24,"tag":164,"props":1380,"children":1381},{},[1382],{"type":30,"value":1383},"What's interesting",{"type":30,"value":1385}," (1997), but we will skip this part since it falls more into the domain of Reinforcement Learning. Just recall the analogy I mentioned above between Meta-Learning in Reinforcement-Learning and Meta-Learning in Genetic Algorithms. It seems he was already pointing in that direction, and that was actually one of the main trends of Meta-Learning at that time. However, another direction was the one initiated by Bengio brothers back in 1992, and that was the one that brought us to the point we are nowadays.",{"type":24,"tag":39,"props":1387,"children":1388},{},[1389,1391,1401,1403,1408],{"type":30,"value":1390},"In that sense, Hochreiter made another interesting publication in 2001, called ",{"type":24,"tag":61,"props":1392,"children":1395},{"href":1393,"rel":1394},"https://www.researchgate.net/publication/225182080_Learning_To_Learn_Using_Gradient_Descent",[65],[1396],{"type":24,"tag":164,"props":1397,"children":1398},{},[1399],{"type":30,"value":1400},"Learning To Learn Using Gradient Descent",{"type":30,"value":1402},", where they used the aforementioned paradigm of the inner prediction algorithm and the outer optimizer (of the parametric Learning Rule) and proposed that this ",{"type":24,"tag":188,"props":1404,"children":1405},{},[1406],{"type":30,"value":1407},"outer optimizer should be updated by a Gradient Descent",{"type":30,"value":1409},". Oh, and finally they called that Meta-Learning. They proposed for this a task with sequences and used a Neural Network (don't be surprised, LeNet already existed) to perform the experiments. Both the inner predictor (task level) and the outer optimizer (meta-level) were RNNs. Before this publication, a similar approach was already studied called Adaptive Learning, which already used a Neural Network to optimize a learning rule. However, the setting there is different (no Meta-Learning at all).",{"type":24,"tag":39,"props":1411,"children":1412},{},[1413],{"type":24,"tag":402,"props":1414,"children":1416},{"alt":683,"src":1415},"https://i.imgur.com/8bSfzGl.png",[],{"type":24,"tag":39,"props":1418,"children":1419},{},[1420,1422,1427],{"type":30,"value":1421},"After that, obviously more publications about Meta-Learning appeared. However, until 2015 the focus of interest in ",{"type":24,"tag":188,"props":1423,"children":1424},{},[1425],{"type":30,"value":1426},"Machine Learning was on other topics",{"type":30,"value":1428}," (you know that it was a time of big changes, where the first truly big Neural Networks arrived, and everything began to explode), and I don't feel that this publications repercussion on the evolution of Meta-Learning is worth enough to include in this basic summary. So with this, I think we already have an idea of how the knowledge about Meta-Learning arrived in 2015, and how it was viewed back then when the interest returned with new motivations.",{"type":24,"tag":32,"props":1430,"children":1432},{"id":1431},"the-comeback-of-meta-learning-and-its-relation-to-few-shot-learning",[1433],{"type":24,"tag":188,"props":1434,"children":1435},{},[1436],{"type":30,"value":1437},"The comeback of Meta-Learning and its relation to Few-Shot Learning",{"type":24,"tag":39,"props":1439,"children":1440},{},[1441,1443,1448,1450,1455,1457,1462],{"type":30,"value":1442},"The interest in Meta-Learning returned when ML research gazed a further step than plain basic ML tasks. Before 2015, most of the applications ",{"type":24,"tag":188,"props":1444,"children":1445},{},[1446],{"type":30,"value":1447},"relied on vast amounts of data",{"type":30,"value":1449},", but at the time of making ",{"type":24,"tag":188,"props":1451,"children":1452},{},[1453],{"type":30,"value":1454},"ML accessible to anyone",{"type":30,"value":1456}," (not just the big fishes in the industry), that scenario ",{"type":24,"tag":188,"props":1458,"children":1459},{},[1460],{"type":30,"value":1461},"was not realistic",{"type":30,"value":1463},". Yes, there were already public datasets, but when trying to make some slightly ambiguous applications, it was needed some data conditions that were not easy to find. Not all small companies or particular researchers had access to a batch of 1 million images of, let's say, water impurities, and it was a too concrete phenomenon to find a huge open dataset about it.",{"type":24,"tag":39,"props":1465,"children":1466},{},[1467,1469,1479,1481,1486,1488,1492,1494,1499],{"type":30,"value":1468},"In 2015, Koch et al. presented the publication ",{"type":24,"tag":61,"props":1470,"children":1473},{"href":1471,"rel":1472},"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf",[65],[1474],{"type":24,"tag":164,"props":1475,"children":1476},{},[1477],{"type":30,"value":1478},"Siamese Neural Networks for One-shot Image Recognition",{"type":30,"value":1480},". As the title says, they introduced the concept of ",{"type":24,"tag":188,"props":1482,"children":1483},{},[1484],{"type":30,"value":1485},"One-shot Learning",{"type":30,"value":1487}," where they proposed to solve a ",{"type":24,"tag":188,"props":1489,"children":1490},{},[1491],{"type":30,"value":724},{"type":30,"value":1493}," (in the paper an image classification task) with just ",{"type":24,"tag":188,"props":1495,"children":1496},{},[1497],{"type":30,"value":1498},"one single sample (image) per class",{"type":30,"value":1500},". They argued that humans are able to do so (e.g., in the example we made before about recognizing a new bird, we may recognize it after seeing it just one time, at least with a good memory capacity!) so why couldn't a system do so automatically? In this paper Meta-Learning was not mentioned (they just described a method to match patterns efficiently with one single image in the Omniglot dataset), but the seed of curiosity was already planted to raise interest in this topic. It is not hard to imagine that the Meta-Learning term returned to action after that. In the end, we defined before that Meta-Learning was intended to use in order to make systems able to learn tasks more efficiently, and that included several interests. One of that interests may be precisely One-shot Learning. So from the more generic old definition, now we have a concrete motivation.",{"type":24,"tag":39,"props":1502,"children":1503},{},[1504],{"type":24,"tag":402,"props":1505,"children":1507},{"alt":683,"src":1506},"https://i.imgur.com/OktLV6n.png",[],{"type":24,"tag":39,"props":1509,"children":1510},{},[1511,1513,1518,1520,1525,1527,1532,1534,1539,1541,1546,1548,1553],{"type":30,"value":1512},"We will continue in the next section reviewing how Meta-learning was proposed after that, but first I would like to discuss the ",{"type":24,"tag":188,"props":1514,"children":1515},{},[1516],{"type":30,"value":1517},"feasibility of One-shot Learning",{"type":30,"value":1519},". Solving One-shot Learning problems using Meta-Learning is yet another, although possible, ",{"type":24,"tag":188,"props":1521,"children":1522},{},[1523],{"type":30,"value":1524},"ideal case",{"type":30,"value":1526},". One will not always dispose of enough data from the task domain that will allow performing Meta-Learning efficiently, and not always a representative enough task domain may be defined. Reality may be sometimes demoralizing, and yes, expecting to approach an ideal solution with another ideal approach is not exactly a guarantee of success. But there is still a motivation behind. One-shot Learning may be a too ambitious purpose in some cases, but if the term One-shot comes from one single sample, doesn't exist a Two-shot, or a Three-shot, and so on? Yes, it does. Actually, One-shot Learning is a particularization of ",{"type":24,"tag":188,"props":1528,"children":1529},{},[1530],{"type":30,"value":1531},"Few-shot Learning",{"type":30,"value":1533}," or ",{"type":24,"tag":188,"props":1535,"children":1536},{},[1537],{"type":30,"value":1538},"K-shot Learning",{"type":30,"value":1540},", which mean ",{"type":24,"tag":188,"props":1542,"children":1543},{},[1544],{"type":30,"value":1545},"learning with just few or K samples respectively",{"type":30,"value":1547},". Thus, the ",{"type":24,"tag":188,"props":1549,"children":1550},{},[1551],{"type":30,"value":1552},"level of ambition is adjustable to the resources and needs",{"type":30,"value":1554}," in each case. For example, maybe we would not be able to have 1 million images of water impurities, but getting 100 labeled images of them may be more accessible, as well as a 100-shot learning problem may be more realistic to solve than a One-shot one.",{"type":24,"tag":39,"props":1556,"children":1557},{},[1558,1560,1565,1567,1571],{"type":30,"value":1559},"So to conclude this section, Meta-Learning re-arose as a solution to Few-shot Learning. Said that I think it is important to not lose the old, more general perspective of it. That allows us to apply Meta-Learning for several problems such as ",{"type":24,"tag":188,"props":1561,"children":1562},{},[1563],{"type":30,"value":1564},"Active Learning",{"type":30,"value":1566},", ",{"type":24,"tag":188,"props":1568,"children":1569},{},[1570],{"type":30,"value":1124},{"type":30,"value":1572},", etc. But let's continue the story.",{"type":24,"tag":32,"props":1574,"children":1576},{"id":1575},"the-modern-meta-learning-approaches-presented",[1577],{"type":24,"tag":188,"props":1578,"children":1579},{},[1580],{"type":30,"value":1581},"The modern Meta-Learning approaches presented",{"type":24,"tag":39,"props":1583,"children":1584},{},[1585,1587,1594,1596,1601],{"type":30,"value":1586},"If you search for ",{"type":24,"tag":61,"props":1588,"children":1591},{"href":1589,"rel":1590},"https://imgur.com/fzTtsDP",[65],[1592],{"type":30,"value":1593},"Meta-Learning publications since 2016",{"type":30,"value":1595},", you may fall off your seat. However, what actually happened is that the modern strategies to perform Meta-Learning (most of them focused on One-Shot Learning) exploded. The ",{"type":24,"tag":188,"props":1597,"children":1598},{},[1599],{"type":30,"value":1600},"main Meta-Learning strategies",{"type":30,"value":1602}," nowadays are divided into 3 or 4 \"families\". Apart from the solutions that we will review in this section, the rest of the publications focus on experimenting with them and studying the behavior, modifying them with some witty hacks, or, what is most common nowadays, trying to use them in specific scenarios.",{"type":24,"tag":49,"props":1604,"children":1606},{"id":1605},"usage-of-memories",[1607],{"type":30,"value":1608},"Usage of memories",{"type":24,"tag":39,"props":1610,"children":1611},{},[1612,1614,1619,1621,1631,1633,1638,1640,1645,1647,1652,1654,1661,1663,1668],{"type":30,"value":1613},"Santoro et al. were the first (as far as I know) to refer to ",{"type":24,"tag":188,"props":1615,"children":1616},{},[1617],{"type":30,"value":1618},"Meta-Learning for solving the One-shot Learning",{"type":30,"value":1620}," issue with ",{"type":24,"tag":61,"props":1622,"children":1625},{"href":1623,"rel":1624},"https://proceedings.mlr.press/v48/santoro16.pdf",[65],[1626],{"type":24,"tag":164,"props":1627,"children":1628},{},[1629],{"type":30,"value":1630},"Meta-Learning with Memory-Augmented Neural Networks",{"type":30,"value":1632}," (2016). They propose the architecture ",{"type":24,"tag":188,"props":1634,"children":1635},{},[1636],{"type":30,"value":1637},"MANN",{"type":30,"value":1639}," as a modification of ",{"type":24,"tag":188,"props":1641,"children":1642},{},[1643],{"type":30,"value":1644},"Neural Turing Machine",{"type":30,"value":1646}," (Graves et al. 2014)  to achieve Meta-Learning by reducing complexity of the original mechanism, thus allowing to ",{"type":24,"tag":188,"props":1648,"children":1649},{},[1650],{"type":30,"value":1651},"learn in fewer steps",{"type":30,"value":1653},". Recall that one of the requirements of Deep Learning refers to the size of the dataset. There's a great post about ",{"type":24,"tag":61,"props":1655,"children":1658},{"href":1656,"rel":1657},"https://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html",[65],[1659],{"type":30,"value":1660},"NTM and MANN",{"type":30,"value":1662},". It is a complex mechanism and we may talk a lot about this, but I'd suggest to skip this part yet (the topic is interesting but may be better to learn about this another day, you don't want to overwhelm today after reading this post) and only have in mind this first approach to achieve ",{"type":24,"tag":188,"props":1664,"children":1665},{},[1666],{"type":30,"value":1667},"Meta-Learning by learning a storage mechanism",{"type":30,"value":1669},". There's a discussion about if this may be considered Meta-Learning or not.",{"type":24,"tag":39,"props":1671,"children":1672},{},[1673],{"type":24,"tag":402,"props":1674,"children":1676},{"alt":683,"src":1675},"https://i.imgur.com/muJQ04p.png",[],{"type":24,"tag":49,"props":1678,"children":1680},{"id":1679},"metric-learning",[1681],{"type":30,"value":1682},"Metric Learning",{"type":24,"tag":39,"props":1684,"children":1685},{},[1686,1688,1693,1695,1699,1701,1711,1713,1718,1720,1725,1727,1732,1734,1744,1746,1751,1753,1758,1760,1765],{"type":30,"value":1687},"This idea of \"",{"type":24,"tag":188,"props":1689,"children":1690},{},[1691],{"type":30,"value":1692},"reducing the complexity",{"type":30,"value":1694}," of the algorithm to reduce the need for data\" is also used in the second family of approaches we will review, ",{"type":24,"tag":188,"props":1696,"children":1697},{},[1698],{"type":30,"value":1682},{"type":30,"value":1700},". This idea was proposed by Vinayls et al. in ",{"type":24,"tag":61,"props":1702,"children":1705},{"href":1703,"rel":1704},"https://arxiv.org/pdf/1606.04080.pdf",[65],[1706],{"type":24,"tag":164,"props":1707,"children":1708},{},[1709],{"type":30,"value":1710},"Matching Networks for One-Shot Learning",{"type":30,"value":1712}," (2016). The authors aimed to switch from a Computer Vision space, where the problem is and which is typically solved through Deep Learning to ",{"type":24,"tag":188,"props":1714,"children":1715},{},[1716],{"type":30,"value":1717},"another space",{"type":30,"value":1719}," more likely to be solved by ",{"type":24,"tag":188,"props":1721,"children":1722},{},[1723],{"type":30,"value":1724},"non-parametric approaches",{"type":30,"value":1726}," (which don't need further training, e.g. kNN). To do so, they just train an ",{"type":24,"tag":188,"props":1728,"children":1729},{},[1730],{"type":30,"value":1731},"embedding Network with attention",{"type":30,"value":1733},", where attention acts as the final non-parametric matching algorithm (relates a given test image to each training one, related to one class each since we are working with One-shot Learning). The encoder network they propose is a RNN, the attention they use is Cosine Similarity and the loss they train with is a log one at Learning level while this is projected to the Meta-Learning level. This idea was also followed in ",{"type":24,"tag":61,"props":1735,"children":1738},{"href":1736,"rel":1737},"https://arxiv.org/pdf/1703.05175.pdf",[65],[1739],{"type":24,"tag":164,"props":1740,"children":1741},{},[1742],{"type":30,"value":1743},"Prototypical Networks for Few-Shot Learning",{"type":30,"value":1745}," in 2017, where Snell et al. proposed a ",{"type":24,"tag":188,"props":1747,"children":1748},{},[1749],{"type":30,"value":1750},"similar pipeline",{"type":30,"value":1752}," but instead of using a kNN-like algorithm (which Vinyals's attention mechanism stands for), they use a soft view of it, being ",{"type":24,"tag":188,"props":1754,"children":1755},{},[1756],{"type":30,"value":1757},"each class in the space a Gaussian distribution instead of a discrete frontier",{"type":30,"value":1759},". Each class distribution is called prototype in this paper, and it allows to use more than one sample per class, thus extending the problem to a Few-shot Learning problem instead of a One-shot one. The embedding function is learned from minimizing the negative log probability of the true class of the test samples. This approach of Meta-Learning is still popular nowadays due to its ",{"type":24,"tag":188,"props":1761,"children":1762},{},[1763],{"type":30,"value":1764},"simplicity",{"type":30,"value":1766},", and usually a good first step to experiment with Meta-Learning.",{"type":24,"tag":39,"props":1768,"children":1769},{},[1770],{"type":24,"tag":402,"props":1771,"children":1773},{"alt":683,"src":1772},"https://i.imgur.com/Yi60wou.png",[],{"type":24,"tag":39,"props":1775,"children":1776},{},[1777],{"type":24,"tag":402,"props":1778,"children":1780},{"alt":683,"src":1779},"https://i.imgur.com/hSDCqyd.png",[],{"type":24,"tag":49,"props":1782,"children":1784},{"id":1783},"optimizer-meta-learning",[1785],{"type":30,"value":1786},"Optimizer Meta-Learning",{"type":24,"tag":39,"props":1788,"children":1789},{},[1790,1792,1797,1799,1809,1811,1816,1818,1822,1824,1829,1830,1835,1836,1841,1843,1848,1850,1855,1857,1862,1864,1869,1871,1876,1878,1888,1890,1895,1897,1902,1904,1909,1911,1921,1923,1928,1930,1935],{"type":30,"value":1791},"Another way to achieve Meta-Learning recovers the old idea that Schmidhuber played with, ",{"type":24,"tag":188,"props":1793,"children":1794},{},[1795],{"type":30,"value":1796},"learning an optimal optimizer",{"type":30,"value":1798}," (do you remember?). I think this idea need no longer presentation. Andrychowicz et al. presented in 2016 ",{"type":24,"tag":61,"props":1800,"children":1803},{"href":1801,"rel":1802},"https://arxiv.org/pdf/1606.04474.pdf",[65],[1804],{"type":24,"tag":164,"props":1805,"children":1806},{},[1807],{"type":30,"value":1808},"Learning to Learn By Gradient Descent By Gradient Descent",{"type":30,"value":1810},", which is not a typo but its true name. Does this sound to you? If it doesn't, I'll remind you the Hochreiter publication in 2001 called Learning to Learn Using Gradient Descent. The idea is pretty similar. As well as you have an inner algorithm to solve the task, you also have an outer training algorithm. This ",{"type":24,"tag":188,"props":1812,"children":1813},{},[1814],{"type":30,"value":1815},"training algorithm may also be optimized",{"type":30,"value":1817}," by training at the ",{"type":24,"tag":188,"props":1819,"children":1820},{},[1821],{"type":30,"value":539},{"type":30,"value":1823},". So Andrychowicz calls them ",{"type":24,"tag":188,"props":1825,"children":1826},{},[1827],{"type":30,"value":1828},"optimizee",{"type":30,"value":649},{"type":24,"tag":188,"props":1831,"children":1832},{},[1833],{"type":30,"value":1834},"optimizer",{"type":30,"value":308},{"type":24,"tag":188,"props":1837,"children":1838},{},[1839],{"type":30,"value":1840},"optimizee is a parametric algorithm",{"type":30,"value":1842}," so it is actually optimized by its parameters (called ",{"type":24,"tag":188,"props":1844,"children":1845},{},[1846],{"type":30,"value":1847},"Meta-parameters",{"type":30,"value":1849},"). Wait, isn't this the same than Hochreiter proposed? This gave me a bit of confusion and to be honest is one of the things I'm less sure about in the whole Meta-Learning topic. But my interpretation is that Hochreiter's idea was just a ",{"type":24,"tag":188,"props":1851,"children":1852},{},[1853],{"type":30,"value":1854},"generalization",{"type":30,"value":1856}," that ",{"type":24,"tag":188,"props":1858,"children":1859},{},[1860],{"type":30,"value":1861},"allowed",{"type":30,"value":1863}," to use ",{"type":24,"tag":188,"props":1865,"children":1866},{},[1867],{"type":30,"value":1868},"gradient descent in both the optimizer and the optimizee",{"type":30,"value":1870},", while this paper present ",{"type":24,"tag":188,"props":1872,"children":1873},{},[1874],{"type":30,"value":1875},"architectures for specifically that",{"type":30,"value":1877},". However, the most important idea for you here is that this view is still one of the main approaches of Meta-Learning. Later on, Larochelle et al. presented in 2017 the publication ",{"type":24,"tag":61,"props":1879,"children":1882},{"href":1880,"rel":1881},"https://openreview.net/pdf?id=rJY0-Kcll",[65],[1883],{"type":24,"tag":164,"props":1884,"children":1885},{},[1886],{"type":30,"value":1887},"Optimization as a model for Few-Shot Learning",{"type":30,"value":1889},", which builds on the ",{"type":24,"tag":188,"props":1891,"children":1892},{},[1893],{"type":30,"value":1894},"same idea",{"type":30,"value":1896}," but in this case the ",{"type":24,"tag":188,"props":1898,"children":1899},{},[1900],{"type":30,"value":1901},"optimizer is the Gradient Descent itself",{"type":30,"value":1903},", and instead of modifying Meta-Parameters acts as a ",{"type":24,"tag":188,"props":1905,"children":1906},{},[1907],{"type":30,"value":1908},"weight predictor",{"type":30,"value":1910},". The other important publication about that was done by Mishra et al in 2018, called ",{"type":24,"tag":61,"props":1912,"children":1915},{"href":1913,"rel":1914},"https://arxiv.org/pdf/1707.03141.pdf",[65],[1916],{"type":24,"tag":164,"props":1917,"children":1918},{},[1919],{"type":30,"value":1920},"A simple Neural Attentive Meta-Learner",{"type":30,"value":1922},", where they ",{"type":24,"tag":188,"props":1924,"children":1925},{},[1926],{"type":30,"value":1927},"extend",{"type":30,"value":1929}," the idea of Andrychowicz by ",{"type":24,"tag":188,"props":1931,"children":1932},{},[1933],{"type":30,"value":1934},"instead of an RNN using an (soft) Attentional NN as the optimizer",{"type":30,"value":1936}," of Meta-parameters.",{"type":24,"tag":39,"props":1938,"children":1939},{},[1940],{"type":24,"tag":402,"props":1941,"children":1943},{"alt":683,"src":1942},"https://i.imgur.com/ZXAsypi.png",[],{"type":24,"tag":39,"props":1945,"children":1946},{},[1947],{"type":24,"tag":402,"props":1948,"children":1950},{"alt":683,"src":1949},"https://i.imgur.com/MN8bSAL.png",[],{"type":24,"tag":39,"props":1952,"children":1953},{},[1954],{"type":24,"tag":402,"props":1955,"children":1957},{"alt":683,"src":1956},"https://i.imgur.com/AVs0R5k.png",[],{"type":24,"tag":49,"props":1959,"children":1961},{"id":1960},"initialization-meta-learning",[1962],{"type":30,"value":1963},"Initialization Meta-Learning",{"type":24,"tag":39,"props":1965,"children":1966},{},[1967,1969,1974,1976,1986,1988,1993,1995,2000,2002,2007,2009,2014,2016,2020,2022,2027,2029,2034,2036,2040,2042,2047,2049,2054,2056,2061,2063,2073,2075,2080,2082,2092,2094,2099,2101,2111,2112,2117,2119,2124,2126,2131,2133,2138,2140,2150,2152,2157],{"type":30,"value":1968},"However, the probably ",{"type":24,"tag":188,"props":1970,"children":1971},{},[1972],{"type":30,"value":1973},"most popular",{"type":30,"value":1975}," approach of Meta-Learning is the one presented by Finn et al. in ",{"type":24,"tag":61,"props":1977,"children":1980},{"href":1978,"rel":1979},"https://arxiv.org/pdf/1703.03400.pdf",[65],[1981],{"type":24,"tag":164,"props":1982,"children":1983},{},[1984],{"type":30,"value":1985},"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",{"type":30,"value":1987}," (2017). These authors have made HUGE contributions to Meta-Learning, but this was the most iconic one. There, they presented ",{"type":24,"tag":188,"props":1989,"children":1990},{},[1991],{"type":30,"value":1992},"MAML",{"type":30,"value":1994},", a popular algorithm that aims to ",{"type":24,"tag":188,"props":1996,"children":1997},{},[1998],{"type":30,"value":1999},"find a proper initialization for the whole domain of tasks",{"type":30,"value":2001},".  This is applicable to ",{"type":24,"tag":188,"props":2003,"children":2004},{},[2005],{"type":30,"value":2006},"any combination of parameters",{"type":30,"value":2008},", thus becoming (as the title says) ",{"type":24,"tag":188,"props":2010,"children":2011},{},[2012],{"type":30,"value":2013},"Model-Agnostic",{"type":30,"value":2015},". At the ",{"type":24,"tag":188,"props":2017,"children":2018},{},[2019],{"type":30,"value":539},{"type":30,"value":2021},", weights follow a ",{"type":24,"tag":188,"props":2023,"children":2024},{},[2025],{"type":30,"value":2026},"path guided by a batch of tasks",{"type":30,"value":2028}," at each ",{"type":24,"tag":188,"props":2030,"children":2031},{},[2032],{"type":30,"value":2033},"meta-step",{"type":30,"value":2035},", where at each ",{"type":24,"tag":188,"props":2037,"children":2038},{},[2039],{"type":30,"value":724},{"type":30,"value":2041}," the model learns and gives a ",{"type":24,"tag":188,"props":2043,"children":2044},{},[2045],{"type":30,"value":2046},"final loss",{"type":30,"value":2048}," (after the desired few updates). This way it becomes able to (desirably) ",{"type":24,"tag":188,"props":2050,"children":2051},{},[2052],{"type":30,"value":2053},"learn quickly when facing a new task",{"type":30,"value":2055},". This algorithm is also pretty popular in Meta-Reinforcement Learning. But aside from MAML, we also have ",{"type":24,"tag":188,"props":2057,"children":2058},{},[2059],{"type":30,"value":2060},"Reptile",{"type":30,"value":2062},", presented by Nichol et al. in ",{"type":24,"tag":61,"props":2064,"children":2067},{"href":2065,"rel":2066},"https://arxiv.org/pdf/1803.02999.pdf",[65],[2068],{"type":24,"tag":164,"props":2069,"children":2070},{},[2071],{"type":30,"value":2072},"On First-Order Meta-Learning Algorithms",{"type":30,"value":2074}," (2018), where basically the ",{"type":24,"tag":188,"props":2076,"children":2077},{},[2078],{"type":30,"value":2079},"Meta-Learning trajectory follows also the individual tasks",{"type":30,"value":2081}," Learning one (what they find out to be the optimal path). Furthermore, Finn's team also presented in 2018 ",{"type":24,"tag":61,"props":2083,"children":2086},{"href":2084,"rel":2085},"https://arxiv.org/pdf/1806.02817.pdf",[65],[2087],{"type":24,"tag":164,"props":2088,"children":2089},{},[2090],{"type":30,"value":2091},"Probabilistic Model-Agnostic Meta-Learning",{"type":30,"value":2093}," (",{"type":24,"tag":188,"props":2095,"children":2096},{},[2097],{"type":30,"value":2098},"Probabilistic MAML",{"type":30,"value":2100},"), while Kim et al. presented ",{"type":24,"tag":61,"props":2102,"children":2105},{"href":2103,"rel":2104},"https://arxiv.org/pdf/1806.03836.pdf",[65],[2106],{"type":24,"tag":164,"props":2107,"children":2108},{},[2109],{"type":30,"value":2110},"Bayesian Model-Agnostic Meta-Learning",{"type":30,"value":2093},{"type":24,"tag":188,"props":2113,"children":2114},{},[2115],{"type":30,"value":2116},"BMAML",{"type":30,"value":2118},"). To be honest, I'm not sure about the conceptual difference between both, but the conclusion is that the flexibility of MAML allowed even to introduce ",{"type":24,"tag":188,"props":2120,"children":2121},{},[2122],{"type":30,"value":2123},"uncertainty",{"type":30,"value":2125},", where the learned (and therefore initialized in MAML) ",{"type":24,"tag":188,"props":2127,"children":2128},{},[2129],{"type":30,"value":2130},"weights worked in probabilistic frameworks",{"type":30,"value":2132},", thus being ",{"type":24,"tag":188,"props":2134,"children":2135},{},[2136],{"type":30,"value":2137},"distributions",{"type":30,"value":2139},". Also Finn's team (again) presented ",{"type":24,"tag":61,"props":2141,"children":2144},{"href":2142,"rel":2143},"https://arxiv.org/pdf/1902.08438.pdf",[65],[2145],{"type":24,"tag":164,"props":2146,"children":2147},{},[2148],{"type":30,"value":2149},"Online Meta-Learning",{"type":30,"value":2151}," (2019), where they used ",{"type":24,"tag":188,"props":2153,"children":2154},{},[2155],{"type":30,"value":2156},"MAML in an Online scenario",{"type":30,"value":2158},"**, where tasks were presented in a manner in which no information about future tasks was available at each batch.",{"type":24,"tag":39,"props":2160,"children":2161},{},[2162],{"type":24,"tag":402,"props":2163,"children":2165},{"alt":683,"src":2164},"https://i.imgur.com/6f3Fw6i.png",[],{"type":24,"tag":39,"props":2167,"children":2168},{},[2169],{"type":24,"tag":402,"props":2170,"children":2172},{"alt":683,"src":2171},"https://i.imgur.com/cLdIkro.png",[],{"type":24,"tag":39,"props":2174,"children":2175},{},[2176],{"type":24,"tag":402,"props":2177,"children":2179},{"alt":683,"src":2178},"https://i.imgur.com/XJ2Y8cG.png",[],{"type":24,"tag":39,"props":2181,"children":2182},{},[2183],{"type":24,"tag":402,"props":2184,"children":2186},{"alt":683,"src":2185},"https://i.imgur.com/fpWeEt4.png",[],{"type":24,"tag":39,"props":2188,"children":2189},{},[2190],{"type":24,"tag":402,"props":2191,"children":2193},{"alt":683,"src":2192},"https://i.imgur.com/wX4paDu.png",[],{"type":24,"tag":49,"props":2195,"children":2197},{"id":2196},"modular-meta-learning",[2198],{"type":30,"value":2199},"Modular Meta-Learning",{"type":24,"tag":39,"props":2201,"children":2202},{},[2203,2205,2215,2217,2222,2224,2229,2231,2236,2238,2243],{"type":30,"value":2204},"Last, in 2020 Chen et al. published ",{"type":24,"tag":61,"props":2206,"children":2209},{"href":2207,"rel":2208},"https://arxiv.org/pdf/1909.05557.pdf",[65],[2210],{"type":24,"tag":164,"props":2211,"children":2212},{},[2213],{"type":30,"value":2214},"Modular Meta-Learning with Shrinkage",{"type":30,"value":2216},", where they referred to what I think was the last of the big Meta-Learning approaches. They formalized the popular procedure when pretraining + fine-tuning is done by ",{"type":24,"tag":188,"props":2218,"children":2219},{},[2220],{"type":30,"value":2221},"modules",{"type":30,"value":2223}," (e.g. the typical frozen backbone while fine-tuning heads). What they proposed is ",{"type":24,"tag":188,"props":2225,"children":2226},{},[2227],{"type":30,"value":2228},"(meta-)learning the priors",{"type":30,"value":2230}," in which ",{"type":24,"tag":188,"props":2232,"children":2233},{},[2234],{"type":30,"value":2235},"each module",{"type":30,"value":2237}," has to ",{"type":24,"tag":188,"props":2239,"children":2240},{},[2241],{"type":30,"value":2242},"shrink",{"type":30,"value":2244}," (i.e. the strength to adapt to the task training). This way, they opened a door for new publications.",{"type":24,"tag":39,"props":2246,"children":2247},{},[2248],{"type":24,"tag":402,"props":2249,"children":2251},{"alt":683,"src":2250},"https://i.imgur.com/qUL4xKq.png",[],{"type":24,"tag":49,"props":2253,"children":2255},{"id":2254},"summary",[2256],{"type":30,"value":2257},"Summary",{"type":24,"tag":39,"props":2259,"children":2260},{},[2261],{"type":30,"value":2262},"So, summarizing, the main strategies proposed to perform Meta-Learning are:",{"type":24,"tag":956,"props":2264,"children":2265},{},[2266,2270,2275,2280,2285],{"type":24,"tag":960,"props":2267,"children":2268},{},[2269],{"type":30,"value":1608},{"type":24,"tag":960,"props":2271,"children":2272},{},[2273],{"type":30,"value":2274},"Metric Learning (converting to non-parametric algorithm)",{"type":24,"tag":960,"props":2276,"children":2277},{},[2278],{"type":30,"value":2279},"Optimizer Learning",{"type":24,"tag":960,"props":2281,"children":2282},{},[2283],{"type":30,"value":2284},"Initialization Learning",{"type":24,"tag":960,"props":2286,"children":2287},{},[2288],{"type":30,"value":2199},{"type":24,"tag":32,"props":2290,"children":2292},{"id":2291},"meta-learning-interesting-uses",[2293],{"type":24,"tag":188,"props":2294,"children":2295},{},[2296],{"type":30,"value":2297},"Meta-Learning interesting uses",{"type":24,"tag":956,"props":2299,"children":2300},{},[2301,2306,2311],{"type":24,"tag":960,"props":2302,"children":2303},{},[2304],{"type":30,"value":2305},"Few-Shot Learning: the first motivation of this wave of Meta-Learning publications. Learning from few data becomes possible when you learn how to learn with few data.",{"type":24,"tag":960,"props":2307,"children":2308},{},[2309],{"type":30,"value":2310},"Active Learning: I'm planning another post for this topic, but the problem stands for learning when human supervision has some costs. Again, possible if you learn how to solve this kind of problem.",{"type":24,"tag":960,"props":2312,"children":2313},{},[2314,2316,2326],{"type":30,"value":2315},"Unsupervised Learning: this is an interesting matter since we have defined everything under a supervised view, i.e. assuming we are able to design a Meta-Learning pipeline depending on a domain we will define usually knowing the classes. However, when we miss this information, we may find for an alternative way to define this schedule. In this direction, Metz et al. presented ",{"type":24,"tag":61,"props":2317,"children":2320},{"href":2318,"rel":2319},"https://arxiv.org/pdf/1804.00222.pdf",[65],[2321],{"type":24,"tag":164,"props":2322,"children":2323},{},[2324],{"type":30,"value":2325},"Meta-Learning Update Rules for Unsupervised Representation Learning",{"type":30,"value":2327}," (2019), where they propose a way to achieve that by finding an (unknown) class space from which to build artificial tasks and train the model from them, thus projecting it to the new unsupervised tasks. The authors perform several experiments with the different main Meta-Learning approaches.",{"type":24,"tag":32,"props":2329,"children":2331},{"id":2330},"the-future-of-meta-learning",[2332],{"type":24,"tag":188,"props":2333,"children":2334},{},[2335],{"type":30,"value":2336},"The future of Meta-Learning",{"type":24,"tag":39,"props":2338,"children":2339},{},[2340,2342,2347],{"type":30,"value":2341},"No, I'm not a prophet, but I have some ideas on what would be the natural ",{"type":24,"tag":188,"props":2343,"children":2344},{},[2345],{"type":30,"value":2346},"direction",{"type":30,"value":2348}," of all this.",{"type":24,"tag":39,"props":2350,"children":2351},{},[2352,2354,2359,2361,2366],{"type":30,"value":2353},"First, Meta-Learning has ",{"type":24,"tag":188,"props":2355,"children":2356},{},[2357],{"type":30,"value":2358},"already presented",{"type":30,"value":2360}," the most intuitive ",{"type":24,"tag":188,"props":2362,"children":2363},{},[2364],{"type":30,"value":2365},"approaches",{"type":30,"value":2367}," to be performed. As they still can be improved (just like any approach has received publications responding to it), the most important work there is presumably done.",{"type":24,"tag":39,"props":2369,"children":2370},{},[2371,2373,2378,2380,2385,2387,2391,2392,2397,2399,2404,2406,2411,2413,2417,2419,2424],{"type":30,"value":2372},"However, Meta-learning just began. All Machine Learning problems that may potentially be affected by the (at least temporal) amount of data, may follow a Meta-Learning strategy. Probably, the ",{"type":24,"tag":188,"props":2374,"children":2375},{},[2376],{"type":30,"value":2377},"Online scenario will gain strength",{"type":30,"value":2379}," in Meta-Learning research in the following years, since a common case is that a project begins with few data and later adds more and more. Furthermore, ",{"type":24,"tag":188,"props":2381,"children":2382},{},[2383],{"type":30,"value":2384},"building good schedules",{"type":30,"value":2386}," is still not accomplished, so ",{"type":24,"tag":188,"props":2388,"children":2389},{},[2390],{"type":30,"value":1124},{"type":30,"value":649},{"type":24,"tag":188,"props":2393,"children":2394},{},[2395],{"type":30,"value":2396},"Unsupervised scenarios",{"type":30,"value":2398}," will study in depth the application of Meta-Learning, for sure. Also, Meta-Learning still has a lot to say in other kinds of ",{"type":24,"tag":188,"props":2400,"children":2401},{},[2402],{"type":30,"value":2403},"data limitations",{"type":30,"value":2405}," such as ",{"type":24,"tag":188,"props":2407,"children":2408},{},[2409],{"type":30,"value":2410},"Incremental Learning",{"type":30,"value":2412}," (temporal bias issues), ",{"type":24,"tag":188,"props":2414,"children":2415},{},[2416],{"type":30,"value":1564},{"type":30,"value":2418}," (annotation issues), ",{"type":24,"tag":188,"props":2420,"children":2421},{},[2422],{"type":30,"value":2423},"Federated Learning",{"type":30,"value":2425}," (privacy issues)...",{"type":24,"tag":39,"props":2427,"children":2428},{},[2429,2431,2436,2437,2442],{"type":30,"value":2430},"Last, but not less important, Meta-Learning still has to deliver strong ",{"type":24,"tag":188,"props":2432,"children":2433},{},[2434],{"type":30,"value":2435},"frameworks",{"type":30,"value":649},{"type":24,"tag":188,"props":2438,"children":2439},{},[2440],{"type":30,"value":2441},"stable implementations",{"type":30,"value":2443}," so it becomes more and more popular. So, congratulations for reading this post and preparing for the future!",{"type":24,"tag":32,"props":2445,"children":2447},{"id":2446},"additional-resources",[2448],{"type":24,"tag":188,"props":2449,"children":2450},{},[2451],{"type":30,"value":2452},"Additional resources",{"type":24,"tag":39,"props":2454,"children":2455},{},[2456,2458,2464,2466,2473],{"type":30,"value":2457},"To complete this recap, I'm including a couple of summaries I did some time ago in two formats. First, a slide presentation which may be useful for a shorter ",{"type":24,"tag":61,"props":2459,"children":2462},{"href":2460,"rel":2461},"https://drive.google.com/file/d/12xTctbkXcOHNX-ZtTA3ZaKUEi5Ulj_vc/view?usp=sharing",[65],[2463],{"type":30,"value":2254},{"type":30,"value":2465},". Second, a sheet with ",{"type":24,"tag":61,"props":2467,"children":2470},{"href":2468,"rel":2469},"https://docs.google.com/spreadsheets/d/1IcaGSqPEVuF8iHD5G2wfl8xwpJmVuvnDOwrkZHIK1IU/edit?usp=sharing",[65],[2471],{"type":30,"value":2472},"a collection of important papers and notes",{"type":30,"value":98},{"type":24,"tag":32,"props":2475,"children":2477},{"id":2476},"thank-you-reader",[2478],{"type":24,"tag":188,"props":2479,"children":2480},{},[2481],{"type":30,"value":2482},"Thank you reader",{"type":24,"tag":39,"props":2484,"children":2485},{},[2486],{"type":30,"value":2487},"This is my first post, and writing it has been tough and has given me more work that I initially thought. However, the experience has filled me with more interest in continue making this blog live. As far as my life permits me to do it, I will be adding more content. This has just began!",{"type":24,"tag":32,"props":2489,"children":2491},{"id":2490},"references",[2492],{"type":24,"tag":188,"props":2493,"children":2494},{},[2495],{"type":30,"value":2496},"References",{"type":24,"tag":39,"props":2498,"children":2499},{},[2500,2509,2511],{"type":24,"tag":164,"props":2501,"children":2502},{},[2503],{"type":24,"tag":2504,"props":2505,"children":2506},"span",{},[2507],{"type":30,"value":2508},"1",{"type":30,"value":2510}," ",{"type":24,"tag":188,"props":2512,"children":2513},{},[2514],{"type":24,"tag":61,"props":2515,"children":2517},{"href":290,"rel":2516},[65],[2518],{"type":24,"tag":164,"props":2519,"children":2520},{},[2521],{"type":30,"value":2522},"Picking groups instead of samples: A close look at Static Pool-based Meta-Active Learning",{"type":24,"tag":39,"props":2524,"children":2525},{},[2526,2534,2535],{"type":24,"tag":164,"props":2527,"children":2528},{},[2529],{"type":24,"tag":2504,"props":2530,"children":2531},{},[2532],{"type":30,"value":2533},"2",{"type":30,"value":2510},{"type":24,"tag":188,"props":2536,"children":2537},{},[2538],{"type":24,"tag":61,"props":2539,"children":2541},{"href":1023,"rel":2540},[65],[2542],{"type":24,"tag":164,"props":2543,"children":2544},{},[2545],{"type":30,"value":1030},{"type":24,"tag":39,"props":2547,"children":2548},{},[2549,2557,2558],{"type":24,"tag":164,"props":2550,"children":2551},{},[2552],{"type":24,"tag":2504,"props":2553,"children":2554},{},[2555],{"type":30,"value":2556},"3",{"type":30,"value":2510},{"type":24,"tag":188,"props":2559,"children":2560},{},[2561],{"type":24,"tag":61,"props":2562,"children":2564},{"href":1210,"rel":2563},[65],[2565],{"type":24,"tag":164,"props":2566,"children":2567},{},[2568],{"type":30,"value":1217},{"type":24,"tag":39,"props":2570,"children":2571},{},[2572,2580,2581],{"type":24,"tag":164,"props":2573,"children":2574},{},[2575],{"type":24,"tag":2504,"props":2576,"children":2577},{},[2578],{"type":30,"value":2579},"4",{"type":30,"value":2510},{"type":24,"tag":188,"props":2582,"children":2583},{},[2584],{"type":24,"tag":61,"props":2585,"children":2587},{"href":1259,"rel":2586},[65],[2588],{"type":24,"tag":164,"props":2589,"children":2590},{},[2591],{"type":30,"value":2592},"Metalearning Machines Learn to Learn",{"type":24,"tag":39,"props":2594,"children":2595},{},[2596,2604,2605],{"type":24,"tag":164,"props":2597,"children":2598},{},[2599],{"type":24,"tag":2504,"props":2600,"children":2601},{},[2602],{"type":30,"value":2603},"5",{"type":30,"value":2510},{"type":24,"tag":188,"props":2606,"children":2607},{},[2608],{"type":24,"tag":61,"props":2609,"children":2611},{"href":1279,"rel":2610},[65],[2612],{"type":24,"tag":164,"props":2613,"children":2614},{},[2615],{"type":30,"value":1286},{"type":24,"tag":39,"props":2617,"children":2618},{},[2619,2627,2628],{"type":24,"tag":164,"props":2620,"children":2621},{},[2622],{"type":24,"tag":2504,"props":2623,"children":2624},{},[2625],{"type":30,"value":2626},"6",{"type":30,"value":2510},{"type":24,"tag":188,"props":2629,"children":2630},{},[2631],{"type":24,"tag":61,"props":2632,"children":2634},{"href":1364,"rel":2633},[65],[2635],{"type":24,"tag":164,"props":2636,"children":2637},{},[2638],{"type":30,"value":1371},{"type":24,"tag":39,"props":2640,"children":2641},{},[2642,2650,2651],{"type":24,"tag":164,"props":2643,"children":2644},{},[2645],{"type":24,"tag":2504,"props":2646,"children":2647},{},[2648],{"type":30,"value":2649},"7",{"type":30,"value":2510},{"type":24,"tag":188,"props":2652,"children":2653},{},[2654],{"type":24,"tag":61,"props":2655,"children":2657},{"href":1376,"rel":2656},[65],[2658],{"type":24,"tag":164,"props":2659,"children":2660},{},[2661],{"type":30,"value":1383},{"type":24,"tag":39,"props":2663,"children":2664},{},[2665,2673,2674],{"type":24,"tag":164,"props":2666,"children":2667},{},[2668],{"type":24,"tag":2504,"props":2669,"children":2670},{},[2671],{"type":30,"value":2672},"8",{"type":30,"value":2510},{"type":24,"tag":188,"props":2675,"children":2676},{},[2677],{"type":24,"tag":61,"props":2678,"children":2680},{"href":1393,"rel":2679},[65],[2681],{"type":24,"tag":164,"props":2682,"children":2683},{},[2684],{"type":30,"value":1400},{"type":24,"tag":39,"props":2686,"children":2687},{},[2688,2696,2697],{"type":24,"tag":164,"props":2689,"children":2690},{},[2691],{"type":24,"tag":2504,"props":2692,"children":2693},{},[2694],{"type":30,"value":2695},"9",{"type":30,"value":2510},{"type":24,"tag":188,"props":2698,"children":2699},{},[2700],{"type":24,"tag":61,"props":2701,"children":2703},{"href":1471,"rel":2702},[65],[2704],{"type":24,"tag":164,"props":2705,"children":2706},{},[2707],{"type":30,"value":1478},{"type":24,"tag":39,"props":2709,"children":2710},{},[2711,2719,2720],{"type":24,"tag":164,"props":2712,"children":2713},{},[2714],{"type":24,"tag":2504,"props":2715,"children":2716},{},[2717],{"type":30,"value":2718},"10",{"type":30,"value":2510},{"type":24,"tag":188,"props":2721,"children":2722},{},[2723],{"type":24,"tag":61,"props":2724,"children":2726},{"href":1623,"rel":2725},[65],[2727],{"type":24,"tag":164,"props":2728,"children":2729},{},[2730],{"type":30,"value":1630},{"type":24,"tag":39,"props":2732,"children":2733},{},[2734,2742,2743],{"type":24,"tag":164,"props":2735,"children":2736},{},[2737],{"type":24,"tag":2504,"props":2738,"children":2739},{},[2740],{"type":30,"value":2741},"11",{"type":30,"value":2510},{"type":24,"tag":188,"props":2744,"children":2745},{},[2746],{"type":24,"tag":61,"props":2747,"children":2749},{"href":1656,"rel":2748},[65],[2750],{"type":24,"tag":164,"props":2751,"children":2752},{},[2753],{"type":30,"value":2754},"Explanation of One-shot Learning with Memory-Augmented Neural Networks",{"type":24,"tag":39,"props":2756,"children":2757},{},[2758,2766,2767],{"type":24,"tag":164,"props":2759,"children":2760},{},[2761],{"type":24,"tag":2504,"props":2762,"children":2763},{},[2764],{"type":30,"value":2765},"12",{"type":30,"value":2510},{"type":24,"tag":188,"props":2768,"children":2769},{},[2770],{"type":24,"tag":61,"props":2771,"children":2773},{"href":1703,"rel":2772},[65],[2774],{"type":24,"tag":164,"props":2775,"children":2776},{},[2777],{"type":30,"value":1710},{"type":24,"tag":39,"props":2779,"children":2780},{},[2781,2789,2790],{"type":24,"tag":164,"props":2782,"children":2783},{},[2784],{"type":24,"tag":2504,"props":2785,"children":2786},{},[2787],{"type":30,"value":2788},"13",{"type":30,"value":2510},{"type":24,"tag":188,"props":2791,"children":2792},{},[2793],{"type":24,"tag":61,"props":2794,"children":2796},{"href":1736,"rel":2795},[65],[2797],{"type":24,"tag":164,"props":2798,"children":2799},{},[2800],{"type":30,"value":1743},{"type":24,"tag":39,"props":2802,"children":2803},{},[2804,2812,2813],{"type":24,"tag":164,"props":2805,"children":2806},{},[2807],{"type":24,"tag":2504,"props":2808,"children":2809},{},[2810],{"type":30,"value":2811},"14",{"type":30,"value":2510},{"type":24,"tag":188,"props":2814,"children":2815},{},[2816],{"type":24,"tag":61,"props":2817,"children":2819},{"href":1801,"rel":2818},[65],[2820],{"type":24,"tag":164,"props":2821,"children":2822},{},[2823],{"type":30,"value":1808},{"type":24,"tag":39,"props":2825,"children":2826},{},[2827,2835,2836],{"type":24,"tag":164,"props":2828,"children":2829},{},[2830],{"type":24,"tag":2504,"props":2831,"children":2832},{},[2833],{"type":30,"value":2834},"15",{"type":30,"value":2510},{"type":24,"tag":188,"props":2837,"children":2838},{},[2839],{"type":24,"tag":61,"props":2840,"children":2842},{"href":1880,"rel":2841},[65],[2843],{"type":24,"tag":164,"props":2844,"children":2845},{},[2846],{"type":30,"value":1887},{"type":24,"tag":39,"props":2848,"children":2849},{},[2850,2858,2859],{"type":24,"tag":164,"props":2851,"children":2852},{},[2853],{"type":24,"tag":2504,"props":2854,"children":2855},{},[2856],{"type":30,"value":2857},"16",{"type":30,"value":2510},{"type":24,"tag":188,"props":2860,"children":2861},{},[2862],{"type":24,"tag":61,"props":2863,"children":2865},{"href":1913,"rel":2864},[65],[2866],{"type":24,"tag":164,"props":2867,"children":2868},{},[2869],{"type":30,"value":1920},{"type":24,"tag":39,"props":2871,"children":2872},{},[2873,2881,2882],{"type":24,"tag":164,"props":2874,"children":2875},{},[2876],{"type":24,"tag":2504,"props":2877,"children":2878},{},[2879],{"type":30,"value":2880},"17",{"type":30,"value":2510},{"type":24,"tag":188,"props":2883,"children":2884},{},[2885],{"type":24,"tag":61,"props":2886,"children":2888},{"href":1978,"rel":2887},[65],[2889],{"type":24,"tag":164,"props":2890,"children":2891},{},[2892],{"type":30,"value":2893},"Model-Agnostic Meta-Learning",{"type":24,"tag":39,"props":2895,"children":2896},{},[2897,2905,2906],{"type":24,"tag":164,"props":2898,"children":2899},{},[2900],{"type":24,"tag":2504,"props":2901,"children":2902},{},[2903],{"type":30,"value":2904},"18",{"type":30,"value":2510},{"type":24,"tag":188,"props":2907,"children":2908},{},[2909],{"type":24,"tag":61,"props":2910,"children":2912},{"href":2065,"rel":2911},[65],[2913],{"type":24,"tag":164,"props":2914,"children":2915},{},[2916],{"type":30,"value":2072},{"type":24,"tag":39,"props":2918,"children":2919},{},[2920,2928,2929],{"type":24,"tag":164,"props":2921,"children":2922},{},[2923],{"type":24,"tag":2504,"props":2924,"children":2925},{},[2926],{"type":30,"value":2927},"19",{"type":30,"value":2510},{"type":24,"tag":188,"props":2930,"children":2931},{},[2932],{"type":24,"tag":61,"props":2933,"children":2935},{"href":2084,"rel":2934},[65],[2936],{"type":24,"tag":164,"props":2937,"children":2938},{},[2939],{"type":30,"value":2091},{"type":24,"tag":39,"props":2941,"children":2942},{},[2943,2951,2952],{"type":24,"tag":164,"props":2944,"children":2945},{},[2946],{"type":24,"tag":2504,"props":2947,"children":2948},{},[2949],{"type":30,"value":2950},"20",{"type":30,"value":2510},{"type":24,"tag":188,"props":2953,"children":2954},{},[2955],{"type":24,"tag":61,"props":2956,"children":2958},{"href":2103,"rel":2957},[65],[2959],{"type":24,"tag":164,"props":2960,"children":2961},{},[2962],{"type":30,"value":2110},{"type":24,"tag":39,"props":2964,"children":2965},{},[2966,2974,2975],{"type":24,"tag":164,"props":2967,"children":2968},{},[2969],{"type":24,"tag":2504,"props":2970,"children":2971},{},[2972],{"type":30,"value":2973},"21",{"type":30,"value":2510},{"type":24,"tag":188,"props":2976,"children":2977},{},[2978],{"type":24,"tag":61,"props":2979,"children":2981},{"href":2142,"rel":2980},[65],[2982],{"type":24,"tag":164,"props":2983,"children":2984},{},[2985],{"type":30,"value":2149},{"type":24,"tag":39,"props":2987,"children":2988},{},[2989,2997,2998],{"type":24,"tag":164,"props":2990,"children":2991},{},[2992],{"type":24,"tag":2504,"props":2993,"children":2994},{},[2995],{"type":30,"value":2996},"22",{"type":30,"value":2510},{"type":24,"tag":188,"props":2999,"children":3000},{},[3001],{"type":24,"tag":61,"props":3002,"children":3004},{"href":2207,"rel":3003},[65],[3005],{"type":24,"tag":164,"props":3006,"children":3007},{},[3008],{"type":30,"value":2214},{"type":24,"tag":39,"props":3010,"children":3011},{},[3012,3020,3021],{"type":24,"tag":164,"props":3013,"children":3014},{},[3015],{"type":24,"tag":2504,"props":3016,"children":3017},{},[3018],{"type":30,"value":3019},"23",{"type":30,"value":2510},{"type":24,"tag":188,"props":3022,"children":3023},{},[3024],{"type":24,"tag":61,"props":3025,"children":3027},{"href":2318,"rel":3026},[65],[3028],{"type":24,"tag":164,"props":3029,"children":3030},{},[3031],{"type":30,"value":2325},{"title":10,"searchDepth":211,"depth":211,"links":3033},[3034,3035,3036,3037,3038,3046,3047,3048,3049,3050],{"id":269,"depth":211,"text":275},{"id":345,"depth":211,"text":351},{"id":1009,"depth":211,"text":1015},{"id":1431,"depth":211,"text":1437},{"id":1575,"depth":211,"text":1581,"children":3039},[3040,3041,3042,3043,3044,3045],{"id":1605,"depth":216,"text":1608},{"id":1679,"depth":216,"text":1682},{"id":1783,"depth":216,"text":1786},{"id":1960,"depth":216,"text":1963},{"id":2196,"depth":216,"text":2199},{"id":2254,"depth":216,"text":2257},{"id":2291,"depth":211,"text":2297},{"id":2330,"depth":211,"text":2336},{"id":2446,"depth":211,"text":2452},{"id":2476,"depth":211,"text":2482},{"id":2490,"depth":211,"text":2496},"content:articles:2022-11-21-meta-learning.md","articles/2022-11-21-meta-learning.md",["ShallowRef",3054],{},{"preference":567,"value":567,"unknown":3056,"forced":9},true,[3058,3061,3083],{"title":3059,"_path":3060,"layout":230},"About","/",{"title":3062,"_path":3063,"children":3064,"layout":3082},"Articles","/articles",[3065,3066,3067,3070,3073,3076,3079],{"title":11,"_path":7,"layout":13},{"title":250,"_path":249,"layout":13},{"title":3068,"_path":3069,"layout":13},"Meta-Learning implementation","/articles/2022-12-20-meta-learning-implementation",{"title":3071,"_path":3072,"layout":13},"Meta-Learning: MAML evaluation and discussion by Metabloggism","/articles/2023-02-07-meta-learning-analysis",{"title":3074,"_path":3075,"layout":13},"Introduction of SHORTS","/articles/2023-02-08-shorts",{"title":3077,"_path":3078,"layout":13},"Short: ONNX","/articles/2023-02-09-short-onnx",{"title":3080,"_path":3081,"layout":13},"Short: Shap values","/articles/2023-04-14-short-shap","page",{"title":229,"_path":228,"layout":230},{"uil:github":3085,"uil:linkedin":3089,"material-symbols:arrow-upward":3091,"ph:arrow-left":3093},{"left":3086,"top":3086,"width":3087,"height":3087,"rotate":3086,"vFlip":9,"hFlip":9,"body":3088},0,24,"\u003Cpath fill=\"currentColor\" d=\"M12 2.247a10 10 0 0 0-3.162 19.487c.5.088.687-.212.687-.475c0-.237-.012-1.025-.012-1.862c-2.513.462-3.163-.613-3.363-1.175a3.636 3.636 0 0 0-1.025-1.413c-.35-.187-.85-.65-.013-.662a2.001 2.001 0 0 1 1.538 1.025a2.137 2.137 0 0 0 2.912.825a2.104 2.104 0 0 1 .638-1.338c-2.225-.25-4.55-1.112-4.55-4.937a3.892 3.892 0 0 1 1.025-2.688a3.594 3.594 0 0 1 .1-2.65s.837-.262 2.75 1.025a9.427 9.427 0 0 1 5 0c1.912-1.3 2.75-1.025 2.75-1.025a3.593 3.593 0 0 1 .1 2.65a3.869 3.869 0 0 1 1.025 2.688c0 3.837-2.338 4.687-4.563 4.937a2.368 2.368 0 0 1 .675 1.85c0 1.338-.012 2.413-.012 2.75c0 .263.187.575.687.475A10.005 10.005 0 0 0 12 2.247Z\"/>",{"left":3086,"top":3086,"width":3087,"height":3087,"rotate":3086,"vFlip":9,"hFlip":9,"body":3090},"\u003Cpath fill=\"currentColor\" d=\"M20.47 2H3.53a1.45 1.45 0 0 0-1.47 1.43v17.14A1.45 1.45 0 0 0 3.53 22h16.94a1.45 1.45 0 0 0 1.47-1.43V3.43A1.45 1.45 0 0 0 20.47 2ZM8.09 18.74h-3v-9h3ZM6.59 8.48a1.56 1.56 0 1 1 0-3.12a1.57 1.57 0 1 1 0 3.12Zm12.32 10.26h-3v-4.83c0-1.21-.43-2-1.52-2A1.65 1.65 0 0 0 12.85 13a2 2 0 0 0-.1.73v5h-3v-9h3V11a3 3 0 0 1 2.71-1.5c2 0 3.45 1.29 3.45 4.06Z\"/>",{"left":3086,"top":3086,"width":3087,"height":3087,"rotate":3086,"vFlip":9,"hFlip":9,"body":3092},"\u003Cpath fill=\"currentColor\" d=\"M11 20V7.825l-5.6 5.6L4 12l8-8l8 8l-1.4 1.425l-5.6-5.6V20h-2Z\"/>",{"left":3086,"top":3086,"width":3094,"height":3094,"rotate":3086,"vFlip":9,"hFlip":9,"body":3095},256,"\u003Cpath fill=\"currentColor\" d=\"M224 128a8 8 0 0 1-8 8H59.31l58.35 58.34a8 8 0 0 1-11.32 11.32l-72-72a8 8 0 0 1 0-11.32l72-72a8 8 0 0 1 11.32 11.32L59.31 120H216a8 8 0 0 1 8 8Z\"/>",["Reactive",3097],{}]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{FORMSPREE_URL:"",plausible:{hashMode:false,trackLocalhost:false,domain:"",apiHost:"https://plausible.io",autoPageviews:true,autoOutboundTracking:false},studio:{apiURL:"https://api.nuxt.studio"},mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1693576793614,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:["navTitle","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:{default:"github-light",dark:"github-dark"},preload:["json","js","ts","html","css","vue","diff","shell","markdown","yaml","bash","ini","c","cpp"]},wsUrl:"",documentDriven:{page:true,navigation:true,surround:true,globals:{},layoutFallbacks:["theme"],injectPage:true},host:"",trailingSlash:false,contentHead:true,anchorLinks:{depth:4,exclude:[1]}}},app:{baseURL:"/",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body>
</html>